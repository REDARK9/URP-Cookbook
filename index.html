<html><head><style>body {
   color: black;
}
</style></head><body><p>⟶ E-BOOK</p>
<p><img src="_page_0_Picture_1.jpeg" alt=""></p>
<h1 id="the-universal-render-pipeline-cookbook-recipes-for-shaders-and-visual-effects-">The Universal Render Pipeline cookbook: <strong>Recipes for shaders and visual effects</strong></h1>
<p><img src="_page_0_Picture_3.jpeg" alt=""></p>
<p><img src="_page_0_Picture_4.jpeg" alt=""></p>
<p>© 2025 Unity Technologies</p>
<h2 id="-contents-"><strong>Contents</strong></h2>
<table>
<thead>
<tr>
<th>Introduction 5</th>
</tr>
</thead>
<tbody>
<tr>
<td>Author and contributors 7</td>
</tr>
<tr>
<td>Unity contributors 7</td>
</tr>
<tr>
<td>Getting started with this guide 8</td>
</tr>
<tr>
<td>Starting a new URP project 9</td>
</tr>
<tr>
<td>Importing e-book sample scenes 10</td>
</tr>
<tr>
<td>Stencils 13</td>
</tr>
<tr>
<td>Renderer Features 14</td>
</tr>
<tr>
<td>Instancing 20</td>
</tr>
<tr>
<td>GPU Resident Drawer and GPU occlusion culling 21</td>
</tr>
<tr>
<td>Instancing 23</td>
</tr>
<tr>
<td>SRP Batcher 24</td>
</tr>
<tr>
<td>GPU Instancing 26</td>
</tr>
<tr>
<td>RenderMeshPrimitives 28</td>
</tr>
<tr>
<td>Toon and outline shading 33</td>
</tr>
<tr>
<td>Simple toon shading 35</td>
</tr>
<tr>
<td>Shading 35</td>
</tr>
<tr>
<td>Outlining 36</td>
</tr>
<tr>
<td>Toon shading 37</td>
</tr>
<tr>
<td>Shading 37</td>
</tr>
<tr>
<td>Outlining 41</td>
</tr>
<tr>
<td>Ambient occlusion 44</td>
</tr>
<tr>
<td>SSAO properties 46</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Decals 49</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>URP Decal Projection properties 51</td>
<td></td>
</tr>
<tr>
<td>Creating the material 52</td>
<td></td>
</tr>
<tr>
<td>Adding a decal with code 53</td>
<td></td>
</tr>
<tr>
<td>Water 55</td>
<td></td>
</tr>
<tr>
<td>DepthFade subgraph 57</td>
<td></td>
</tr>
<tr>
<td>TextureMovement subgraph 58</td>
<td></td>
</tr>
<tr>
<td>Water shader 58</td>
<td></td>
</tr>
<tr>
<td>Color 59</td>
<td></td>
</tr>
<tr>
<td>Normal maps 61</td>
<td></td>
</tr>
<tr>
<td>Swell 62</td>
<td></td>
</tr>
<tr>
<td>LUT for color grading 64</td>
<td></td>
</tr>
<tr>
<td>Adaptive Probe Volumes 72</td>
<td></td>
</tr>
<tr>
<td>Using APVs in a scene 74</td>
<td></td>
</tr>
<tr>
<td>Lighting Scenario asset 77</td>
<td></td>
</tr>
<tr>
<td>Fixing issues with APVs 80</td>
<td></td>
</tr>
<tr>
<td>Light leaks 82</td>
<td></td>
</tr>
<tr>
<td>Rendering Layers 83</td>
<td></td>
</tr>
<tr>
<td>Streaming APVs 86</td>
<td></td>
</tr>
<tr>
<td>Sky occlusion 87</td>
<td></td>
</tr>
<tr>
<td>Light probes vs APVs 90</td>
<td></td>
</tr>
<tr>
<td>Screen space refraction 92</td>
<td></td>
</tr>
<tr>
<td>Volumetrics 102</td>
<td></td>
</tr>
<tr>
<td>Volumetric cloud 103</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Procedural noise 114</th>
</tr>
</thead>
<tbody>
<tr>
<td>Types of procedural noise 115</td>
</tr>
<tr>
<td>Implementing procedural noise in Unity 115</td>
</tr>
<tr>
<td>Procedural heightmap example 116</td>
</tr>
<tr>
<td>Using noise to generate a wood texture 119</td>
</tr>
<tr>
<td>Benefits of procedural noise 126</td>
</tr>
<tr>
<td>Challenges and optimization 127</td>
</tr>
<tr>
<td>Compute shaders 128</td>
</tr>
<tr>
<td>ParticleFun 129</td>
</tr>
<tr>
<td>Adding a mesh object 139</td>
</tr>
<tr>
<td>Conclusion 151</td>
</tr>
</tbody>
</table>
<h2 id="-span-id-page-4-0-span-introduction"><span id="page-4-0"></span>Introduction</h2>
<p>A dash of post-processing effects, a cup of decals, a pinch of color grading, and some sparkling water: It&#39;s time to cook up some high-quality lighting and visual effects in your games using the Universal Render Pipeline (URP).</p>
<p>Over 12 chapters, our URP cookbook offers up numerous recipes for creating popular effects. Additionally, sample scenes based on these recipes are available to download from <a href="https://github.com/NikLever/Unity-URP-Cookbook-Unity6">this</a> GitHub repo maintained by the main author, Nik Lever.</p>
<p>This guide is aimed at intermediate Unity users who have developed projects in Unity, know how to use URP features, and have some knowledge of writing HLSL-based shaders.</p>
<p>You&#39;ll get all the ingredients you need to:</p>
<ul>
<li>Create an x-ray-like image effect with stencils.</li>
<li>Build a toon and outline shader with Shader Graph.</li>
<li>Add an ambient occlusion effect with post-processing.</li>
<li>Use Photoshop and a LUT image to add color grading to your scenes.</li>
<li>Produce reflections and refraction, and much more.</li>
</ul>
<p><img src="_page_5_Picture_0.jpeg" alt=""></p>
<p>The e-book and the downloadable sample scenes for each recipe are now updated for Unity 6. New additions to this guide include:</p>
<ul>
<li>Two new recipes for procedural noise and compute shaders</li>
<li>A full revision of the Toon shader</li>
<li>A section on how to implement Adaptive Probe Volumes (APVs), a quick and flexible alternative to light probes</li>
<li>Steps on how to use the new Render Graph API for creating Renderer Features</li>
</ul>
<p>Reference this cookbook alongside the e-book <em><a href="https://unity.com/resources/introduction-to-urp-advanced-creators-unity-6">Introduction to the Universal Render Pipeline for</a>  <a href="https://unity.com/resources/introduction-to-urp-advanced-creators-unity-6">advanced Unity creators</a></em>. There is also a playlist of <a href="https://www.youtube.com/watch?v=NFBr21V0zvU&amp;list=PLX2vGYjWbI0QRLkvupULwSZCPkLyHs-UX">URP tutorials</a> on Unity&#39;s YouTube channel, providing both general and more specialized tips for creating lighting and effects for your games.</p>
<p>We hope you have fun creating beautiful effects for your game.</p>
<p>Many of the recipes in this e-book use <a href="https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl">High-Level Shader</a> <a href="https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl">Language</a> or HLSL. If you are new to this language then see the following resources for a good introduction:</p>
<ul>
<li><a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/writing-shaders-urp-landing.html">Example</a> <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/writing-shaders-urp-landing.html">of</a> <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/writing-shaders-urp-landing.html">custom shaders from the Unity Manual</a></li>
<li><a href="https://www.ronja-tutorials.com/post/002-hlsl/">Ronja&#39;s HLSL tutorials</a></li>
<li><a href="https://www.udemy.com/course/learn-unity-shaders-from-scratch/">Udemy: Learn Unity Shaders from Scratch</a></li>
<li><a href="https://thebookofshaders.com/">The Book of Shaders</a></li>
</ul>
<p><img src="_page_5_Picture_13.jpeg" alt=""></p>
<p>This image is from <em>PRINCIPLES,</em> a sample of what URP can achieve in the hands of experienced developers. <em>PRINCIPLES</em> is an adventure game from COLOPL Creators, the technology brand of COLOPL Inc, who developed the series of <em>Shironeko Project</em> and <em>Quiz RPG: The World of Mystic Wiz</em>. Experience a deep underworld that makes use of Unity&#39;s latest features, including URP, for stunning graphics and immersive 3D sound. <em>PRINCIPLES</em> is currently available on <a href="https://apps.apple.com/jp/app/principles/id1620294510">App Store</a> or <a href="https://play.google.com/store/apps/details?id=jp.colopl.ruins">Google Play</a>. You can also watch an interview with the studio <a href="https://www.youtube.com/watch?v=qKTX5GKKpwM">here</a>.</p>
<h2 id="-span-id-page-6-0-span-author-and-contributors"><span id="page-6-0"></span>Author and contributors</h2>
<p>Nik Lever, the main author of this e-book, has been creating real-time 3D content since the mid-90s and using Unity since 2006. For over 30 years, he&#39;s led the small development company Catalyst Pictures, and has provided courses since 2018 with the aim of helping game developers expand their knowledge in a rapidly evolving industry.</p>
<h4 id="-unity-contributors-"><strong>Unity contributors</strong></h4>
<p><strong>Steven Cannavan</strong> is a senior software development consultant at Unity, specializing in graphics and rendering. He has over 15 years of experience in the game development industry.</p>
<p><strong>MingWai Chan</strong> is a senior technical artist on Unity&#39;s graphics engineering team. She has worked at Unity for eight years and has been using the Editor since 2012.</p>
<p><strong>Oliver Schnabel</strong> is a senior technical product manager in Unity&#39;s graphics team, where he integrates customer insights and works with global studios to prioritize the development of a more performant, unified, and scalable rendering stack. He brings extensive experience in computer graphics and real-time development.</p>
<p><strong>Jonas Mortensen</strong> is a senior technical artist on Unity&#39;s Graphics team.</p>
<p><strong>Adrien Moulin</strong> is a senior graphics developer in Unity&#39;s render pipeline team. He has over eight years of experience in the simulation and real-time software industry. He is currently focused on delivering the best possible foundations and APIs to the Scriptable Render Pipeline users.</p>
<p><strong>Mathieu Muller</strong> is the lead product manager for Graphics at Unity. He leads the Graphics product management team and oversees the Graphics roadmap and product vision.</p>
<p><strong>Damian Nachman</strong> is a senior technical product manager in Unity&#39;s graphics team, specializing in low-level graphics development and optimization. He has 10 years of experience with working on real-time graphics engines and benchmarking across multiple industries.</p>
<h2 id="-span-id-page-7-0-span-getting-started-with-this-guide"><span id="page-7-0"></span>Getting started with this guide</h2>
<p>You can follow the steps in each recipe to recreate the lighting and visual effects by opening a new URP project. Additionally, you can access the <a href="https://github.com/NikLever/Unity-URP-Cookbook-Unity6">Github page</a> that accompanies this guide, which provides you with downloadable sample scenes for each recipe.</p>
<p><img src="_page_7_Figure_2.jpeg" alt=""></p>
<p>All of the recipes have been updated extensively to work in Unity 6.</p>
<h2 id="-span-id-page-8-0-span-starting-a-new-urp-project"><span id="page-8-0"></span>Starting a new URP project</h2>
<p>Open a new project using URP via the Unity Hub. Click <strong>New</strong>, and verify that the Unity version selected at the top of the window is 6000.01 or newer. Choose a name and location for the project, select the <strong>3D (URP)</strong> template, and click <strong>Create</strong>.</p>
<table>
<thead>
<tr>
<th></th>
<th>New project<br>Editor Version: 6000.0.18f1 ≤ Silicon (</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>=<br>All templates</td>
<td>Q Search all templates</td>
<td></td>
</tr>
<tr>
<td>ම<br>Core<br>다.<br>Sample</td>
<td>Universal 2D<br>SRP<br>Core</td>
<td>SRF</td>
</tr>
<tr>
<td>ಳು<br>Learning</td>
<td>Universal 3D<br>SRP<br>Core</td>
<td>Universal 3D</td>
</tr>
<tr>
<td></td>
<td>High Definition 3D<br>SRP<br>Core</td>
<td>This template includes the settings and<br>assets you need to start creating with the<br>Universal Render Pipeline (URP).</td>
</tr>
<tr>
<td></td>
<td>Universal 3D sample<br>SRP<br>0<br>Sample</td>
<td>11<br>Read more<br>PROJECT SETTINGS</td>
</tr>
<tr>
<td></td>
<td>High Definition 3D sample<br>SRP<br>C<br>Sample</td>
<td>Project name<br>My project</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Create project<br>Cancel</td>
</tr>
</tbody>
</table>
<p>If you create a new project with the Universal 3D template, you might have to download the template for the first time.</p>
<p><strong>Note:</strong> The template ensures that your project is set to use a linear color space, which is required for calculating lighting correctly.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th>SampleScene - Test - Windows, Mac, Linux - Unity 6 Preview (6000.0.18f1) <Metal></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Unity 6 PREVIEW   2 NL + Asset Store ▼ 0</td>
<td></td>
<td>&gt; 11 11</td>
<td></td>
<td>ල<br>Q Layout ▼<br>4</td>
</tr>
<tr>
<td>a :<br>= Hierarchy</td>
<td># Scene<br>Game</td>
<td>TwoToneDither<br>TintSG</td>
<td>11</td>
<td>O Inspector<br>· Lighting<br>a :</td>
</tr>
<tr>
<td>ഇ<br>+ ▼ a. All<br>▼ SampleScene<br>11<br>Main Camera<br>Directional Light<br>Global Volume</td>
<td>Game<br>Display 1 ▼ Free Aspect</td>
<td>▼ Scale @- 1x</td>
<td>Play Focused ▼ 卷 岭 四 Stats Gizn</td>
<td>URP Empty<br>1<br>Template</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Welcome to the Universal Render<br>Pipeline<br>This template includes the settings and<br>assets you need to start creating with the<br>Universal Render Pipeline.</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>URP Documentation<br>Read more about URP</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Forums<br>Get answers and support</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Report bugs<br>Submit a report</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Remove Readme Assets</td>
</tr>
<tr>
<td>Project<br>Console<br>++<br>★ Favorites</td>
<td>* Project Settings<br>Rendering Debugger</td>
<td>Render Graph Viewer<br>a</td>
<td>a :<br>2 3 # # 0 22</td>
<td></td>
</tr>
<tr>
<td>Q All Materials<br>Q All Models<br>Q All Prefabs</td>
<td>Assets<br>InputSystem_Actions<br>Q Readme<br>Scenes</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Assets<br>Scenes<br>Settings<br>&gt; Tutorialinfo</td>
<td>Settings<br>TutorialInfo</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Packages</td>
<td></td>
<td></td>
<td></td>
<td>Asset Labels<br>0</td>
</tr>
<tr>
<td></td>
<td>Assets/Readme.asset</td>
<td></td>
<td></td>
<td>· None ·<br>AssetBundle<br>None</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>※ &amp; ◎</td>
</tr>
</tbody>
</table>
<p>This template is empty but has URP and its assets preconfigured and installed.</p>
<p><span id="page-9-0"></span>Go to <strong>Edit &gt; Project Settings</strong>, and open the <strong>Graphics</strong> panel. You&#39;ll see the <strong>Default Render Pipeline Asset.</strong> This <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/universalrp-asset.html">URP Asset</a> controls the global rendering and quality settings of a project and creates the rendering pipeline instance. Meanwhile, the rendering pipeline instance contains intermediate resources and the render pipeline implementation.</p>
<p><strong>PC_RPAsset</strong> is the default URP Asset selected, but you can switch to <strong>Mobile_RPAsset</strong> for an asset more suited to a device with more restricted resources.</p>
<table>
<thead>
<tr>
<th>Project</th>
<th>Project Settings</th>
<th></th>
<th>E Console</th>
<th>Rendering Debugger</th>
<th>Render Graph Viewer</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Q</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Adaptive Performance<br>Audio<br>Burst AOT Settings<br>Editor</td>
<td></td>
<td></td>
<td>Graphics</td>
<td></td>
<td></td>
<td>0</td>
<td>12</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>1<br>Set Default Render Pipeline Asset</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Graphics</td>
<td></td>
<td></td>
<td>Set the Default Render Pipeline Asset that Unity uses when you don&#39;t have asset in the active Quality</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Input Manager<br>Input System Package<br>Settings</td>
<td></td>
<td>Level.</td>
<td>Default Render Pipeline</td>
<td></td>
<td></td>
<td>D PC_RPAsset (Universal Render Pipeline Asset)</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>Memory Settings<br>Package Manager</td>
<td></td>
<td></td>
<td>Shader Stripping<br>Lightmap Modes</td>
<td></td>
<td>Automatic</td>
<td></td>
<td>&gt;</td>
<td></td>
</tr>
<tr>
<td>Physics</td>
<td></td>
<td>Fog Modes</td>
<td></td>
<td></td>
<td>Automatic</td>
<td></td>
<td>&gt;</td>
<td></td>
</tr>
<tr>
<td>Settings<br>Physics 2D</td>
<td></td>
<td></td>
<td>Instancing Variants</td>
<td></td>
<td>Strip Unused</td>
<td></td>
<td>&gt;</td>
<td></td>
</tr>
<tr>
<td>Player</td>
<td></td>
<td></td>
<td>BatchRendererGroup Variants</td>
<td></td>
<td>Strip if no Entities Graphics package</td>
<td></td>
<td>&gt;</td>
<td></td>
</tr>
<tr>
<td>Preset Manager</td>
<td>P</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>&gt;</td>
</tr>
</tbody>
</table>
<p>The Graphics panel in Project Settings</p>
<h3 id="importing-e-book-sample-scenes">Importing e-book sample scenes</h3>
<p>You can clone the repository from <a href="https://github.com/NikLever/Unity-URP-Cookbook-Unity6">here</a> or download the code in a zip file and unzip it.</p>
<table>
<thead>
<tr>
<th>&gt; 0 tags</th>
<th></th>
<th>Go to file</th>
<th>Code -</th>
<th>About</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Local</td>
<td>Codespaces</td>
<td></td>
<td>Examples from the Unity URP Cookbook<br>e-book</td>
</tr>
<tr>
<td>Raymarching</td>
<td>&gt;-<br>Clone</td>
<td></td>
<td>3</td>
<td>13<br>O stars</td>
</tr>
<tr>
<td>LUT</td>
<td>HTTPS<br>GitHub CLI</td>
<td></td>
<td></td>
<td>0<br>1 watching</td>
</tr>
<tr>
<td>Raymarching</td>
<td><a href="https://github.com/NikLever/Unity-URP-Coo">https://github.com/NikLever/Unity-URP-Coo</a></td>
<td></td>
<td>10</td>
<td>ంది<br>0 forks</td>
</tr>
<tr>
<td>Initial commit</td>
<td>Use Git or checkout with SVN using the web URL.</td>
<td></td>
<td></td>
<td>Releases</td>
</tr>
<tr>
<td>Added Instancing</td>
<td>Open with GitHub Desktop</td>
<td></td>
<td></td>
<td>No releases published</td>
</tr>
<tr>
<td>Initial commit</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>ង្រ<br>Download ZIP</td>
<td></td>
<td>Packages</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>No packages published</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Languages</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>ShaderLab 68.4%<br>● C# 28.0%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>HLSL 3.6%<br>0</td>
</tr>
</tbody>
</table>
<p>The GitHub repository from where you can download the project by clicking the green Code button</p>
<p>Once the project is unzipped and downloaded, import it from the Unity Hub via <strong>Open &gt; Add project from disk.</strong></p>
<p><img src="_page_10_Figure_2.jpeg" alt=""></p>
<p>Import the sample project from Unity Hub.</p>
<p>It&#39;s important that you are working in the same version of the Editor as that used for the sample project. If the Editor versions don&#39;t match, the Hub will show a warning message about a missing Editor version. You can install the missing version from the blue button at the bottom right, as seen in this image.</p>
<table>
<thead>
<tr>
<th>Unity6E-book: Warning</th>
<th>×</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Missing Editor Version<br>il<br>To open your project, install Editor version 6000.0.4f1 or select a different version below.<br>Please note: using a different Editor version than the one your project was created with<br>may introduce risks.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MISSING VERSION</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>6000.0.4f1 @ Intel</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>6000.0.4f1 &amp; Silicon</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>INSTALLS</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>C anno n 19f1 &amp; cilinan</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ു Install Other Editor Version<br>Cancel Install Version 6000.0.4f1</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Install the version of the Unity Editor that matches any tutorial project you&#39;re following and/or downloading. This is easy to do via the Unity Hub.</p>
<p><img src="_page_11_Picture_1.jpeg" alt=""></p>
<h4 id="once-the-correct-editor-version-is-installed-you-will-be-able-to-open-the-project-as-normal-">Once the correct Editor version is installed, you will be able to open the project as normal.</h4>
<p>Each recipe is contained in a folder along with the steps and files referred to in this book.</p>
<h2 id="-span-id-page-12-0-span-stencils"><span id="page-12-0"></span>Stencils</h2>
<p><img src="_page_12_Picture_1.jpeg" alt=""></p>
<p>In the Made with Unity game <em><a href="https://tunicgame.com/">TUNIC</a></em> (created by Andrew Shouldice, TUNIC Team, 22nd Century Toys LLC, and Isometricorp Games Ltd., published by Finji), the main character&#39;s silhouette is drawn when props are blocking him. This effect can be achieved with Renderer Features in URP. It&#39;s also explained in this <a href="https://www.youtube.com/watch?v=3CpEn_mmr3o">video tutorial.</a></p>
<p><span id="page-13-0"></span>URP has two assets that control the final render, the <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/urp-universal-renderer.html">Universal Renderer Asset</a> and the <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/universalrp-asset.html">URP</a>  <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/universalrp-asset.html">Asset.</a> From the former, you can add <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/urp-renderer-feature.html">Renderer Features</a> to be injected into any stage of the rendering pipeline, such as:</p>
<ul>
<li>Rendering shadows</li>
<li>Rendering prepasses</li>
<li>Rendering G-buffer</li>
<li>Rendering Deferred lights</li>
<li>Rendering opaques</li>
<li>Rendering Skybox</li>
<li>Rendering transparents</li>
<li>Rendering post-processing</li>
</ul>
<h2 id="renderer-features">Renderer Features</h2>
<p>Renderer features provide you with ample opportunity to experiment with lighting and effects. This section will focus on Stencils, using only the bare minimum of required code.</p>
<p>To work along, open the sample scene via <strong>Scenes &gt; Renderer Features Stencils &gt; SmallRoom - Stencil</strong> in the Editor.</p>
<p><img src="_page_13_Picture_13.jpeg" alt=""></p>
<p>Stencils in action: As the magnifying glass moves over the desk, it can see through to reveal what is in the drawers.</p>
<p>As the above image shows, the aim in this example is to convert the lens of the magnifying glass so it allows you to to see through the desk, like an X-ray image. The approach uses a combination of Layer Masks, shaders, and Renderer Features. The first step is to change the material used by the lens, in this case a material called <strong>MaskMat,</strong> with a shader called <strong>Custom/StencilMask</strong>.</p>
<p><img src="_page_14_Picture_0.jpeg" alt=""></p>
<pre><code><span class="hljs-keyword">Shader </span><span class="hljs-string">"Custom/StencilMask"</span>
{
 Properties{}
    <span class="hljs-keyword">SubShader{
</span> Tags {
 <span class="hljs-string">"RenderType"</span> = <span class="hljs-string">"Opaque"</span>
 }
 Pass {
 ZWrite Off
 HLSLPROGRAM
 <span class="hljs-comment">#pragma vertex vert</span>
 <span class="hljs-comment">#pragma fragment frag</span>
 <span class="hljs-comment">#include </span>
<span class="hljs-string">"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"</span>
 struct Attributes
 {
 float4 positionOS : POSITION<span class="hljs-comment">;</span>
 }<span class="hljs-comment">;</span>
 struct Varyings
 {
 float4 positionHCS : SV_POSITION<span class="hljs-comment">;</span>
 }<span class="hljs-comment">;</span>
 Varyings vert(Attributes IN)
 {
 Varyings OUT<span class="hljs-comment">;</span>
 OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz)<span class="hljs-comment">;</span>
 return OUT<span class="hljs-comment">;</span>
 }
 half4 frag() : SV_Target
</code></pre><p><img src="_page_15_Picture_0.jpeg" alt=""></p>
<p> { return (half4)0; } ENDHLSL } } }</p>
<blockquote>
<p>Notice that Custom/StencilMask has the command ZWrite Off. In most cases, if you set ZWrite Off for an object, it will disappear because the object doesn&#39;t write its depth value into the depth buffer, so it won&#39;t obstruct objects that are behind it anymore. The object will still be rendered with the same order, but the drawn pixel content is being overridden by the objects behind. To clarify, setting ZWrite Off doesn&#39;t change the render order (but setting render queue index does). If you change its render queue index to a higher value than Geometry, then it will reappear. For this example, it&#39;s been left at 2000, the Geometry value.</p>
<p>The only action you want the lens to perform is to write a value to the Stencil buffer. Since you need to consider the stencil writes and not the output of the shader to the color buffer, you can disable the color writes, ColorMask 0. This is a slightly optimized approach, especially if you want this to work with the <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/rendering/deferred-rendering-path-landing.html">Deferred Rendering path</a> as the scene would be rendered before the lens mask.</p>
</blockquote>
<p> This example uses two custom layers, <strong>Mask</strong> and <strong>SeeThrough</strong>. The lens is in the Mask layer, while the desk, but not its children, is in the SeeThrough layer.</p>
<p>This scene uses the Renderer Data object named <strong>See Through Settings_Renderer</strong>, located in the same folder as the scene file, materials, and shader: <strong>Scenes &gt; Renderer Feature Stencils</strong>. The script attached to the Main Camera, AutoLoadPipelineAsset, ensures this is set as the Scriptable Render Pipeline Asset in <strong>Project Settings &gt; Graphics</strong>. Now let&#39;s check the settings for this asset.</p>
<table>
<thead>
<tr>
<th>V Auto Load Pipeline Asset (Script)</th>
<th></th>
<th>2 T</th>
</tr>
</thead>
<tbody>
<tr>
<td>Script</td>
<td>AutoLoadPipelineAsset</td>
<td>0</td>
</tr>
<tr>
<td>Pipeline Asset</td>
<td>&amp; SeeThroughSettings (Universal Render Pipeline ල</td>
</tr>
</tbody>
</table>
<p>Pipeline Asset set for the Main Camera &gt; Auto Load Pipeline Asset script</p>
<p>Select the <strong>SeeThrough Settings_Renderer</strong> via <strong>Scenes &gt; Renderer Feature Stencils</strong>. The first setting changed from the default is the <strong>Opaque Layer Mask</strong>. Note that this excludes Mask and SeeThrough.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>See Through Settings_Renderer (Universal Rende ® :</th>
<th>Open</th>
</tr>
</thead>
<tbody>
<tr>
<td>Filtering<br>Opaque Layer Mask Mixed</td>
<td></td>
<td></td>
<td>9</td>
</tr>
<tr>
<td>Transparent Layer M</td>
<td></td>
<td>Nothing<br>Everything</td>
<td>A</td>
</tr>
<tr>
<td>Rendering<br>Rendering Path<br>Depth Priming Mo<br>Depth Texture Mode<br>Depth Attachment Fi</td>
<td>1</td>
<td>Default<br>TransparentFX<br>Ignore Raycast<br>Water</td>
<td>P<br>D<br>V</td>
</tr>
<tr>
<td>Depth Texture Forma<br>RenderPass</td>
<td></td>
<td>UI<br>SeeBehind<br>Mask</td>
<td>P</td>
</tr>
<tr>
<td>Native RenderPass</td>
<td></td>
<td>See Through</td>
<td></td>
</tr>
<tr>
<td>Shadows<br>Transparent Receive</td>
<td></td>
<td>Toon<br>sland</td>
</tr>
</tbody>
</table>
<p>Change the Opaque Layer Mask in the See Through Settings_Renderer.</p>
<p>In the Renderer Features list in the Inspector, there are two Render Objects features named <strong>Mask</strong> and <strong>SeeThrough</strong>. If you disable the SeeThrough option, the desk disappears. This happens because, as part of a filtered-out layer that&#39;s using the Opaque Layer Mask, it&#39;s not a part of the default render – it only gets rendered because of the Render Objects feature.</p>
<table>
<thead>
<tr>
<th>V √ Mask (Render Objects)</th>
<th></th>
<th></th>
<th>9</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>Mask</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Event</td>
<td>BeforeRenderingOpaques</td>
<td></td>
<td></td>
<td>&gt;</td>
</tr>
<tr>
<td>Filters<br>&gt;</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Queue</td>
<td>Opaque</td>
<td></td>
<td></td>
<td>V</td>
</tr>
<tr>
<td>Layer Mask</td>
<td>Mask</td>
<td></td>
<td></td>
<td>P</td>
</tr>
<tr>
<td>LightMode Tags<br>A</td>
<td></td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr>
<td>V Overrides</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Override Mode</td>
<td>Material</td>
<td></td>
<td></td>
<td>V</td>
</tr>
<tr>
<td>Material</td>
<td>None (Material)</td>
<td></td>
<td></td>
<td>O</td>
</tr>
<tr>
<td>Pass Index</td>
<td>O</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Depth</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Stencil</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Value</td>
<td></td>
<td>1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Compare Functio Always</td>
<td></td>
<td></td>
<td></td>
<td>P</td>
</tr>
<tr>
<td>Pass</td>
<td>Replace</td>
<td></td>
<td></td>
<td>P</td>
</tr>
<tr>
<td>Fail</td>
<td>Keep</td>
<td></td>
<td></td>
<td>&gt;</td>
</tr>
<tr>
<td>Z Fail</td>
<td>Keep</td>
<td></td>
<td></td>
<td>D</td>
</tr>
<tr>
<td>Camera</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Settings for Mask (Render Objects)</p>
<p>The image above shows that Mask is set to use the Event <strong>BeforeRenderingOpaques</strong> and be filtered so it only works on rendered pixels in the Mask Layer. In the Overrides panel, the <strong>Stencil</strong> option is enabled. The value it will save to the buffer is 1. To make sure this write happens, the <strong>Compare Function</strong> is set to <strong>Always</strong>, and <strong>Pass</strong> is set to <strong>Replace</strong> so it always replaces the existing value. <strong>Fail</strong> and <strong>Z Fail</strong> are set to <strong>Keep</strong>.</p>
<p>URP will attempt to render the Mask Layer. Since no override material is set, it will use the materials defined by the objects in this Mask Layer, which is just the lens with the MaskMat material and the StencilMask shader. Setting Compare Function to <strong>Always</strong> and Pass to <strong>Replace</strong> ensures that the Stencil buffer is wherever the lens is in vision, with the value for each pixel set to 1.</p>
<table>
<thead>
<tr>
<th>V &gt; See Through (Render Objects)</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>SeeThrough</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Event</td>
<td>AfterRenderingOpaques</td>
<td></td>
<td>&gt;</td>
</tr>
<tr>
<td>V Filters</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Queue</td>
<td>Opaque</td>
<td></td>
<td>&gt;</td>
</tr>
<tr>
<td>Layer Mask</td>
<td>See Through</td>
<td></td>
<td>V</td>
</tr>
<tr>
<td>LightMode Tags<br>A</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr>
<td>T Overrides</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Override Mode</td>
<td>Material</td>
<td></td>
<td>V</td>
</tr>
<tr>
<td>Material</td>
<td>None (Material)</td>
<td></td>
<td>O</td>
</tr>
<tr>
<td>Pass Index</td>
<td>O</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Depth</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Stencil</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Value</td>
<td>1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Compare Functic Not Equal</td>
<td></td>
<td></td>
<td>D</td>
</tr>
<tr>
<td>Pass</td>
<td>Keep</td>
<td></td>
<td>V</td>
</tr>
<tr>
<td>Fail</td>
<td>Keep</td>
<td></td>
<td>V</td>
</tr>
<tr>
<td>Z Fail</td>
<td>Keep</td>
<td></td>
<td>V</td>
</tr>
<tr>
<td>Camera</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The settings for See Through (Render Objects)</p>
<p>Let&#39;s look at the second Render Objects Renderer Feature (shown above). This is set to use the Event <strong>AfterRenderingOpaques</strong>, meaning it will apply after the Stencil buffer has been set. Its <strong>Layer Mask</strong> is set to <strong>SeeThrough</strong> and <strong>Value</strong> set to <strong>1</strong>. If the Value 1 is found, the pixel shouldn&#39;t be rendered.</p>
<p>The Compare Function setting is set to <strong>Not Equal</strong>, while Pass, Fail, and Z Fail are all set to <strong>Keep</strong>. This Render Objects pass will only read from the Stencil buffer but not write to it. So this pass will render any pixel in the layer See Through, where the Stencil buffer does not contain the value 1. It leaves the default render only where the lens is. Try changing the Compare Function to <strong>Equal</strong> to flip the result so the desk appears in the lens only.</p>
<p><img src="_page_18_Picture_1.jpeg" alt=""></p>
<p>The effect of changing the Compare Function to Equal</p>
<p>Renderer Features are a great way to achieve dramatic custom effects.</p>
<h2 id="-span-id-page-19-0-span-instancing"><span id="page-19-0"></span>Instancing</h2>
<p><img src="_page_19_Picture_1.jpeg" alt=""></p>
<p>The popular Made with Unity game <em><a href="https://genshin.hoyoverse.com/pc-launcher/?utm_source=EU_google_EUT2_search_20220719&amp;mhy_trace_channel=ga_channel&amp;new=1&amp;gclid=CjwKCAiAwc-dBhA7EiwAxPRylGcsg_43UUG55LGlMh3WR8vYBuSHby1XJ3T78jU-_0aD5VapH8kRWhoCGMcQAvD_BwE#/GI008">Genshin Impact</a></em>, by HoYoverse, features a vast open world with lush vegetation. It runs on all the major platforms, from mobile devices to the latest consoles. This section offers tips on how to recreate a similar grass effect in a performant way.</p>
<p>Exchanging data between the CPU and GPU is a major bottleneck in the rendering pipeline. If you have a model that needs to be rendered many times using the same geometry and material, then Unity provides some great tools to do so, which are covered in this chapter.</p>
<h2 id="-span-id-page-20-0-span-gpu-resident-drawer-and-gpu-occlusion-culling"><span id="page-20-0"></span>GPU Resident Drawer and GPU occlusion culling</h2>
<p>Before starting on the instancing recipe, let&#39;s look at a generic solution new to Unity 6, GPU Resident Drawer, which is available via the Rendering section of the URP Asset<strong>.</strong></p>
<p>The <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/gpu-resident-drawer.html">GPU Resident Drawer</a> is a GPU-driven rendering system that&#39;s designed to optimize CPU time. It enables GameObjects to take advantage of the</p>
<p>BatchRenderGroup API, so they can benefit from its faster batching and improved CPU performance.</p>
<p>With GPU Resident Drawer, you can author your game using GameObjects, and when processed, they will be ingested and rendered via a special fast path that handles better instancing. When you enable this feature, games that are CPU-bound due to a high number of draw calls will see a reduction in this bottleneck as the amount of draw calls is reduced.</p>
<table>
<thead>
<tr>
<th>Statistics</th>
<th></th>
<th>2</th>
<th></th>
<th></th>
<th>Open</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Audio:</td>
<td>Rendering</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Level: - 74.8 dB<br>Clipping: 0.0%</td>
<td>DSP load: 0.1%<br>Stream load: 0.0%</td>
<td></td>
<td>Renderer List</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Graphics:<br>- FPS (Playmode Off<br>ender thread 27.7ms<br>Batches: 3569 Save<br>d by batching: 0</td>
<td></td>
<td></td>
<td>0 @ PC_High_Renderer (Universal Renderer Data)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>1 @ PC_High_ScreenRenderer (Universal Renderer Data)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>+</td>
<td></td>
</tr>
<tr>
<td>Screen: 2128x1399 - 34.1 MB<br>SetPass calls: 185</td>
<td>Shadow casters: 736</td>
<td>Depth Texture</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Visible skinned meshes: 0</td>
<td></td>
<td>Opaque Texture</td>
<td></td>
<td>1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Animation components playing: 0<br>Animator components playing: 0</td>
<td></td>
<td></td>
<td>Opaque Downsampling</td>
<td>2x Bilinear</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Terrain Holes</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>GPU Resident Drawer</td>
<td>Disabled</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>SRP Batcher</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Dynamic Batching</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Debug Level</td>
<td></td>
<td>Disabled</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Store Actions</td>
<td></td>
<td>Auto</td>
<td>V</td>
<td></td>
</tr>
<tr>
<td>Statistics</td>
<td></td>
<td>2</td>
<td></td>
<td></td>
<td>Open</td>
<td></td>
</tr>
<tr>
<td>Audio:</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Level: -74.8 dB<br>DSP load: 0.1%</td>
<td></td>
<td>T Rendering</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Clipping: 0.0%</td>
<td>Stream load: 0.0%</td>
<td>Renderer List</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Granhies:</td>
<td>- FPS (Playmode Off</td>
<td></td>
<td>0 @ PC_High_Renderer (Universal Renderer Data)</td>
<td></td>
<td>0<br>Default</td>
<td></td>
</tr>
<tr>
<td>CPU: main 25.7ms   ender thread 8.9ms<br>Batches: 506 Saved by batching: 0</td>
<td></td>
<td></td>
<td>1 @ PC_High_ScreenRenderer (Universal Renderer Data)</td>
<td></td>
<td>0<br>Set Default</td>
<td></td>
</tr>
<tr>
<td>INDEZAREN VENS: 219.2k</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>+</td>
<td></td>
</tr>
<tr>
<td>Screen: 2128x1399 - 34.1 MB<br>SetPass calls: 183</td>
<td>Shadow casters: 339</td>
<td>Depth Texture</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Visible skinned meshes: 0</td>
<td></td>
<td>Opaque Texture</td>
<td></td>
<td>V</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Animation components playing: 0</td>
<td></td>
<td></td>
<td>Opaque Downsampling</td>
<td>2x Bilinear</td>
<td>œ</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Animator components playing: 0</td>
<td></td>
<td>Terrain Holes</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>GPU Resident Drawer</td>
<td>Instanced Drawing</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Small-Mesh Screen-Percentage</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>GPU Occlusion Culling</td>
<td>&gt;</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>SRP Batcher</td>
<td></td>
<td>&gt;</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Dynamic Batching</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Debug Level</td>
<td></td>
<td>Disabled</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Charo Actione</td>
<td></td>
<td>Auto</td>
<td></td>
</tr>
</tbody>
</table>
<p>The GPU Resident Drawer and GPU occlusion culling options available via the URP Asset in Unity 6</p>
<p>Notice from the screengrabs above that the batches necessary to render the garden environment from the <a href="https://discussions.unity.com/t/introducing-the-new-urp-3d-sample/929894">URP 3D Sample</a> in Editor mode is 3569. When the GPU Resident Drawer is set to <strong>Instanced Drawing</strong> this drops to just 506.</p>
<p>The improvements you will see are dependent on the scale of your scenes and the amount of instancing you utilize. The more instanceable objects you render, the bigger the benefits gain.</p>
<p>GPU Resident Drawer is targeted for MeshRenderers. It will not handle Skinned Mesh Renderers, VFX Graphs, particle systems, or similar effects renderers. No changes to your existing content are required to take advantage of it. Also note that if you&#39;re using custom shaders, you&#39;ll need to ensure they&#39;re compatible with DOTS instancing; see <a href="https://github.com/Unity-Technologies/EntityComponentSystemSamples/blob/master/GraphicsSamples/URPSamples/Assets/SampleScenes/6. Misc/SimpleDotsInstancingShader/SceneAssets/CustomDotsInstancingShader.shader">this simplified</a>  <a href="https://github.com/Unity-Technologies/EntityComponentSystemSamples/blob/master/GraphicsSamples/URPSamples/Assets/SampleScenes/6. Misc/SimpleDotsInstancingShader/SceneAssets/CustomDotsInstancingShader.shader">version</a> as an example.</p>
<p>Note: GPU Resident Drawer requires the Forward+ renderer, and <strong>Project Settings &gt; Graphics &gt; BatchRendererGroup Variants</strong> needs to be set to <strong>Keep All</strong>.</p>
<table>
<thead>
<tr>
<th>Graphics</th>
<th></th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>Set Default Render Pipeline Asset</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Set the Default Render Pipeline Asset that Unity uses when you don&#39;t have asset in the active Quality Level.</td>
<td></td>
</tr>
<tr>
<td>Default Render Pipeline</td>
<td>PC_RPAsset - Dither (Universal Render Pipeline Asset)</td>
<td>0</td>
</tr>
<tr>
<td>Shader Stripping</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Lightmap Modes</td>
<td>Automatic</td>
<td></td>
</tr>
<tr>
<td>Fog Modes</td>
<td>Automatic</td>
<td></td>
</tr>
<tr>
<td>Instancing Variants</td>
<td>Strip Unused</td>
<td></td>
</tr>
<tr>
<td>BatchRendererGroup Variants</td>
<td>Keep All</td>
<td></td>
</tr>
<tr>
<td>Shader Loading<br>Log Shader Compilation</td>
<td>Strip if no Entities Graphics package<br>Strip All<br>V Keep All</td>
</tr>
</tbody>
</table>
<p>When you enable GPU Resident Drawer, <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/gpu-culling.html">GPU occlusion culling</a> also becomes available as an option. This uses a GPU-driven approach to ensure you don&#39;t render things you can&#39;t see on the screen; depending on your content, it can reduce CPU work dramatically.</p>
<p><img src="_page_21_Figure_5.jpeg" alt=""></p>
<p>Viewing the Occlusion Test using the Rendering Debugger</p>
<p><span id="page-22-0"></span><img src="_page_22_Picture_0.jpeg" alt=""></p>
<p>To see if GPU occlusion culling is effective for your scene go to <strong>Window &gt; Analysis &gt; Rendering Debugger</strong>, and select <strong>GPU Resident Drawer &gt; Occlusion Test Overlay</strong>. This displays a heatmap of culled instances. The heatmap displays blue if there are few culled instances, through to red if there are many culled instances. If you enable this setting, culling might be slower.</p>
<h3 id="instancing">Instancing</h3>
<p>A field full of grass will be used to illustrate the concept of <a href="https://www.google.com/url?q=https://docs.unity3d.com/6000.0/Documentation/Manual/GPUInstancing.html&amp;sa=D&amp;source=docs&amp;ust=1732790230224144&amp;usg=AOvVaw3160DtiCnehcU3JiX-nbyw">instancing.</a> It&#39;s far from photorealistic but sufficient to illustrate the techniques involved. You&#39;ll find the example in the <strong>Scenes &gt; Instancing</strong> folder.</p>
<p><strong>Note:</strong> Thanks go to the author of the article, <a href="https://prog.world/making-grass-in-unity-with-gpu-instancing/">&quot;Making Grass in Unity with GPU Instancing,</a>&quot; for the assets.</p>
<p><img src="_page_22_Picture_6.jpeg" alt=""></p>
<p>A field of grass rendered using an SRP Batcher-compatible material</p>
<p>To start, you need a single blade of grass and just two triangles, to keep things simple. The UV is set so the base of each grass blade has a V value of <strong>0</strong> and the tip a V value of <strong>1</strong>. You can use this to offset the tip vertex to simulate wind.</p>
<p><img src="_page_22_Picture_9.jpeg" alt=""></p>
<p>Grass blade model and UV</p>
<h2 id="-span-id-page-23-0-span-srp-batcher"><span id="page-23-0"></span>SRP Batcher</h2>
<p>Take a look at the Shader Graph subgraph in the folder <strong>Scenes &gt; Instancing &gt; Common &gt; Grass Wave.</strong> The aim of this is to perturb the X value of the object&#39;s vertex based on WindSpeed, WindShiftStrength, and WindStrength. To ensure that all the grass blades behave slightly differently, a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Simple-Noise-Node.html">Noise</a> node is used in the subgraph called <strong>Perturb Grass</strong>. The vertex Y and Z positions are passed directly to the output, but the offset for the X value is processed using a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Lerp-Node.html">Lerp</a> node.</p>
<p>The T input, which controls the interpolation, comes from the UV&#39;s V value. At the base of the grass blade, this is 0, meaning the result of lerp will be the A input to the lerp, which is the modeled position. The tip of the blade V is 1, ensuring that the result of the lerp is the B input, the processed offset.</p>
<p><img src="_page_23_Figure_4.jpeg" alt=""></p>
<p>The Grass Wave subgraph</p>
<p>Now that you have a method of deforming each blade, it&#39;s time to turn this into a complete shader that you can use as the material shader for each blade of grass.</p>
<p>Take a look in the folder <strong>Scenes &gt; Instancing &gt; 1 - SRP Batcher &gt; SRP Batcher Shader</strong>. This is a simple shader, just the Grass Wave subgraph controlling the Vertex &gt; Position and a Sample Texture 2D acting as the base color input for the fragment shader.</p>
<p><img src="_page_24_Picture_0.jpeg" alt=""></p>
<p>Now, let&#39;s use the following code to populate a field of grass.</p>
<pre><code>_startPosition = -_fieldSize / <span class="hljs-number">2.0</span>f;
_cellSize = <span class="hljs-keyword">new</span> <span class="hljs-type">Vector2</span>(_fieldSize.x / GrassDensity, _fieldSize.y / GrassDensity);

<span class="hljs-keyword">var</span> grassEntities = <span class="hljs-keyword">new</span> <span class="hljs-type">Vector2</span>[GrassDensity, GrassDensity];
<span class="hljs-keyword">var</span> halfCellSize = _cellSize / <span class="hljs-number">2.0</span>f;

<span class="hljs-keyword">for</span> (<span class="hljs-keyword">var</span> i = <span class="hljs-number">0</span>; i &lt; grassEntities.GetLength(<span class="hljs-number">0</span>); i++) {
     <span class="hljs-keyword">for</span> (<span class="hljs-keyword">var</span> j = <span class="hljs-number">0</span>; j &lt; grassEntities.GetLength(<span class="hljs-number">1</span>); j++) {
 grassEntities[i, j] =
 <span class="hljs-keyword">new</span> <span class="hljs-type">Vector2</span>(_cellSize.x * i + _startPosition.x, 
 _cellSize.y * j + _startPosition.y) + 
 <span class="hljs-keyword">new</span> <span class="hljs-type">Vector2</span>( Random.Range(-halfCellSize.x, halfCellSize.x),
 Random.Range(-halfCellSize.y, halfCellSize.y));
     }
}
_abstractGrassDrawer.Init(grassEntities, _fieldSize);
</code></pre><p>Looking more closely at this code example you see:</p>
<ul>
<li>_fieldSize is (40, 40).</li>
<li>_startPosition is (-20, -20).</li>
<li>GrassDensity is set to 250 in the GitHub sample.</li>
<li>cellSize is (0.16, 0.16).</li>
<li>Two loops are iterated through setting each element of the _grassEntities 2D array.</li>
<li>Base position for each blade is _startPosition plus the current cell; then a small random factor is introduced.</li>
<li>_abstractGrassDrawer is a base class for two versions of using the grasspopulating code.<ul>
<li>For the initial version, ignore GPU Instancing and see how well SRP batcher handles the problem by opening and running the scene <strong>Scenes &gt; Instancing &gt; 1 - SRP Batcher &gt; 1 - SRP</strong>.</li>
<li>First, you need to populate the scene with the grass blade model Prefab, at each position in the grassEntities 2D Array. The code is in the file <strong>Scenes &gt; Instancing &gt; Scripts &gt; GameObjectGrassDrawer.cs</strong>.</li>
</ul>
</li>
</ul>
<p><span id="page-25-0"></span><img src="_page_25_Picture_0.jpeg" alt=""></p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">override</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Init</span>(<span class="hljs-params">Vector2[,] grassEntities, Vector2 fieldSize</span>) </span>{
    _grassEntities = <span class="hljs-keyword">new</span> GameObject[grassEntities.GetLength(<span class="hljs-number">0</span>),
 grassEntities.GetLength(<span class="hljs-number">1</span>)];
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">var</span> i = <span class="hljs-number">0</span>; i &lt; grassEntities.GetLength(<span class="hljs-number">0</span>); i++) {
 <span class="hljs-keyword">for</span> (<span class="hljs-keyword">var</span> j = <span class="hljs-number">0</span>; j &lt; grassEntities.GetLength(<span class="hljs-number">1</span>); j++) {
 _grassEntities[i, j] = 
 Instantiate(_grassPrefab,
 <span class="hljs-keyword">new</span> Vector3(
 grassEntities[i, j].x, 
                           <span class="hljs-number">0.0</span>f, 
                           grassEntities[i, j].y),
 Quaternion.identity);
 }
    }
}
</code></pre><p>Here, you iterate over the grassEntities array using Instantiate to create a new GameObject from the assigned prefab. It works but dramatically impacts the frame rate for the scene. You can see from the image of the grass field on <a href="#page-28-0">page 29</a> that the frame rate is a sluggish 29 fps for 62,500 blades, running on a 2022 MacBook Air with the following specs:</p>
<ul>
<li>Built-in Retina Display</li>
<li>Processor: Apple M2 2022</li>
<li>Memory: 8 GB</li>
</ul>
<p>How can you optimize the scene?</p>
<p>Note: For a non-square terrain, you could create a draw tool saving each blade position in a list. For example, <a href="https://bronsonzgeb.com/index.php/2021/08/08/unity-editor-tools-the-place-objects-tool/">this blog post</a> by game developer Bronson Zgeb explains how to build a tool to streamline placing objects in the scene every time you click in it.</p>
<h4 id="-gpu-instancing-"><strong>GPU Instancing</strong></h4>
<p>One optimization technique is to enable <a href="https://docs.unity3d.com/Manual/GPUInstancing.html">GPU instancing.</a> Look at <strong>Scenes &gt; Instancing &gt; 2 - GPU Instancing &gt; 2 - GPU Instancing</strong> from the GitHub samples for an example of this technique.</p>
<p>A material setting called <strong>Enable GPU Instancing</strong> instructs the renderer to batch any models that use the same material, thereby reducing the number of draw calls. The setting is available in the Advanced Options panel.</p>
<p>The SRP Batcher and GPU Instancing are mutually exclusive. When using URP, if a material is compatible with the SRP Batcher, then SRP Batcher will be used, even if Enable GPU Instancing is selected. A shader created with Shader Graph is compatible with SRP Batcher</p>
<p><img src="_page_26_Picture_0.jpeg" alt=""></p>
<p>by default. To disable SRP Batcher compatibility, select the Shader Graph that will create the HLSL shader, and click on <strong>View Generated Shader</strong> in the Inspector.</p>
<p><img src="_page_26_Picture_2.jpeg" alt=""></p>
<p>Generating an HLSL shader from Shader Graph</p>
<p>The shader will be created, placed in the Temp folder, and opened in your chosen text or code editor. Change the Shader name to: Shader &quot;Custom/GPU Instancing Shader&quot;</p>
<p>Then search for CBUFFER, and comment out the CBUFFER macros:</p>
<pre><code>// Graph Properties
//CBUFFER_START(UnityPerMaterial)
 float4 _MainTexture_TexelSize<span class="hljs-comment">;</span>
 half _WindShiftStrength<span class="hljs-comment">;</span>
 half _WindSpeed<span class="hljs-comment">;</span>
 half _WindStrength<span class="hljs-comment">;</span>
//CBUFFER_END
</code></pre><p>Save the shader in Assets.</p>
<table>
<thead>
<tr>
<th># √ Grass Field (Script)</th>
<th></th>
<th>3</th>
<th>14</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Script</td>
<td># GrassField</td>
<td></td>
<td></td>
<td>O</td>
</tr>
<tr>
<td>Abstract Grass Drawer</td>
<td>Ground (Game Object Grass Drawer)</td>
<td></td>
<td></td>
<td>O</td>
</tr>
<tr>
<td>Field Size</td>
<td>Y 40<br>× 40</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>#<br>Game Object Grass Drawer (Script)</td>
<td></td>
<td>?</td>
<td>14</td>
<td></td>
</tr>
<tr>
<td>Script</td>
<td>GameObjectGrassDrawer</td>
<td></td>
<td></td>
<td>0</td>
</tr>
<tr>
<td>Grass Prefab</td>
<td>GPU Instancing Grass Prefab</td>
<td></td>
<td></td>
<td>O</td>
</tr>
</tbody>
</table>
<p>Scripts assigned to the Ground GameObject in the GPU Instancing scene</p>
<p><span id="page-27-0"></span><img src="_page_27_Picture_0.jpeg" alt=""></p>
<p>Notice the GPU Instancing scene uses the same version of Abstract Grass Drawer as the SRP Batcher scene. The only difference is the <strong>GameObjectGrassDrawer</strong> version in GPU Instancing is assigned a different prefab with a material that uses the GPU Instancing shader.</p>
<p><img src="_page_27_Picture_3.jpeg" alt=""></p>
<p>GPU Instancing Shader is not compatible with SRP Batcher</p>
<p>If you check the GPU Instancing shader in the Inspector, you can see it&#39;s not compatible with SRP Batcher.</p>
<p>Any change to the graph that you used to generate the code will necessitate repeating the customization steps:</p>
<ul>
<li><ol>
<li>View Generated Shader or Regenerate.</li>
</ol>
</li>
<li><ol>
<li>Edit the Shader name.</li>
</ol>
</li>
<li><ol>
<li>Comment out the CBUFFER macros.</li>
</ol>
</li>
<li><ol>
<li>Save to Assets.</li>
</ol>
</li>
</ul>
<p>However, after all this work, the testing shows only a marginal improvement over SRP Batcher, probably due to being CPU bound. There has to be a better way.</p>
<h2 id="rendermeshprimitives">RenderMeshPrimitives</h2>
<p>The Unity <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/Graphics.html">Graphics API</a> has a number of methods for directly rendering a mesh by bypassing the need for a GameObject. The method used here is <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/Graphics.RenderMeshPrimitives.html">RenderMeshPrimitives,</a> a feature introduced in Unity LTS 2021. Prior to that, you would have needed to use <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/Graphics.DrawMeshInstancedProcedural.html">DrawMeshInstancedProcedural, w</a>hich is now marked as obsolete.</p>
<p>With RenderMeshPrimitives, you should use a material that sources the individual mesh position using a ComputeBuffer. You can see it in action by viewing the scene <strong>Scenes &gt; Instancing &gt; 3 - RenderMeshPrimitives &gt; 3 - RenderMeshPrimitives</strong>.</p>
<p><img src="_page_27_Figure_15.jpeg" alt=""></p>
<p>The Instancing scenes in the Project window</p>
<p><span id="page-28-0"></span><img src="_page_28_Picture_0.jpeg" alt=""></p>
<p>As you can see from the image of the grass field below, the improvement in frame rate is nothing short of remarkable – 377 fps. The scenes created with SRP Batcher and GPU Instancing were running at around 20 and 50 fps respectively.</p>
<p>The difference in this case is that the grass field is rendered using a single draw call.</p>
<table>
<thead>
<tr>
<th>Event #4: Draw Mesh (instanced) (1 draw calls, 62500 instances)</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Shader</td>
<td>Shader Graphs/Instanced Grass Shader Graph, SubShader #0</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Pass</td>
<td>Universal Forward</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Keywords</td>
<td>PROCEDURAL INSTANCING ON</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Blend</td>
<td>One Zero</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ZClip</td>
<td>True</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ZTest</td>
<td>LessEqual</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ZWrite</td>
<td>On</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Cull</td>
<td>Back</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Conservative</td>
<td>False</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Frame Debugger stats for the grass field</p>
<p><img src="_page_28_Picture_6.jpeg" alt=""></p>
<p>The grass field is rendered using RenderMeshPrimitives.</p>
<p>You achieve this by making the positions of each blade a Material property. The data to render the blades resides on the GPU, which uses its parallelism to render the entire field at an optimal speed.</p>
<p>Let&#39;s review the code to generate the positions. You&#39;ll find it in the UpdatePositions method in the file <strong>Scenes &gt; Instancing &gt; Scripts &gt; InstancedGrassDrawer.cs</strong>.</p>
<p><img src="_page_29_Picture_0.jpeg" alt=""></p>
<pre><code>_positionsCount = _positions.Count<span class="hljs-comment">;</span>
_positionBuffer?.Release()<span class="hljs-comment">;</span>
if (_positionsCount == <span class="hljs-number">0</span>) return<span class="hljs-comment">;</span>
_positionBuffer = new ComputeBuffer(_positionsCount, <span class="hljs-number">8</span>)<span class="hljs-comment">;</span>
_positionBuffer.SetData(_positions)<span class="hljs-comment">;</span>
_instanceMaterial.SetBuffer(<span class="hljs-keyword">Shader.PropertyToID("PositionsBuffer"), </span>_positionBuffer)<span class="hljs-comment">;</span>
</code></pre><p>_positions holds a Vector2 List of grass positions. If _positionsBuffer exists, then you release it. If you&#39;re unfamiliar with a &quot;<strong>?</strong>&quot; following a variable, it&#39;s a null check, meaning it&#39;s shorthand for:</p>
<pre><code><span class="hljs-keyword">if</span> (positionsBuffer != <span class="hljs-keyword">null</span>) _positionsBuffer.<span class="hljs-keyword">Release</span>()
</code></pre><p>You create a ComputeBuffer that takes a count parameter and the byte size of each item. A Vector2 contains two floats. A single float is 32 bits or 4 bytes, making two floats 8 bytes. It&#39;s simple to populate a ComputeBuffer by using SetData passing the _positions List. Now you can use the SetBuffer method to copy this to the material. You&#39;ll access this buffer in the material using the name positionsBuffer.</p>
<p>Take a look at the graph in <strong>Scenes &gt; Instancing &gt; 3 - RenderMeshPrimitives &gt; Instanced Grass Shader</strong>.</p>
<p><img src="_page_29_Figure_6.jpeg" alt=""></p>
<p>Getting the vertex position from a ComputeBuffer</p>
<p><img src="_page_30_Picture_0.jpeg" alt=""></p>
<p>Starting at the bottom, you can see the <strong>Space</strong> parameter for Grass Mesh vertex position is set to <strong>World</strong>. But there&#39;s an important code block that needs adding whenever you use this technique: A #pragma is required by any meshes rendered using RenderMeshPrimitive. This is done using a custom function. Instead of sourcing the function from a file, you add a string:</p>
<pre><code>#<span class="hljs-keyword">pragma</span> instancing_options procedural:ConfigureProcedural
<span class="hljs-keyword">Out</span> = <span class="hljs-keyword">In</span>;
</code></pre><p>The code method now used by this shader to generate positional values will come from a function with the name ConfigureProcedural. Other than that, this <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Custom-Function-Node.html">Custom Function</a>  node simply passes its input, <strong>In</strong>, to its output, <strong>Out</strong>.</p>
<p>The heavy lifting is done in the Custom Function called ShaderGraphFunction, which is found in the file InstancedPosition, in the same folder as the scene file.</p>
<pre><code><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> defined(UNITY_PROCEDURAL_INSTANCING_ENABLED)</span>
StructuredBuffer&lt;float2&gt; PositionsBuffer;
<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span>
float2 <span class="hljs-built_in">position</span>;
<span class="hljs-keyword">void</span> ConfigureProcedural () {
      <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> defined(UNITY_PROCEDURAL_INSTANCING_ENABLED)</span>
      <span class="hljs-built_in">position</span> = PositionsBuffer[unity_InstanceID];
      <span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span>
}
<span class="hljs-keyword">void</span> ShaderGraphFunction_float (out float2 PositionOut) {
      PositionOut = <span class="hljs-built_in">position</span>;
}
</code></pre><p>The position is set using the ConfigureProcedural method and passed to the output using the ShaderGraphFunction for which the script has float and half versions.</p>
<p>At this point in the graph, the individual blade location is a float2 with the first float being the X value and the second the Z. A <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Split-Node.html">Split</a> node is used to convert this into the individual floats, and a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Combine-Node.html">Combine</a> node to move the second float to the third. The Split and Combine nodes call the individual floats RGBA not XYZW, but by moving G to B, you&#39;re effectively moving Y to Z. The blade and vertex positions are now established, and you can combine these to get the actual world position of the vertex.</p>
<p>With this shader ready, you now use it with a material that has the inputs WindSpeed, WindStrength, WindShiftStrength, and MainTexture, the same as those used by the SRP Batcher and GPU Instancing versions. The only difference is in how the position of each vertex is calculated. Refer back to the script InstancedGrassDrawer.cs to see how to render the grass blades. The variables in the script are initialized in the Init method called by the Awake method of the GrassField.cs script.</p>
<p><img src="_page_31_Picture_0.jpeg" alt=""></p>
<pre><code><span class="hljs-keyword">public</span> <span class="hljs-keyword">override</span> void Init(Vector2[,] grassEntities, Vector2 fieldSize) {
      _grassEntities = grassEntities;
      _grassBounds = <span class="hljs-keyword">new</span> <span class="hljs-type">Bounds</span>(transform.position, 
 <span class="hljs-keyword">new</span> <span class="hljs-type">Vector3</span>(fieldSize.x, <span class="hljs-number">0.0</span>f, fieldSize.y));
      _positions = <span class="hljs-keyword">new</span> <span class="hljs-type">List</span>&lt;Vector2&gt;();
      _renderParams = <span class="hljs-keyword">new</span> <span class="hljs-type">RenderParams</span>(_instanceMaterial);
      _renderParams.worldBounds = _grassBounds;
      _renderParams.shadowCastingMode = ShadowCastingMode.Off;
}
</code></pre><p>To use Graphics.RenderMeshPrimitives, you need a RenderParams instance. This is created from the assigned Material, _instanceMaterial. Two other properties are additionally assigned.</p>
<p>The actual rendering is done using the Update callback:</p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title">Update</span><span class="hljs-params">()</span> </span>{
      <span class="hljs-keyword">if</span> (_positionsCount == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span>;
      Graphics.RenderMeshPrimitives(_renderParams, _instanceMesh, <span class="hljs-number">0</span>, 
_positionsCount);
}
</code></pre><p>RenderMeshPrimitives takes four parameters, a RenderParams instance, the mesh to render, a submesh index, and a count value identifying how many copies to render. When using the shader, each copy will have a unique unity_InstanceID, which will have the value 0 to count -1.</p>
<p>Rendering using a <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/ComputeBuffer.html">ComputeBuffer</a> is a fast and fairly simple setup. By manipulating the _positionBuffer, you could mow the grass or blow it away. To avoid passing data between the CPU and the GPU, this is best handled with a <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/ComputeShader.html">ComputeShader</a>. You&#39;ll find more information about performance boosts with compute shaders in a later recipe.</p>
<h4 id="more-resources">More resources</h4>
<ul>
<li>The <a href="https://prog.world/making-grass-in-unity-with-gpu-instancing/">assets</a> for this recipe</li>
<li><a href="https://github.com/ColinLeung-NiloCat/UnityURP-MobileDrawMeshInstancedIndirectExample">Example project</a> using DrawMeshInstancedIndirect</li>
<li>GPU Instancing <a href="https://docs.unity3d.com/Manual/GPUInstancing.html">documentation</a></li>
<li>GPU Instancing <a href="https://catlikecoding.com/unity/tutorials/rendering/part-19/Rendering-19.pdf">article</a> from CatLikeCoding</li>
<li>Using <a href="https://www.udemy.com/course/compute-shaders/learn/lecture/22732855/?instructorPreviewMode=student_v4#overview">ComputeBuffers</a> for instancing</li>
</ul>
<h2 id="-span-id-page-32-0-span-toon-and-outline-shading"><span id="page-32-0"></span>Toon and outline shading</h2>
<p><img src="_page_32_Figure_1.jpeg" alt=""></p>
<p>The third-person action-shooter and Made with Unity game <em><a href="https://www.roll7.co.uk/rollerdrome">Rollerdrome</a></em>, by Roll7, has a distinctive art direction that makes the game look like a comic book, achieved with cel shading techniques. Don&#39;t miss this <a href="https://www.youtube.com/watch?v=G1NY0LKDqJo">interview</a> with the creators.</p>
<p><img src="_page_33_Picture_1.jpeg" alt=""></p>
<p>This recipe is based on common ways of creating a toon shader and an outline shader.</p>
<p>One scene, three different looks: Standard shading (left), simple toon shading (center), and toon shading (right)</p>
<p>Often used together, toon and outline shaders present two distinct challenges. The toon shader takes the color that would be created using a URPcompatible Lit shader, and ramps the output rather than allowing continuous gradients, thereby requiring a custom lighting model.</p>
<p><img src="_page_33_Picture_5.jpeg" alt=""></p>
<p>The scene with the simple ramped toon shader</p>
<p><span id="page-34-0"></span><img src="_page_34_Picture_0.jpeg" alt=""></p>
<h2 id="simple-toon-shading">Simple toon shading</h2>
<p>To see what the shader looks like, go to <strong>Scenes &gt; Toon Shading &gt; Simple Toon Shading</strong>. We&#39;ll split the challenge into two different shaders, with the first shader being the main color choice for each pixel in the model.</p>
<h4 id="-shading-"><strong>Shading</strong></h4>
<p>The simplest possible lighting model uses <a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/diffuse-lambertian-shading.html">Lambert lighting.</a> This lighting model simply takes the dot product of the light direction and the world space normal at this point. The image below shows the shader graph for this shader.</p>
<p><img src="_page_34_Figure_6.jpeg" alt=""></p>
<p>The Simple Toon Shading Shader Graph</p>
<p>Starting on the left is a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Normal-Vector-Node.html">Normal Vector</a> node and a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Main-Light-Direction-Node.html">Main Light Direction</a> node. These two vectors are put into a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Dot-Product-Node.html">Dot Product</a> node. The output from this will be 1 when the two vectors are inline and -1 when the two vectors are in opposite directions. For lighting you want the maximum when the two vectors are in opposite directions.</p>
<p><img src="_page_34_Figure_9.jpeg" alt=""></p>
<p>The Lambert Lighting model</p>
<p><span id="page-35-0"></span><img src="_page_35_Picture_0.jpeg" alt=""></p>
<p>A <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Remap-Node.html">Remap</a> node is used here. This useful node can convert an input value from a range to a new value based on another range. The <strong>In Min Max</strong> is set to <strong>-1, 1</strong> and the <strong>Out Min Max</strong> is set to <strong>1, 0</strong>. Now if the In value is -1, the Out will be 1 and if the In value is 1 the Out value is 0.</p>
<p>The graph has two properties: Texture and Shades. Shades is an integer value defining how many ramped steps are allowed in the range 0 to 1. You need a mechanic that will step the value rather than give a continuous interpolation. First, use a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Divide-Node.html">Divide</a> node to divide 1 by the Shades property. If Shades is 4 then at this point the output of the Divide node will be 0.25. Then, use a second Divide node, dividing the output of the Remap node by the output from the Shades Divide node. The effect is to change the range from [0, 1] to [0, Shades]. Use a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Round-Node.html">Round</a> node to convert the value to an integer value. At this point the value can only be 0, 1, …, Shades.</p>
<h4 id="-outlining-"><strong>Outlining</strong></h4>
<p>The simplest technique for adding outlines is to add a second pass that only renders backfacing polygons and uses a vertex shader that moves the vertex a small amount along the vertex normal. This shader is included in the GitHub samples via <strong>Scenes &gt; Toon Shading &gt; VertexOutline</strong>; its graph is shown here:</p>
<p><img src="_page_35_Figure_6.jpeg" alt=""></p>
<p>An outline shader using a back-facing vertex shift technique</p>
<p>The Normal Vector node, with Space set to <strong>Object</strong>, is fed into a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Multiply-Node.html">Multiply</a> node. This is multiplied by the <strong>Thickness</strong> value for the material. The output from this is added to the Object Position, moving the vertex position slightly out from the object modeled location. This is the input to the <strong>Vertex Position</strong> property. The shader property <strong>Universal &gt; Render Face</strong> is set to <strong>Back</strong> using the panel in <strong>Graph Inspector &gt; Graph Settings</strong>. A shader graph allows a single pass only so to add this to the render you need to add a second material using the GameObject Inspector.</p>
<p><span id="page-36-0"></span><img src="_page_36_Picture_0.jpeg" alt=""></p>
<p><img src="_page_36_Figure_2.jpeg" alt=""></p>
<p>Adding a second material</p>
<p>The scene in <strong>Scenes &gt; Toon Shading &gt; Simple Toon Shading</strong> shows the second material being used. View the material VertexOutline in the same folder, in the Inspector, and set Thickness to 0.02.</p>
<p>Using this technique is effective for simple convex shapes. But as shapes become more complex it does not create a consistent equal thickness line, as the image below demonstrates. Let&#39;s look at an alternative approach.</p>
<p><img src="_page_36_Picture_6.jpeg" alt=""></p>
<p>Outlining using an expanded back facing model: simple sphere (left), complex model (right)</p>
<h3 id="toon-shading">Toon shading</h3>
<p>To see what the alternative shader looks like, go to <strong>Scenes &gt; Toon Shading &gt; Toon Shading</strong>. As before, the shading task is split into two different shaders: Shading and outlining. The first step is to set the main color choice for each pixel in the model.</p>
<h4 id="-shading-"><strong>Shading</strong></h4>
<p>The only Shader Graph node for the Main Light is the <strong>Main Light Direction</strong> node.</p>
<p><strong>Note</strong>: In URP the Main Light is the Direction Light with the greatest intensity.</p>
<p><img src="_page_37_Picture_0.jpeg" alt=""></p>
<p>The Main Light Direction node was used in the simple toon shading recipe. To improve the simple version you can incorporate light color and shadows. Let&#39;s start by accessing the Main Light using a custom function, which you&#39;ll find in the file <strong>Shaders &gt; HLSL &gt; Custom Lighting. hlsl</strong>.</p>
<pre><code>void MainLight_float(float3 WorldPos, out float3 <span class="hljs-keyword">Direction, </span>out float3 Color, out 
float <span class="hljs-keyword">DistanceAtten, </span>out float <span class="hljs-keyword">ShadowAtten)
</span>{
<span class="hljs-comment">#ifdef SHADERGRAPH_PREVIEW</span>
 <span class="hljs-keyword">Direction </span>= float3(<span class="hljs-number">0</span>.<span class="hljs-number">5</span>, <span class="hljs-number">0</span>.<span class="hljs-number">5</span>, <span class="hljs-number">0</span>)<span class="hljs-comment">;</span>
 Color = <span class="hljs-number">1</span><span class="hljs-comment">;</span>
 <span class="hljs-keyword">DistanceAtten </span>= <span class="hljs-number">1</span><span class="hljs-comment">;</span>
 <span class="hljs-keyword">ShadowAtten </span>= <span class="hljs-number">1</span><span class="hljs-comment">;</span>
<span class="hljs-comment">#else</span>
     float4 <span class="hljs-keyword">shadowCoord </span>= TransformWorldToShadowCoord(WorldPos)<span class="hljs-comment">;</span>
 Light mainLight = GetMainLight(<span class="hljs-keyword">shadowCoord);
</span> <span class="hljs-keyword">Direction </span>= mainLight.<span class="hljs-keyword">direction;
</span> Color = mainLight.color<span class="hljs-comment">;</span>
 <span class="hljs-keyword">DistanceAtten </span>= mainLight.<span class="hljs-keyword">distanceAttenuation;
</span>     <span class="hljs-comment">#if !defined(_MAIN_LIGHT_SHADOWS) || defined(_RECEIVE_SHADOWS_OFF)</span>
 <span class="hljs-keyword">ShadowAtten </span>= <span class="hljs-number">1</span>.<span class="hljs-number">0</span>h<span class="hljs-comment">;</span>
     <span class="hljs-comment">#else</span>
 <span class="hljs-keyword">ShadowSamplingData </span><span class="hljs-keyword">shadowSamplingData </span>= GetMainLightShadowSamplingData()<span class="hljs-comment">;</span>
 float <span class="hljs-keyword">shadowStrength </span>= GetMainLightShadowStrength()<span class="hljs-comment">;</span>
 <span class="hljs-keyword">ShadowAtten </span>= SampleShadowmap(<span class="hljs-keyword">shadowCoord, </span>
 TEXTURE2D_ARGS(_MainLightShadowmapTexture,
 sampler_MainLightShadowmapTexture),
 <span class="hljs-keyword">shadowSamplingData, </span><span class="hljs-keyword">shadowStrength, </span>false)<span class="hljs-comment">;</span>
 <span class="hljs-comment">#endif</span>
<span class="hljs-comment">#endif</span>
}
</code></pre><p>It&#39;s good practice to add a block of code inside a #ifdef SHADERGRAPH_PREVIEW preprocessor directive that defines the behavior while creating the <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Create-Shader-Graph.html">Shader Graph Asset.</a> This specifies the values to default to in the graph preview window.</p>
<p>The WorldPos is converted into a shadow coordinate using the function TransformWorldToShadowCoord. The functions used in this code come from the <a href="https://github.com/Unity-Technologies/Graphics/blob/master/Packages/com.unity.render-pipelines.universal/ShaderLibrary/RealtimeLights.hlsl">Universal Render Pipeline package</a> and are available to custom functions in Shader Graph. When the function GetMainLight is used with a float4, the <strong>ShadowAttenuation</strong> property of the returned light is set. This is needed in the graph that uses this custom function.</p>
<p><img src="_page_38_Picture_0.jpeg" alt=""></p>
<p>This code is used in the Main Light subgraph (see image below), which you&#39;ll find in the folder <strong>Shaders &gt; Subgraphs</strong>. Let&#39;s review it.</p>
<p><img src="_page_38_Figure_3.jpeg" alt=""></p>
<p>Main Light subgraph</p>
<p>The <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Custom-Function-Node.html">Custom Function</a> node takes a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Position-Node.html">Position</a> node set to <strong>Absolute World</strong> as its only input. The function returns Direction, Color, DistanceAtten (which remains unused), and ShadowAtten. To allow for self shadowing, you&#39;ll need to get the dot product of the light direction and the World normal, and clamp this between 0 and 1. You don&#39;t want negative values.</p>
<p><img src="_page_39_Picture_0.jpeg" alt=""></p>
<p>Now that you have a way of accessing the Main Light, you can use it to create a simple toon shader. Take a look at <strong>Scenes &gt; Toon Shading &gt; Toon Shading</strong> to see the graph (also in image below).</p>
<p><img src="_page_39_Figure_3.jpeg" alt=""></p>
<p>Simple Toon graph</p>
<p>The first node is the Main Light subgraph. The ShadowAttenuation and SelfShadowing outputs are multiplied together. The trick is to pass this output into a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Sample-Gradient-Node.html?q=sample gradient">Sample Gradient</a> node that works with a ramped gradient, so light levels are not smooth, but instead jump in stages based on the gradient.</p>
<p>Taking a smoothly changing input and processing it with a gradient is another useful technique for a number of shading challenges. The rest of the graph combines light color with the ramped level, then combines this with the sampled texture to generate the color to use for the base color.</p>
<p><span id="page-40-0"></span><img src="_page_40_Picture_0.jpeg" alt=""></p>
<h4 id="-outlining-"><strong>Outlining</strong></h4>
<p>A more sophisticated technique for outlining uses <a href="https://en.wikipedia.org/wiki/Sobel_operator">Sobel Filter</a> edge detection. You can use a postprocessing technique for this. Imagine the current pixel being analyzed is the orange square in the image to the right. You want to check how a buffer for the orange pixel differs from the gray squares. The buffers to check are the Color G-buffer and the Normal G-buffer. If the variation meets a property intensity then you can consider the orange pixel to be on an edge boundary and update its color to an <strong>Outline Color</strong> property.</p>
<p><img src="_page_40_Figure_4.jpeg" alt=""></p>
<p>Sobel Filtering</p>
<p>To handle this you can create a Shader Graph using <strong>Create &gt; Shader Graph &gt; URP &gt; Full Screen Shader Graph.</strong>  Or you can change the behavior of an existing Shader Graph using the Graph Inspector which is accessed by clicking on the icon in the top right of the Shader Graph window (the &quot;i&quot; in a circle icon).</p>
<table>
<thead>
<tr>
<th>Graph Inspector</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision<br>Target Settings</td>
<td>Single</td>
<td></td>
</tr>
<tr>
<td>Active Targets</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Universal</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Universal</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Material</td>
<td>Fullscreen</td>
<td></td>
</tr>
<tr>
<td>Allow Material Override<br>Blend Mode<br>Depth Test<br>Depth Write<br>Enable Stencil<br>Reference<br>Read Mask<br>Write Mask<br>Comparison</td>
<td>Sprite Unlit<br>Fullscreen<br>Unlit<br>Canvas<br>Decal<br>Lit<br>Six-way Smoke Lit<br>Sprite Custom Lit<br>Sprite Lit</td>
</tr>
</tbody>
</table>
<p>The Material property drop-down in the Shader Graph Inspector</p>
<p>Take a look at <strong>Scenes &gt; Toon Shading &gt; Outline</strong> to see the Shader Graph. Let&#39;s look first at the <strong>Detect edges based on the world space normals</strong> section of the graph, starting on the left.</p>
<ul>
<li><ol>
<li>The first node is a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Screen-Node.html">Screen</a> node, which gives the rendered image size in pixels. You can convert this into a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Vector-2-Node.html">Vector2</a>.</li>
</ol>
</li>
<li><ol>
<li>Then you divide the <strong>Outline Thickness</strong> property by this vector using a Divide node. Now you have a value you can use as an offset value.</li>
</ol>
</li>
<li><ol>
<li>You use a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Split-Node.html">Split</a> node to access the x and y values of the offset value; the Split node provides RGBA outputs wherein r=x and g=y.</li>
</ol>
</li>
<li><ol>
<li>Create two Vector2s, one to give an offset in the x and the other in the y. The x offset has y=0 and the y offset has x=0. Now you can use <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Add-Node.html">Add</a> and <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Subtract-Node.html">Subtract</a> nodes to add and subtract the offset from the current <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Screen-Position-Node.html">Screen Position.</a></li>
</ol>
</li>
<li><ol>
<li>At this point you have four positions in screen space: [0,0] at the bottom left and [1,1] top right. Use four URP Sample Buffer nodes to get the world normal at this position.</li>
</ol>
</li>
</ul>
<p><img src="_page_41_Picture_0.jpeg" alt=""></p>
<ul>
<li><ol>
<li>Subtract the normal at the left from the normal at the right and get the <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Length-Node.html">Length</a> of this new vector, and then subtract the lower normal from the upper normal to get the Length of the vector.</li>
</ol>
</li>
<li><ol>
<li>You now have a scalar value of the difference between the pixel to the left of the current screen pixel and the pixel to the right and between the pixel below and the one above.</li>
</ol>
</li>
<li><ol>
<li>Use an Add node to sum these values to give a total difference as a scalar value for the four pixels being analyzed. To complete the analysis use a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Step-Node.html">Step</a> node, returning 1 if the value is over the <strong>Normal Threshold</strong> property.</li>
</ol>
</li>
</ul>
<p><img src="_page_41_Figure_5.jpeg" alt=""></p>
<p>Detecting edges based on world-space normals.</p>
<ol>
<li>You do more or less the same thing for the color buffer, except you&#39;ll access the Blit Source, not Normal World Space, and use Color Threshold rather than Normal Threshold.</li>
</ol>
<p><img src="_page_41_Figure_8.jpeg" alt=""></p>
<p>For the Color edge detection, the sample Source Buffer is set to Blit Source.</p>
<p><img src="_page_42_Picture_0.jpeg" alt=""></p>
<ul>
<li><ol>
<li>After step 9 there should be two outputs that are either 0 or 1. The remainder of the graph is shown below. The lines appearing from the left are from the output of the normal edge detection (upper) and color edge detection (lower). Use an Add node to add these values and now you should have a value that can be 0, 1, or 2.</li>
</ol>
</li>
<li><ol>
<li>Use a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Saturate-Node.html">Saturate</a> node to clamp this value between 0 and 1.</li>
</ol>
</li>
<li><ol>
<li>Then use a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Lerp-Node.html">Lerp</a> node to interpolate between the existing pixel, supplied by a URP Sample Buffer node and the Outline Color property. There is an option to use an Overlay. If this is set to <strong>On</strong> then the output is the composite of the existing Blit Source and the Outline Color property. If Overlay is set to <strong>Off</strong> then just the Outline Color or black is passed. The final output goes to the Fragment Base Color input.</li>
</ol>
</li>
</ul>
<p><img src="_page_42_Figure_5.jpeg" alt=""></p>
<p>The final nodes of the Outline shader</p>
<p>This recipe provides the key techniques for toon shading.</p>
<h4 id="-more-resources-"><strong>More resources</strong></h4>
<ul>
<li><a href="https://github.com/UnityTechnologies/open-project-1/tree/devlogs/1-toon-shading">Unity Open Project</a> GitHub (most of the code from this chapter is from the Open Project)</li>
<li>Unity Open Project on <a href="https://www.youtube.com/watch?v=O4N4s6BKNH0">YouTube</a></li>
<li><a href="https://www.youtube.com/watch?v=RC91uxRTId8">YouTube tutorials</a> from Ned Makes Games (includes a short series on toon shading)</li>
<li><a href="https://www.youtube.com/watch?v=xgZ0NpaMByU">YouTube tutorial</a> about using Unity&#39;s SobelFilter.shader from AE Tuts (a channel that focuses on Shader Graph tutorials)</li>
<li><a href="https://alexanderameye.github.io/notes/edge-detection-outlines/">Edge detection using a Sobel filter</a> by Alexander Ameye</li>
<li>Daniel Ilett&#39;s Cel Shading <a href="https://danielilett.com/2019-05-29-tut2-intro/">series</a> and <a href="https://www.youtube.com/watch?v=VGEz8oKyMpY">Fullscreen outline shader tutorial</a></li>
</ul>
<h2 id="-span-id-page-43-0-span-ambient-occlusion"><span id="page-43-0"></span>Ambient occlusion</h2>
<p><img src="_page_43_Picture_1.jpeg" alt=""></p>
<p>The racing game, <em><a href="https://www.circuit-superstars.com/">Circuit Superstars</a></em> by Original Fire Games, is a game made with Unity that uses URP features like Scree Space Ambient Occlusion (SSAO), to ground the cars and models in the environment and add depth to the visuals.</p>
<p><img src="_page_44_Picture_1.jpeg" alt=""></p>
<p>Ambient Occlusion</p>
<p>Ambient occlusion is a post-processing technique that darkens creases, holes, intersections, and surfaces that are close to one another. In the real world, such areas tend to block out or occlude ambient light, thereby appearing darker. In the image above, the left side is rendered without ambient occlusion and on the right, rendered with it. Notice how the edges around the steps are darkened.</p>
<p>URP implements the real-time <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/post-processing-ssao.html">Screen Space Ambient Occlusion</a> (SSAO) effect as a Renderer Feature. The pass code it uses can be viewed <a href="https://github.com/Unity-Technologies/Graphics/blob/master/Packages/com.unity.render-pipelines.universal/Runtime/Passes/ScreenSpaceAmbientOcclusionPass.cs">here.</a></p>
<p><strong>Note</strong>: The SSAO effect is a Renderer Feature and works independently from the postprocessing effects in URP. This effect does not depend on or interact with Volumes.</p>
<p>To see it in action, open <strong>Scenes &gt; Ambient Occlusion &gt; Ambient Occlusion</strong>. This scene is a low-polygon city environment available as a <a href="https://assetstore.unity.com/packages/3d/environments/urban/toony-tiny-city-demo-176087?utm_source=demand-gen&amp;utm_medium=pdf&amp;utm_campaign=render-with-quality-and-flexibility&amp;utm_content=urp-cookbook">free asset</a> on the Unity Asset Store.</p>
<p>The scene uses the URP Asset named <strong>Ambient_Occlusion_URP_Settings</strong>. This is loaded automatically when you open the scene via the AutoLoadPipelineAsset script attached to <strong>Scene &gt; Main Camera</strong>. The URP Asset uses the <strong>Ambient_Occlusion_URP_Settings_Renderer</strong>.</p>
<p><span id="page-45-0"></span>To add SSAO to your scene, view the Universal Renderer Data asset in the Inspector, and click on <strong>Add Renderer Feature</strong>. In the options drop-down menu, select <strong>Screen Space Ambient Occlusion</strong>.</p>
<p><img src="_page_45_Picture_2.jpeg" alt=""></p>
<p>Adding a SSAO Renderer Feature</p>
<h2 id="ssao-properties">SSAO properties</h2>
<p>Once you add a SSAO Renderer Feature, you can control the result via the Inspector. Let&#39;s look at the available properties.</p>
<table>
<thead>
<tr>
<th>V &gt; Screen Space Ambient Occlusion</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Method</td>
<td>Interleaved Gradient</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Intensity</td>
<td>2.51</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Radius</td>
<td>0.07</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Falloff Distance</td>
<td>100</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Direct Lighting Strength</td>
<td></td>
<td>0.901</td>
<td></td>
</tr>
<tr>
<td>Quality</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Source</td>
<td>Depth Normals</td>
<td></td>
<td>-</td>
</tr>
<tr>
<td>Normal Quality</td>
<td>Medium</td>
<td></td>
<td>9</td>
</tr>
<tr>
<td>Downsample</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>After Opaque</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Blur Quality</td>
<td>High</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Samples</td>
<td>Low</td>
<td></td>
<td>0</td>
</tr>
</tbody>
</table>
<p>SSAO options</p>
<ul>
<li><strong>Method</strong>: Choose between Interleaved Gradient and Blue Noise</li>
<li><strong>Intensity</strong>: This controls the strength of the darkening.</li>
<li><strong>Radius</strong>: This controls how many samples of the normal texture are taken around the current pixel. Larger values have a significant impact on performance, so keep them as low as possible. The radius value is scaled based on the distance from the camera to the object that is being rendered at the target pixel.</li>
<li><strong>Falloff Distance</strong>: This controls the scene distance where ambient occlusion does not apply.</li>
</ul>
<p><img src="_page_46_Picture_0.jpeg" alt=""></p>
<ul>
<li><strong>Direct Lighting Strength</strong>: This property is dependent on the <strong>After Opaque</strong> option being disabled since it relies on being handled when lighting calculations are being done. It affects the strength of ambient occlusion where direct light hits.</li>
<li><strong>Quality &gt; Source</strong>: This option selects the source of the normal vector values. The SSAO Renderer Feature uses normal vectors for calculating how exposed each point on a surface is to ambient lighting. <strong>Available choices for Source</strong>:<ul>
<li><strong>Depth Normals</strong>: SSAO uses the normal texture generated by the DepthNormals pass. This option lets Unity make use of a more accurate normal texture.</li>
<li><strong>Depth</strong>: SSAO reconstructs the normal vectors using the depth texture instead. Use this option only if you want to avoid using the DepthNormals pass block in your custom shaders. Selecting this option enables the <strong>Normal Quality</strong> property.</li>
</ul>
</li>
</ul>
<p>When switching between these two options, there might be a variation in performance, which depends on the target platform and the application. In a wide range of applications the difference in performance is small. In most cases, Depth Normals produces a better visual look.</p>
<ul>
<li><strong>Quality &gt; Source &gt; Normal Quality</strong>: This is active when the <strong>Source</strong> property is set to <strong>Depth</strong>.<ul>
<li>The options in this property (Low, Medium, High) determine the number of samples of the depth texture that Unity takes when reconstructing the normal vector from the depth texture. The number of samples per quality level are:</li>
<li>Low: 1</li>
<li>Medium: 5</li>
<li>High: 9</li>
</ul>
</li>
</ul>
<p>The performance impact is regarded as medium.</p>
<ul>
<li><strong>Quality &gt; Downsample</strong>: Selecting this halves the resolution of the processing in both the X and Y directions. Since this effectively reduces the number of pixels to process by 75%, it also reduces GPU load significantly but results in an effect with fewer details.</li>
<li><strong>Quality &gt; After Opaque</strong>: This option affects the look of the final render, but it comes with performance implications:<ul>
<li><strong>If disabled:</strong> SSAO has a Depth or Depth Normals prepass (see the Source option below). The SSAO is then calculated after them and applied in the DrawOpaques pass when doing the lighting calculations. It gives a better-looking Ambient Occlusion, and the user can control the Direct Lighting Strength value for SSAO, but it has a negative impact on performance.</li>
<li><strong>If enabled</strong>: SSAO requires a Depth Normals if After Opaque is selected. If Depth is selected, then it either gets the depth from Depth prepass, if that was made, or a CopyDepth pass done after rendering opaques. The SSAO is then added on top</li>
</ul>
</li>
</ul>
<p>of everything after the DrawOpaques pass, instead of being part of the lighting calculations. The benefit here is that a prepass can be skipped, which can help performance.</p>
<p><strong>Note</strong>: You want to also be able to render Depth + Normals in the Render Opaque pass so you can fully skip any prepass with that option enabled to save performance.</p>
<ul>
<li><strong>Quality &gt; Blur Quality</strong>: This can be set to High, Medium, or Low. On devices with limited resources use the lowest setting that gives satisfactory results.</li>
<li><strong>Quality &gt; Samples</strong>: For each pixel, the SSAO Renderer Feature takes the number of samples within the specified radius to calculate the ambient occlusion value. Increasing this from low to medium to high makes the effect smoother and more detailed, but reduces the performance.</li>
</ul>
<p><img src="_page_47_Picture_5.jpeg" alt=""></p>
<p>Two variations of Direct Lighting Strength: 0.2 (left) and 0.9 (right); note the darker lines in between each step on the image of the right</p>
<p>SSAO is a great example of the flexibility of URP. The number of problems that can be addressed using Renderer Features is limited only by your imagination.</p>
<h4 id="more-resources">More resources</h4>
<ul>
<li>YouTube <a href="https://www.youtube.com/watch?v=pgM4pKG1aGE">tutorial</a> from UGuruz</li>
<li>Ambient occlusion <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/post-processing-ssao.html">documentation</a></li>
<li><a href="https://assetstore.unity.com/packages/3d/environments/urban/toony-tiny-city-demo-176087?utm_source=demand-gen&amp;utm_medium=pdf&amp;utm_campaign=render-with-quality-and-flexibility&amp;utm_content=urp-cookbook">Assets</a> used in recipe (thanks to <a href="https://assetstore.unity.com/publishers/38782?utm_source=demand-gen&amp;utm_medium=pdf&amp;utm_campaign=render-with-quality-and-flexibility&amp;utm_content=urp-cookbook">Marcelo Barrio</a>)</li>
</ul>
<h2 id="-span-id-page-48-0-span-decals"><span id="page-48-0"></span>Decals</h2>
<p><img src="_page_48_Picture_1.jpeg" alt=""></p>
<p>One of many use cases for decals is to project a blob shadow onto a 3D surface, like the character Milo from the game <em><a href="https://www.tinykingame.com/">Tinykin</a></em>, made with Unity by Splashteam.</p>
<p>Decals are a great way to add overlays to a surface. They are often used to add visuals such as bullet holes or tire treads to the game environment as the player interacts with the scene. As you can see from the steps in the following image, the decal wraps around a mesh. You&#39;ll find the scene file and assets for this recipe in the folder <strong>Scenes &gt; Decals</strong>.</p>
<p><img src="_page_49_Picture_2.jpeg" alt=""></p>
<p>Decals added to a simple scene</p>
<p>Decals are rendered in a scene using a Renderer Feature. If you look at <strong>Decals_URP_Settings_ Renderer</strong> from the recipe folder you&#39;ll see that the Decals Renderer Feature is added. As usual, the AutoLoadPipelineAsset.cs script attached to the Main Camera ensures the correct pipeline asset is used when you load the scene. To add decals to your custom scene, select the Universal Renderer Data asset currently being used by the Player for rendering, and in the Inspector, choose <strong>Decal</strong> from the Add Renderer Feature drop-down.</p>
<p><img src="_page_49_Picture_5.jpeg" alt=""></p>
<p>The Decal option in the Add Renderer drop-down</p>
<p><span id="page-50-0"></span><img src="_page_50_Picture_0.jpeg" alt=""></p>
<p>To add a decal to a scene when working in the Editor, right-click the Hierarchy window, and select <strong>Rendering &gt; URP Decal Projector</strong>.</p>
<p><img src="_page_50_Picture_3.jpeg" alt=""></p>
<p>Creating a URP Decal Projector</p>
<p>Position and orient a URP Decal Projector in the Editor as you usually would. A Decal Projector uses orthographic projection, so the size of a decal cast on a surface is unaffected by the distance of the projector from the surface. Initially, a new Decal Projector will display as a white block. In addition to the axis arrows, you&#39;ll see a white arrow indicating the direction of projection.</p>
<p><img src="_page_50_Picture_6.jpeg" alt=""></p>
<p>A new Decal Projector</p>
<h2 id="urp-decal-projection-properties">URP Decal Projection properties</h2>
<p>— <strong>Scale Mode:</strong> By default, the URP Decal Projection component has <strong>Scale Mode</strong> set to <strong>Scale Invariant</strong>. That means the size of the decal is determined solely by the <strong>Width</strong> and <strong>Height</strong> properties. Switching to <strong>Inherit from Hierarchy</strong> will combine the GameObject&#39;s Transform Scale with the Width and Height properties.</p>
<p><span id="page-51-0"></span><img src="_page_51_Picture_0.jpeg" alt=""></p>
<ul>
<li><strong>Width and Height:</strong> Properties that control the size of the decal</li>
<li><strong>Projection Depth:</strong> Sets the depth of the projector bounding box; the projector projects decals along the local Z axis</li>
<li><strong>Pivot</strong>: Sets the offset position of the center of the projector bounding box, relative to the origin of the root GameObject</li>
<li><strong>Material</strong>: Sets the Material to project; the Material must use the shader <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/decal-shader-graph-reference.html">Shader Graph/</a> <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/decal-shader-graph-reference.html">Decal</a> (more details about this shortly)</li>
<li><strong>Tiling and Offset:</strong> The tiling and offset values for the Decal Material along its UV axes</li>
<li><strong>Opacity</strong>: Lets you specify the opacity value; a value of 0 makes the decal fully transparent, a value of 1 makes the decal as opaque, as defined by the Material</li>
<li><strong>Draw Distance:</strong> Distance from the Camera to the decal at which this projector stops projecting the decal and URP no longer renders it</li>
<li><strong>Start Fade</strong>: Sets the distance (via a slider) from the Camera at which the projector begins to fade out the decal; values from 0 to 1 represent a fraction of the Draw Distance; with a value of 0.9, Unity starts fading the decal out at 90% of the Draw Distance and finishes fading it out at the Draw Distance</li>
<li><strong>Angle Fade</strong>: Sets the fade out range of the decal based on the angle between the decal&#39;s backward direction and the vertex normal of the receiving surface</li>
</ul>
<h4 id="-creating-the-material-"><strong>Creating the material</strong></h4>
<p>A Decal Projector must use a material that uses the shader Shader Graph/Decal. This example uses the material called DecalMat found in the Scene folder. There is a base map assigned but no normal map; this is useful if you want the appearance of a lumpy surface for the decal.</p>
<p><img src="_page_51_Picture_13.jpeg" alt=""></p>
<p>The material is assigned to the Projector in the Inspector.</p>
<p>Assigning the URP Decal Projector Material</p>
<p><span id="page-52-0"></span><img src="_page_52_Picture_0.jpeg" alt=""></p>
<h2 id="adding-a-decal-with-code">Adding a decal with code</h2>
<p>Although you can add a URP Decal Projector to your scene while developing in the Editor, it&#39;s more common to add them as a result of user interaction at runtime. You can create a prefab to establish the Material, Width, and Height properties, although you can easily update this at runtime in code. This code example focuses on instantiation, positioning, and orientation only. The complete code to add a decal as a result of a mouse press on a Collider can be found in the <a href="https://github.com/NikLever/Unity-URP-Cookbook-Unity6/blob/main/Assets/Scenes/Decals/AddDecal.cs">AddDecal.cs</a> script in the recipe folder.</p>
<pre><code><span class="hljs-keyword">void</span> AddDecalProjector(Vector3 pos, Vector3 <span class="hljs-built_in">normal</span>)
{
 GameObject decalProjectorObject = Instantiate(decalProjectorPrefab);
 <span class="hljs-comment">// Creates a new material instance for the DecalProjector </span>
 <span class="hljs-comment">//if you want individual Decal control over the material</span>
 <span class="hljs-comment">//DecalProjector decalProjectorComponent = decalProjectorObject.</span>
GetComponent&lt;DecalProjector&gt;();
 <span class="hljs-comment">//decalProjectorComponent.material = new Material(decalProjectorComponent.material);</span>
 <span class="hljs-comment">//Move away from surface</span>
 pos += <span class="hljs-built_in">normal</span> * <span class="hljs-number">0.5</span>f;
 Quaternion up = Quaternion.AngleAxis(Random.Range(<span class="hljs-number">0</span>, <span class="hljs-number">360</span>), Vector3.left);
 Quaternion rot = Quaternion.LookRotation(-<span class="hljs-built_in">normal</span>, up.eulerAngles);
 decalProjectorObject.transform.SetPositionAndRotation(pos, rot);
}
</code></pre><p>This function is called when there is a RaycastHit after a mouse-down event over a Collider. pos is the hit.point and normal the hit.normal. The prefab called decalProjectorObject is instantiated. To get the position, you need to move the pos Vector3 away from the surface without exceeding the Projection Depth. This is achieved by moving the point along the normal. To orientate the decal, you first create a randomized up vector. To get the necessary rotation to align the decal to the surface and rotate a random amount around the normal, use the parameters inverse normal and the randomized up vector.</p>
<p><img src="_page_53_Picture_1.jpeg" alt=""></p>
<p>Decals have many uses in games, and the URP Decal Projector is a great tool in your toolbox.</p>
<p>A decal in the Scene view</p>
<h4 id="more-resources">More resources</h4>
<ul>
<li>Decal Renderer <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/renderer-feature-decal.html">documentation</a></li>
<li>YouTube <a href="https://www.youtube.com/watch?v=5p8cKIu3P_8">tutorial</a> by Llam Academy</li>
</ul>
<h2 id="-span-id-page-54-0-span-water"><span id="page-54-0"></span>Water</h2>
<p><img src="_page_54_Picture_1.jpeg" alt=""></p>
<p>Water and aquatic vegetation are two important visual elements for creating beautiful open environments in video games. This image is from the survival game <em><a href="https://www.lensisland.com">Len&#39;s island</a></em>, made with Unity by Flowstudio.</p>
<p><img src="_page_55_Picture_0.jpeg" alt=""></p>
<p>This recipe is for making a simple water shader. It&#39;s created in Shader Graph to make the steps more accessible to artists and designers.</p>
<p>The shader is built in three stages:</p>
<ul>
<li>Creating the water color</li>
<li>Moving tiled normal maps to add wavelets to the surface</li>
<li>Adding moving displacement to the vertex positions to create a swell effect</li>
</ul>
<p><img src="_page_55_Picture_6.jpeg" alt=""></p>
<p>A still from a <a href="https://youtu.be/qPE-nMPBylM">video</a> showing a simple water shader in motion.</p>
<p>To view the final result, open the Water scene in the folder <strong>Scenes &gt; Water</strong>. The final shader uses two subgraphs, DepthFade and TextureMovement; it&#39;s a good idea to look at them before you review the water shader. The Water scene uses the <strong>WaterURPSettings Asset</strong>, with the <strong>Depth Texture</strong> and <strong>Opaque Texture</strong> options enabled. Note that the Opaque Texture is only required if you add further effects not covered in this recipe, such as refraction.</p>
<p><img src="_page_55_Picture_9.jpeg" alt=""></p>
<p>Depth Texture and Opaque Texture selected in the WaterURPSettings asset</p>
<p><span id="page-56-0"></span><img src="_page_56_Picture_0.jpeg" alt=""></p>
<h2 id="depthfade-subgraph">DepthFade subgraph</h2>
<p><img src="_page_56_Figure_3.jpeg" alt=""></p>
<p>The DepthFade subgraph</p>
<p>The shallow and deep parts of the water each require their own color. The final color of the water will be a blend of these two colors, based on a <strong>Depth</strong> property. Depth is the distance between the surface of the water and the geometry below it. Since the water shader is set as transparent, opaque geometry will already be rendered, and because Depth Texture is selected for the URP Settings Asset, the current depth can be read.</p>
<p>A <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Scene-Depth-Node.html?q=Scene Depth">Scene Depth</a> node with Sampling set to Eye mode gives the distance from the eye to the opaque geometry at the current pixel. The <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Screen-Position-Node.html?q=screen position raw">Screen Position</a> node, with Raw selected as the mode of its output value, holds the information about rendering the current pixel of water. A Split node is used since you want the W component, which stores the distance from the eye to the current pixel of water.</p>
<p>Subtracting the water distance from the distance of the existing opaque geometry gives a guide to the depth of the water, albeit a ray from the eye position, not a ray directly down. Next, a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Divide-Node.html?q=Divide node">Divide</a> node controls where the edge between shallow and deep appears. The output from this subgraph should be between 0 and 1, so you&#39;ll use a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Saturate-Node.html?q=Saturate node">Saturate</a> node which acts as a specialized <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Clamp-Node.html?q=Clamp">Clamp</a> node by always restricting the output between 0 and 1.</p>
<p><span id="page-57-0"></span><img src="_page_57_Picture_0.jpeg" alt=""></p>
<h2 id="texturemovement-subgraph">TextureMovement subgraph</h2>
<p><img src="_page_57_Figure_3.jpeg" alt=""></p>
<p>TextureMovement subgraph</p>
<p>The water shader has a number of moving textures that are handled using the TextureMovement subgraph. In this subgraph, a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Time-Node.html?q=Time node">Time</a> node is used as one input to a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Multiply-Node.html?q=Multiply">Multiply</a> node. The input Speed is divided by 100 and forms the second input to the Multiply node. The output from the Multiply node acts as the Offset input to a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Tiling-And-Offset-Node.html?q=tiling">Tiling and Offset</a> node. The <strong>Scale</strong> property forms the Tiling input. Over time, this simple subgraph will update the UV used by a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Sample-Texture-2D-Node.html?q=sample texture">Sample Texture 2D</a> node given a Speed and Scale input.</p>
<h3 id="water-shader">Water shader</h3>
<p>Now it&#39;s time to create the water shader, based on a Lit Shader Graph, via <strong>URP &gt; Lit Shader Graph.</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Visual Scripting</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Shader Graph</td>
<td>URP</td>
<td></td>
<td>Lit Shader Graph</td>
</tr>
<tr>
<td></td>
<td>Shader</td>
<td>Builtin</td>
<td></td>
<td>Unlit Shader Graph</td>
</tr>
<tr>
<td></td>
<td>Shader Variant Collection</td>
<td>Blank Shader Graph</td>
<td></td>
<td>Sprite Custom Lit Shader Graph</td>
</tr>
<tr>
<td></td>
<td>Testing</td>
<td>Sub Graph</td>
<td></td>
<td>Sprite Unlit Shader Graph</td>
</tr>
<tr>
<td></td>
<td>Playables</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Assembly Definition</td>
<td></td>
<td></td>
<td>Sprite Lit Shader Graph</td>
</tr>
<tr>
<td></td>
<td>Assembly Definition Reference</td>
<td></td>
<td></td>
<td>Decal Shader Graph</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Create &gt; Shader Graph &gt; URP &gt; Lit Shader Graph</p>
<p><span id="page-58-0"></span><img src="_page_58_Picture_0.jpeg" alt=""></p>
<p>Next, you&#39;ll use the Graph Inspector to set the <strong>Surface Type.</strong></p>
<p><img src="_page_58_Picture_3.jpeg" alt=""></p>
<p>Setting the Surface Type</p>
<h4 id="edit-the-graph-starting-with-color-">Edit the graph, starting with Color.</h4>
<h3 id="color">Color</h3>
<p><img src="_page_58_Figure_7.jpeg" alt=""></p>
<p>Color and Alpha</p>
<p>Color is handled by adding a DepthFade subgraph. The subgraph uses a float <strong>Depth</strong> property for control. If the output goes directly to the Base Color input of the Fragment shader, it results in the below image: shallow water is black and deeper water white. The higher the value of Depth, the more the black spreads. Black indicates 0 and white 1.</p>
<p><img src="_page_59_Picture_1.jpeg" alt=""></p>
<p>Plugging the output from DepthFade directly into <strong>Fragment &gt; Base Color</strong></p>
<p>Instead of linking the DepthFade directly to the Base Color input, it goes to a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Lerp-Node.html?q=Lerp">Lerp</a> node. <strong>ShallowWaterColor</strong> is input A, replacing the black color, and <strong>DeepWaterColor</strong> is input B, replacing the white. When setting the alpha for these colors make sure the shallow water is more transparent. The Lerp output goes to <strong>Fragment &gt; Base Color</strong>. For the Alpha, you&#39;ll use a Split node, linking the A output with <strong>Fragment &gt; Alpha</strong>. This produces the result seen in the following image.</p>
<p><img src="_page_59_Picture_4.jpeg" alt=""></p>
<p>Dense mesh and colored water</p>
<p><span id="page-60-0"></span><img src="_page_60_Picture_0.jpeg" alt=""></p>
<p>The actual water is a plane, but to allow for vertex displacement, the mesh has many more vertices as the image shows.</p>
<p>This simple flat surface is a start but needs more work, namely, normal maps.</p>
<h3 id="normal-maps">Normal maps</h3>
<p><img src="_page_60_Figure_5.jpeg" alt=""></p>
<p>Controlling the <strong>Fragment &gt; Normal</strong></p>
<p>Normal maps add moving wavelets to the surface. The first input property is <strong>Wave Speed</strong>, which is used as the Speed input to a TextureMovement subgraph. <strong>Scale</strong> is hard set to 50,50, and, via a second TextureMovement node, <strong>Speed</strong> is preprocessed by a Multiply node to be minus half the Wave Speed property.</p>
<p>The next step in calculating the normal is to sample the Normal texture twice using the UV processed by the two TextureMovement subgraph nodes. Add the two normals together to get the combined effect of the two moving textures. The shader has a <strong>Normal Strength</strong> float property, which could be used as the Strength input to a Normal Strength node. But you want the wavelets to die back nearer to the edge. To control this, use the DepthFade subgraph node with the shader property <strong>Edge Distance</strong> controlling the spread. This is used as the T input to a Lerp node blending between 0 and Normal Strength. The output of this stage of the graph goes to Fragment &gt; Normal.</p>
<p><span id="page-61-0"></span><img src="_page_61_Picture_0.jpeg" alt=""></p>
<p>Now you have controllable wavelets whose reflective property can be tweaked by controlling the <strong>Smoothness</strong> of the Fragment using a simple float property. The following image shows the effect of changing the Smoothness value.</p>
<p><img src="_page_61_Picture_2.jpeg" alt=""></p>
<p>Applying different levels of smoothness to the wavelets, left to right: 0, 0.5, and 1</p>
<p>The next step is to enable vertex displacement to add motion to the water.</p>
<p><img src="_page_61_Figure_5.jpeg" alt=""></p>
<h3 id="swell">Swell</h3>
<p>Controlling the swell using Gradient Noise</p>
<p><img src="_page_62_Picture_0.jpeg" alt=""></p>
<p>For this step, you&#39;ll use a TextureMovement subgraph node again. Speed is set using the shader float property <strong>Swell Speed</strong>, and Scale is hard set to 50,50. This acts as the UV input to a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Gradient-Noise-Node.html?q=Gradient noise">Gradient Noise</a> node with Scale hard set to 1. You use a Multiply node to control this value using the shader float property <strong>Displacement</strong>. The purpose of these nodes is to set a Y value for vertex in object space. Notice the <strong>Space</strong> parameter of the Position node is set to Object. This links with a Split node and then a Combine node; Combine receives the R and B values directly from the Split node, with R being Position X and B being Position Z. The G value for Y comes from the Gradient Noise path. The RGB(3) output links to the Vertex &gt; Position.</p>
<p>If you view the scene in Play mode you can see the swell moving through the water, especially at the edges.</p>
<p><img src="_page_62_Picture_4.jpeg" alt=""></p>
<p>The final result</p>
<p>While this recipe forms the basis of a simple water shader, you can enhance it using caustic reflections, refraction, and foam. See the links below for additional guidance.</p>
<h4 id="-more-information-"><strong>More information</strong></h4>
<ul>
<li>Unity <a href="https://www.youtube.com/watch?v=gRq-IdShxpU">YouTube</a> tutorial</li>
<li><a href="https://www.alanzucconi.com/2019/09/13/believable-caustics-reflections/">Caustic reflections</a> tutorial by Alan Zucconi</li>
<li><a href="https://www.youtube.com/watch?v=1yevpCAA_rU&amp;t=831s">Stylized water</a> tutorial by Binary Lunar</li>
</ul>
<h2 id="-span-id-page-63-0-span-lut-for-color-grading"><span id="page-63-0"></span>LUT for color grading</h2>
<p><img src="_page_63_Picture_1.jpeg" alt=""></p>
<p>The mystery adventure FPS game <em><a href="https://obradinn.com/">Return of the Obra Dinn</a></em>, made with Unity by Lucas Pope, achieves a unique look and feel thanks to its lo-fi art style and unique color palette that could be achieved following this recipe.</p>
<p><img src="_page_64_Picture_1.jpeg" alt=""></p>
<p>Using Color Lookup to create grading effects</p>
<p>If you&#39;ve yet to use the post-processing filters available with URP, you&#39;re in for a treat. This recipe involves using one filter, but the steps employed apply to all them. By default, a new URP scene has post-processing disabled, so make sure to enable it via the <strong>Camera &gt; Rendering</strong> panel.</p>
<p><img src="_page_64_Picture_4.jpeg" alt=""></p>
<p>Select Post Processing in Camera &gt; Rendering</p>
<h4 id="additionally-you-ll-need-to-enable-post-processing-in-the-universal-renderer-data-asset-">Additionally, you&#39;ll need to enable post-processing in the Universal Renderer Data asset.</h4>
<p><img src="_page_64_Figure_7.jpeg" alt=""></p>
<p>Selecting post-processing in the Universal Renderer Data asset</p>
<p>To apply the filter where the camera is located, add a Global Volume. Right-click in the Hierarchy window, and select <strong>Volume &gt; Global Volume</strong>.</p>
<p><img src="_page_65_Picture_2.jpeg" alt=""></p>
<p>Creating a Global Volume</p>
<p>Select the new GameObject, and create a new Profile by clicking <strong>New</strong>.</p>
<table>
<thead>
<tr>
<th>DV<br>Volume</th>
<th></th>
<th></th>
<th>?)<br>14</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Mode</td>
<td>Global</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Weight</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Priority</td>
<td>O</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Profile</td>
<td>None (Volume Profile)</td>
<td>ن</td>
<td>New</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>to the scene.</td>
<td>Please select or create a new Volume profile to begin applying effects</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Creating a new Profile</p>
<p>Now you can add an override. Press the <strong>Add Override</strong> button, select post-processing, then choose <strong>Color Lookup</strong>.</p>
<p><img src="_page_65_Picture_8.jpeg" alt=""></p>
<p>Adding a Color Lookup post-processing filter</p>
<p>Click the <strong>All</strong> button. Now you need a LUT (Lookup Table) image texture. This is a strip image that will be used by the filter to change the default rendered colors. You&#39;ll find the image file in <strong>Scenes &gt; LUT &gt; NeutralLUT.png,</strong> or download it using <a href="https://github.com/NikLever/Unity-URP-Cookbook-Unity6/blob/main/Assets/Scenes/LUT/NeutralLUT.png">this link</a>.</p>
<p><img src="_page_66_Figure_2.jpeg" alt=""></p>
<p>A LUT image must have <strong>sRGB (Color Texture)</strong> disabled, which you do by selecting the image and viewing the Inspector.</p>
<table>
<thead>
<tr>
<th>Neutral LUT (Texture 2D) Import Settings</th>
<th>8 花</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Open</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Texture Type</td>
<td>Default</td>
<td></td>
</tr>
<tr>
<td>Texture Shape</td>
<td>2D</td>
<td></td>
</tr>
<tr>
<td>sRGB (Color Texture)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Alpha Source</td>
<td>Input Texture Alpha</td>
<td></td>
</tr>
<tr>
<td>Alpha Is Transparency</td>
<td></td>
</tr>
</tbody>
</table>
<p>Disable sRGB (color Texture) for all LUT textures</p>
<p>Count the blocks in the NeutralLUT image above, and you&#39;ll find there are 32 of them. Alternatively, you can use 16 blocks; whether you choose 32 or 16 blocks, ensure the settings for your URP Asset match your choice. If you choose 32, make sure the post-processing panel has <strong>LUT size</strong> set to <strong>32</strong>. Feel free to experiment with the <strong>Grading Mode</strong> option.</p>
<table>
<thead>
<tr>
<th>Post-processing</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Grading Mode</td>
<td>High Dynamic Range</td>
</tr>
<tr>
<td></td>
<td>The high dynamic range color grading mode works best on<br>platforms that support floating point textures.</td>
</tr>
<tr>
<td>LUT size</td>
<td>32</td>
</tr>
<tr>
<td>Alpha Processing</td>
<td></td>
</tr>
<tr>
<td>Fast sRGB/Linear con</td>
</tr>
</tbody>
</table>
<p>Setting the LUT size</p>
<p>If you assign <strong>NeutralLUT.png</strong> as the Lookup Texture using the <strong>Color Lookup</strong> settings panel, you&#39;ll see no change to the rendered image. The filter uses the texture to set a new color. The code takes the current pixel color and uses this to find a texel on the LUT image. With a neutral LUT image, the texel color will be the same as the current pixel color.</p>
<p>The real magic occurs when you process the image you use as the Lookup Texture using a paint program, such as Photoshop or Krita (there&#39;s a link under More resources, at the end of this section, to a YouTube video explaining how to use Krita for color grading).</p>
<p><img src="_page_67_Figure_2.jpeg" alt=""></p>
<p>Assigning the Lookup Texture</p>
<p>Take a screen grab of your scene, and open it in Photoshop. At the bottom of the <strong>Layers</strong>  panel, find the half black/half white circular button. Select it, and in the panel find <strong>Gradient Map</strong>. A new color adjustment layer is added.</p>
<p><img src="_page_67_Picture_5.jpeg" alt=""></p>
<p>Creating a color adjustment layer</p>
<p>To create a color adjustment layer that results in a high-contrast black-and-white image, click the Gradient Map drop-down and select Basics, black and white.</p>
<p><img src="_page_67_Picture_8.jpeg" alt=""></p>
<p>Selecting a black and white gradient</p>
<p><img src="_page_68_Picture_0.jpeg" alt=""></p>
<p>To boost the contrast, click the gradient to open a new window. Use the stops to adjust the contrast.</p>
<p><img src="_page_68_Picture_2.jpeg" alt=""></p>
<p>Changing the stops to boost the contrast</p>
<h4 id="the-screengrab-should-now-look-black-and-white-">The screengrab should now look black and white.</h4>
<p><img src="_page_68_Picture_5.jpeg" alt=""></p>
<p>The effect of the Gradient Map</p>
<p>Once you have the grading of your choice, you need to apply this layer to the NeutralLUT. png file. Open the file in Photoshop. Back in the screen grab, right-click the adjustment layer, and select <strong>Duplicate Layer</strong>. In the new panel, select NeutralLUT.png as the <strong>Destination &gt; Document.</strong></p>
<p><img src="_page_69_Picture_1.jpeg" alt=""></p>
<p>Duplicating the adjustment layer</p>
<p>Now the texture looks like this:</p>
<p>B&amp;WLUT.png</p>
<p>Save it, and drag it to your project&#39;s Assets folder. Make sure to disable sRGB (Color Texture) in the Inspector panel. The last step is to assign the new LUT texture as the Lookup Texture for the Color Lookup filter.</p>
<p><img src="_page_69_Picture_6.jpeg" alt=""></p>
<p>Using various LUT textures</p>
<p>Using LUT Textures is an efficient way to create dramatic color grading, and this approach can be useful in many games.</p>
<p><img src="_page_70_Picture_0.jpeg" alt=""></p>
<h4 id="-more-resources-"><strong>More resources</strong></h4>
<ul>
<li><a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/add-post-processing.html">Documentation</a> for post-processing in URP</li>
<li>YouTube <a href="https://www.youtube.com/watch?v=zTuCTYbvxac">tutorial</a> by PHLEARN</li>
<li>YouTube <a href="https://www.youtube.com/watch?v=Iurcp8xdpJY">tutorial</a> by GDQuest</li>
</ul>
<h2 id="-span-id-page-71-0-span-adaptive-probe-volumes"><span id="page-71-0"></span>Adaptive Probe Volumes</h2>
<p><img src="_page_71_Picture_1.jpeg" alt=""></p>
<p>The Unity and URP-made game <em><a href="https://thunderfulgames.com/games/lego-bricktales/">LEGO® Bricktales</a></em> by Clockstone immerses players in the world of LEGO, where great lighting plays a huge role in creating its atmosphere and realism of the blocks.</p>
<p>Adaptive Probe Volumes (APVs) are the latest Unity solution for mixed mode lighting and are easier to set up and maintain than light probes. If you are new to lighting techniques available with URP then check out the Unity e-book <a href="https://unity.com/resources/introduction-to-urp-advanced-creators-unity-6">Introduction to the Universal Render Pipeline for</a>  <a href="https://unity.com/resources/introduction-to-urp-advanced-creators-unity-6">advanced Unity creators</a>.</p>
<p><img src="_page_72_Picture_2.jpeg" alt=""></p>
<p>You can combine baked and dynamic objects using the Mixed Mode setting for your lights. When using Mixed Mode it&#39;s recommended to also add probes to your scene, for which there are two options in Unity 6: Light probes or APVs. The two options solve the same problem, namely allowing dynamic objects to move through a scene and be affected by global illumination. But APVs bring additional benefits and setup efficiency compared to traditional light probes.</p>
<p>A probe is simply a point in your scene. At design time the global illumination at this location is calculated. At run time, when rendering a frame, a URP shader that includes lighting calculations uses a blend of the nearest probes for global illumination values.</p>
<p>Note:</p>
<p>Global illumination (GI) is a system that models how light bounces off surfaces onto other surfaces, to create indirect light, rather than being limited to just the light that hits a surface directly from a direct light source.</p>
<h2 id="-span-id-page-73-0-span-using-apvs-in-a-scene"><span id="page-73-0"></span>Using APVs in a scene</h2>
<p>Any technical artist who has carefully positioned light probes for a scene only to find the scene layout has changed will immediately see the benefits of APVs because in many scenes you can use APVs to place all the probes in a matter of seconds. Let&#39;s look at a practical example using the <a href="https://github.com/NikLever/Unity6E-book">FPS Sample: The Inspection.</a></p>
<ol>
<li>First make sure the active URP Asset has the <strong>Light Probe System</strong> option set to <strong>Adaptive Probe Volumes</strong>.</li>
</ol>
<p><img src="_page_73_Figure_4.jpeg" alt=""></p>
<ol>
<li>In the Hierarchy window right-click and select <strong>GameObject &gt; Light &gt; Adaptive Probe Volume</strong> (APV).</li>
</ol>
<table>
<thead>
<tr>
<th>Light</th>
<th>&gt;</th>
<th>Directional Light</th>
</tr>
</thead>
<tbody>
<tr>
<td>Video</td>
<td>&gt;</td>
<td>Point Light</td>
</tr>
<tr>
<td>Ul Toolkit</td>
<td>&gt;</td>
<td>Spot Light</td>
</tr>
<tr>
<td>Rendering</td>
<td>&gt;</td>
<td>Area Light</td>
</tr>
<tr>
<td>Spline</td>
<td>&gt;</td>
<td>Reflection Probe</td>
</tr>
<tr>
<td>Volume</td>
<td>&gt;</td>
<td></td>
</tr>
<tr>
<td>Camera</td>
<td></td>
<td>Adaptive Probe Volume</td>
</tr>
<tr>
<td>Cinemachine</td>
<td></td>
<td>Probe Adjustment Volume</td>
</tr>
<tr>
<td>Visual Scripting Scene Variables</td>
<td>Light Probe Group</td>
</tr>
</tbody>
</table>
<ol>
<li>Set the Mode to <strong>Global</strong> and accept the default settings – Subdivisions of 1, 3, 9 and 27 meters.</li>
</ol>
<p><img src="_page_73_Picture_8.jpeg" alt=""></p>
<ol>
<li>Bake the volume by pressing <strong>Bake Probe Volumes</strong>. The current scene is scanned and the probes are placed based on the geometry in the scene. Probes are at their densest where there is the most geometry.</li>
</ol>
<p><img src="_page_74_Figure_2.jpeg" alt=""></p>
<ol>
<li>To view the result of the bake open <strong>Analysis &gt; Rendering Debugger</strong>. Select <strong>Probe Volumes</strong> and select <strong>Display Probes</strong>. To view the different resolutions choose <strong>Display Bricks</strong>.</li>
</ol>
<table>
<thead>
<tr>
<th>Project<br>Console</th>
<th>Render Graph Viewer</th>
<th>Rendering Debugger</th>
<th>Project Settings</th>
<th>@</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Rese</td>
</tr>
<tr>
<td>Frequently Used</td>
<td>Subdivision Visualization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Probe Volumes</td>
<td>Display Cells</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Display Bricks</td>
<td></td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>Rendering</td>
<td>Debug Draw Distance</td>
<td></td>
<td>500</td>
<td></td>
</tr>
<tr>
<td>Material</td>
<td>Subdivision Preview</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Live Updates</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Lighting</td>
<td>Probe Visualization</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPU Resident Drawer</td>
<td>Display Probes</td>
<td></td>
<td>J</td>
<td></td>
</tr>
<tr>
<td>Render Graph</td>
<td>Probe Shading Mode</td>
<td></td>
<td>SH</td>
<td>V</td>
</tr>
<tr>
<td>Volume</td>
<td>Debug Size</td>
<td></td>
<td>- 0.73</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Exposure Compensation</td>
<td></td>
<td>0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Max Subdivisions Displayed</td>
<td></td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td></td>
<td>Min Subdivisions Displayed</td>
<td></td>
<td></td>
<td>O</td>
</tr>
</tbody>
</table>
<p>For many scenes that would complete the job, and you can head off for a coffee break. But APVs provide much more fidelity. You can add multiple volumes with different subdivisions to have precise control over the placement and density of probes.</p>
<p>Take the oasis environment in the URP 3D Sample as an example. Imagine most of the action in the scene is around the tent and therefore, you want to place most of the probes around it. To achieve this you would:</p>
<ol>
<li>Open <strong>Rendering &gt; Lighting &gt; Adaptive Probe Volumes</strong> and change <strong>Max Probe Spacing</strong> to 81m.</li>
</ol>
<table>
<thead>
<tr>
<th>@ Inspector</th>
<th>. Lighting</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Scene   Adaptive Probe Volumes   Environment</td>
<td></td>
<td></td>
<td></td>
<td>2</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Realtime Lightmaps Baked Lightmaps</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Baking</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Probe Placement</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Probe Positions</td>
<td></td>
<td>Recalculate</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Probe Offset</td>
<td></td>
<td>O</td>
<td></td>
<td>O</td>
<td>O</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Min Probe Spacing</td>
<td></td>
<td>1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Max Probe Spacing</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>9m</td>
<td>27m</td>
<td></td>
<td>81m</td>
<td>243m</td>
</tr>
</tbody>
</table>
<ol>
<li>Add an <strong>Adaptive Probe Volume</strong> set as <strong>Global</strong> and set the <strong>Override Probe Spacing</strong> to 27m&gt;81m.</li>
</ol>
<p><img src="_page_75_Figure_5.jpeg" alt=""></p>
<ol>
<li>Add an Adaptive Probe Volume set as <strong>Local</strong> and set the <strong>Override Probe Spacing</strong> to 1m&gt;9m. Set the Volume to be a bit bigger than the tent.</li>
</ol>
<p><img src="_page_75_Picture_7.jpeg" alt=""></p>
<p><span id="page-76-0"></span><img src="_page_76_Picture_0.jpeg" alt=""></p>
<h4 id="4-bake-the-probe-volumes-">4. Bake the probe Volumes.</h4>
<p>As you can see from the image below, most probes are around the tent.</p>
<p><img src="_page_76_Picture_3.jpeg" alt=""></p>
<p>Probe placement</p>
<h2 id="lighting-scenario-asset">Lighting Scenario asset</h2>
<p>Another feature of APVs is the ability to switch between indirect lighting data. A <strong>Lighting Scenario</strong> asset contains the baked lighting data for a scene or <strong><a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-usebakingsets.html">Baking Set</a></strong>. You can bake different lighting setups into different lighting scenarios, and change which one URP uses at runtime or at design time using the Rendering Debugger.</p>
<table>
<thead>
<tr>
<th>Project<br>글 Console</th>
<th>Rendering Debugger</th>
<th>@ ::</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Reset</td>
</tr>
<tr>
<td>Frequently Used</td>
<td>Display Probes</td>
<td></td>
</tr>
<tr>
<td>Probe Volumes</td>
<td>Debug Probe Sampling</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Virtual Offset Debug</td>
<td></td>
</tr>
<tr>
<td>Rendering</td>
<td>Debug Draw Distance</td>
<td>200</td>
</tr>
<tr>
<td>Material</td>
<td>Probe Adjustment Volumes</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Auto Display Probes</td>
<td>&gt;</td>
</tr>
<tr>
<td>Lighting</td>
<td>Isolate Affected</td>
<td>&gt;</td>
</tr>
<tr>
<td>GPU Resident Drawer</td>
<td>Scenario Blending</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Number Of Cells Blended Per Frame</td>
<td>10000</td>
</tr>
<tr>
<td>Render Graph</td>
<td>Turnover Rate</td>
<td>0.1</td>
</tr>
<tr>
<td>Volume</td>
<td>Scenario Blend Target</td>
<td>Night</td>
</tr>
<tr>
<td></td>
<td>Scenario Blending Factor</td>
<td>0 261</td>
</tr>
</tbody>
</table>
<p>Scenario Blending using the Rendering Debugger</p>
<p>For example, you can create one Lighting Scenario asset for day, and another one for night. At runtime, you can switch or blend between the two.</p>
<table>
<thead>
<tr>
<th></th>
<th>r Baking<br>Baking Mode</th>
<th>Baking Set</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Current Baking Set</td>
<td>് APV-Example Baking Set (Probe Volum () New</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Scenes in Baking Set</td>
<td>Status</td>
<td>Bake :</td>
</tr>
<tr>
<td></td>
<td>�APV-Example</td>
<td>O Active</td>
<td>&gt;<br>+</td>
</tr>
<tr>
<td></td>
<td>Probe Placement</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Probe Positions</td>
<td>Recalculate</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Probe Offset</td>
<td>y o<br>Z 0<br>× 0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Min Probe Spacing</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Max Probe Spacing</td>
<td>9<br>9m<br>27m<br>81m</td>
<td>243m</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Baked Probe Volume data will contain up-to 4 different sizes of Brick.</td>
<td></td>
</tr>
<tr>
<td></td>
<td>&gt; Renderer Filter Settings</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>▼ Lighting Scenarios</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Scenario</td>
<td>Active</td>
<td>Status</td>
</tr>
<tr>
<td></td>
<td>Day</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>1888</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>+ -</td>
</tr>
<tr>
<td></td>
<td>&gt; Sky Occlusion Settings</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Probe Invalidity Settings</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Dilation<br>CARTIA LIMINATION LIANA F</td>
<td>LINEAR LE PARTER PRODUCTION PARTICIPALIES</td>
<td></td>
</tr>
<tr>
<td></td>
<td>▼ Baking</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Baking Mode</td>
<td>Baking Set</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Current Baking Set</td>
<td>APV-Example Baking Set (Probe Volum () New</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Scenes in Baking Set</td>
<td>Status</td>
<td>Bake :</td>
</tr>
<tr>
<td></td>
<td>@ APV-Example</td>
<td>O Active</td>
<td>&gt;</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>+</td>
</tr>
<tr>
<td></td>
<td>▼ Probe Placement</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Probe Positions<br>Probe Offset</td>
<td>Recalculate<br>× 0<br>Y O<br>Z O</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Min Probe Spacing</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Max Probe Spacing</td>
<td>9m<br>81m</td>
<td>243m</td>
</tr>
<tr>
<td>1 c</td>
<td></td>
<td>27m</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Baked Probe Volume data will contain up-to 4 different sizes of Brick.</td>
<td></td>
</tr>
<tr>
<td></td>
<td>► Renderer Filter Settings</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>▼ Lighting Scenarios</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Scenario</td>
<td>Active</td>
<td>Status</td>
</tr>
<tr>
<td></td>
<td>Do</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>= Night</td>
<td></td>
<td>+ -</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>&gt; Sky Occlusion Settings<br>V Probe Invalidity Settings</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Day/night Lighting Scenarios</p>
<ol>
<li>To use a Lighting Scenario asset, go to the active URP Asset and enable <strong>Lighting &gt; Light Probe Lighting &gt; Lighting Scenarios</strong>.</li>
</ol>
<table>
<thead>
<tr>
<th>Lighting</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Main Light</td>
<td>Per Pixel</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Cast Shadows</td>
<td>V</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Shadow Resolution</td>
<td>2048</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Light Probe System</td>
<td>Adaptive Probe Volumes</td>
<td>œ</td>
<td></td>
</tr>
<tr>
<td>Memory Budget</td>
<td>Memory Budget Medium</td>
<td>P</td>
<td></td>
</tr>
<tr>
<td>SH Bands</td>
<td>Spherical Harmonics L1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Enable GPU Streaming</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Enable Disk Streaming</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Enable Lighting Scenarios</td>
<td>&gt;</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Enable Lighting Scenario Blending</td>
<td>V</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Blending Memory Budget</td>
<td>Memory Budget Medium</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Estimated GPU Memory cost: 111 MB.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="_page_78_Picture_0.jpeg" alt=""></p>
<ul>
<li><ol>
<li>To create a new Lighting Scenario asset so you can store baking results inside, do the following:<ul>
<li>a. Open the <strong>Adaptive Probe Volumes</strong> panel in the <strong>Lighting</strong> window.</li>
<li>b. In the <strong>Lighting Scenarios</strong> section, select the <strong>Add</strong> (+) button to add a Lighting Scenario asset.</li>
</ul>
</li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>▼ Lighting Scenarios</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Scenario</td>
<td>Active</td>
<td>Status</td>
</tr>
<tr>
<td></td>
<td>Day</td>
<td>●</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Night<br>==</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>+</td>
</tr>
</tbody>
</table>
<ol>
<li>In the Lighting window, under the <strong>Adaptive Probe Volume</strong> tab, make sure the <strong>Probe Positions</strong> are set to <strong>Don&#39;t Recalculate</strong>. This ensures that Unity will only rebake lighting without changing the probe positions, which could otherwise invalidate previously baked scenarios.</li>
</ol>
<table>
<thead>
<tr>
<th>C Inspector<br>Cighting</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Scene   Adaptive Probe Volumes   Environment</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Realtime Lightmaps Baked Lightmaps</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Baking</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Probe Placement<br>Recalculate</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Probe Positions<br>Probe Offset</td>
<td>V Don&#39;t Recalculate</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Min Probe Spacinc 0.4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li><ol>
<li>To bake into a Lighting Scenario, follow these steps:<ul>
<li>a. In the Lighting Scenarios section, select a Lighting Scenario to make it active.</li>
<li>b. Select <strong>Generate Lighting</strong>. URP stores the baking results in the active Lighting Scenario.</li>
<li>c. Use the drop-down button next to <strong>Generate Lighting</strong> to only focus on the probes if you&#39;re not using lightmaps.</li>
</ul>
</li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th>GPU Baking Profile</th>
<th>Automatic</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Generate Lighting</td>
<td>Bake Probe Volumes</td>
</tr>
<tr>
<td>Scenario Size</td>
<td>7.3 MB</td>
<td>Bake Reflection Probes</td>
</tr>
<tr>
<td>Baking Set Size</td>
<td>18.6 MB</td>
<td>Clear Baked Data</td>
</tr>
</tbody>
</table>
<p><span id="page-79-0"></span>You can set which Lighting Scenario URP uses at runtime using the <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-bakedifferentlightingsetups.html">ProbeReferenceVolume</a> API.</p>
<h4 id="note-">Note:</h4>
<p>If you change the active Lighting Scenarios at runtime, URP changes only the indirect lighting data in the light probes. You might still need to use scripts to move geometry, modify lights or change direct lighting.</p>
<h3 id="fixing-issues-with-apvs">Fixing issues with APVs</h3>
<p><img src="_page_79_Figure_5.jpeg" alt=""></p>
<p>Debug Probe Sampling</p>
<p>To fix issues such as APV artifacts, use <strong>Window &gt; Analysis &gt; Rendering Debugger &gt; Probe Volumes &gt; Debug Probe Sampling</strong> to inspect probes and how they are sampled for a given pixel.</p>
<p><img src="_page_79_Figure_8.jpeg" alt=""></p>
<p>Visualizing Probe Sampling per pixel</p>
<p>Since light probes are added in a grid, placement can sometimes cause rendering errors such as dark areas where it should be light and vice versa. The Editor provides several tools to let a technical artist quickly fix these issues.</p>
<p>Light probes inside geometry are called invalid probes. URP marks a probe as invalid when it fires sampling rays to capture surrounding light data, but the rays hit the unlit backfaces inside geometry. The APV system has several tools to fix these issues.</p>
<table>
<thead>
<tr>
<th>Probe Invalidity Settings</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Dilation</td>
<td>&gt;</td>
<td></td>
</tr>
<tr>
<td>Search Radius</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>Validity Threshold</td>
<td></td>
<td>0.75</td>
</tr>
<tr>
<td>Dilation Iterations</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Squared Distance We ✔</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Virtual Offset</td>
<td>V</td>
<td></td>
</tr>
<tr>
<td>Validity Threshold</td>
<td></td>
<td>0.75</td>
</tr>
<tr>
<td>Search Distance Multi-</td>
<td></td>
<td>0.2</td>
</tr>
<tr>
<td>Geometry Bias</td>
<td></td>
<td>0.01</td>
</tr>
<tr>
<td>Ray Origin Bias</td>
<td></td>
<td>-0.001</td>
</tr>
<tr>
<td>Layer Mask</td>
<td>Default, TransparentFX, Water, UI</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Refresh Virtual Offset Debug</td>
</tr>
</tbody>
</table>
<p>The Probe Invalidity Settings available in the <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-lighting-panel-reference.html">Adaptive Probe Volumes panel</a></p>
<p><strong>Virtual Offset</strong> tries to make invalid light probes valid, by moving their capture points so they&#39;re outside any colliders. And <strong>Dilation</strong> detects light probes that remain invalid after Virtual Offset, and gives them data from valid probes nearby.</p>
<p><img src="_page_80_Picture_6.jpeg" alt=""></p>
<p>You can check which light probes are invalid using the Rendering Debugger<strong>.</strong></p>
<p>In the left-side scene in the image above, Virtual Offset isn&#39;t active and dark bands are visible. In the scene on the right side, Virtual Offset is active.</p>
<p><span id="page-81-0"></span><img src="_page_81_Picture_1.jpeg" alt=""></p>
<p>In the left-side scene in the image above, Dilation isn&#39;t active and some areas are too dark. In the scene on the right, Dilation is active.</p>
<h4 id="-light-leaks-"><strong>Light leaks</strong></h4>
<p>Light leaks are areas that are too light or dark, often in the corners of a wall or ceiling.</p>
<p><img src="_page_81_Picture_5.jpeg" alt=""></p>
<p>A light leak</p>
<p>Light leaks often occur when geometry receives light from a light probe that isn&#39;t visible to the geometry, for example due to the light probe being on the other side of a wall. APVs use regular grids of light probes, so light probes might not follow walls or be at the boundary between different lighting areas.</p>
<p><span id="page-82-0"></span><img src="_page_82_Picture_0.jpeg" alt=""></p>
<p>Try the following techniques to fix light leaks:</p>
<ul>
<li><a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-fixissues.html#create-thicker-walls">Create thicker walls.</a></li>
<li>Add an <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-fixissues.html#add-an-adaptive-probe-volumes-options-override-to-your-scene">Adaptive Probe Volumes Options override</a> to your scene:<ul>
<li>Add a <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/volumes-landing-page.html">Volume,</a> then an <strong>Adaptive Probe Volumes Options</strong> override to the Volume. This adjusts the position that GameObjects use to sample the light probes.</li>
</ul>
</li>
<li><a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-fixissues.html#layers">Enable Rendering Layers</a>:<ul>
<li>In the Lighting window, configure the <strong>Rendering Layer Masks</strong> in the <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-lighting-panel-reference.html">Adaptive</a>  <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-lighting-panel-reference.html">Probe Volumes panel</a> to allow the APV to assign a Rendering Layer Mask to each light probe.</li>
</ul>
</li>
<li><a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-fixissues.html#probevolumesettings">Adjust Baking Set properties</a>:<ul>
<li>If adding a Volume doesn&#39;t work, use the Adaptive Probe Volumes panel in the Lighting window to adjust Virtual Offset and Dilation settings.</li>
</ul>
</li>
<li><a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-fixissues.html#probevolumeadjustment">Use a Probe Adjustment Volume</a> component:<ul>
<li>Use this component to make light probes invalid in a small area. This triggers Dilation during baking, and improves the results of Leak Reduction Mode at runtime.</li>
</ul>
</li>
</ul>
<h4 id="-rendering-layers-"><strong>Rendering Layers</strong></h4>
<p>When switching the URP 3D Sample oasis environment from using light probes/lightmaps to using APV only, an issue arises with light leaks, which you can see on the bright roof and wall in the image below.</p>
<p><img src="_page_82_Picture_13.jpeg" alt=""></p>
<p>Light leaking in the tent in the oasis environment from the URP 3D Sample</p>
<p>This is because some pixels are blending between probes on the inside and outside of the tent. By using <strong>Window &gt; Analysis &gt; Rendering Debugger &gt; Probe Volumes &gt; Debug Probe Sampling</strong>, you can spot which probes are used when interpolating the value for a pixel.</p>
<p><img src="_page_83_Picture_2.jpeg" alt=""></p>
<p>Viewing the interpolated probes for a pixel</p>
<p>One option to fix this is to use a Volume to modify how the APV is sampled at runtime using the <strong>Adaptive Probe Volume Options</strong> override. The <strong>NormalBias</strong> and <strong>ViewBias</strong> settings can be useful for adjusting the sampling position: NormalBias pushes it along the normal (away from walls), while ViewBias pushes it towards the camera (keeping it on the same side of the wall as the camera). When you change these properties in the Volume, you can see the updates in real-time in both the lighting results and the <strong>Debug Probe Sampling View</strong>, where the sampling position and weights are updated accordingly. But a better option is to use Rendering Layers.</p>
<p>APVs support Rendering Layers, allowing you to create up to four different masks and restrict sampling to those specific masks for certain objects. This can be useful to prevent interior objects from sampling exterior probes, or vice versa. Activate and add them using <strong>Window &gt; Rendering &gt; Lighting &gt; Adaptive Probe Volumes &gt; Rendering Layers</strong>.</p>
<table>
<thead>
<tr>
<th>T Rendering Layers</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Rendering Layer Masks</td>
<td>&gt;</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Exterior<br>ll</td>
<td></td>
<td>Default</td>
<td>D</td>
</tr>
<tr>
<td>Interior</td>
<td></td>
<td>Interior</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>You&#39;ll also need to add a layer via <strong>Project Settings &gt; Tags and Layers &gt; Rendering Layers</strong>:</p>
<table>
<thead>
<tr>
<th>Tags and Layers<br>A la fin Rim Billing in a &quot;</th>
<th></th>
<th>0 花</th>
</tr>
</thead>
<tbody>
<tr>
<td>► Layers</td>
<td></td>
<td></td>
</tr>
<tr>
<td>▼ Rendering Layers</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Layer O</td>
<td>Default</td>
<td></td>
</tr>
<tr>
<td>Layer 1</td>
<td>Screen</td>
<td></td>
</tr>
<tr>
<td>Layer 2</td>
<td>Lamps</td>
<td></td>
</tr>
<tr>
<td>Layer 3</td>
<td>Interior</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>+</td>
</tr>
</tbody>
</table>
<p>To implement this, edit the meshes themselves to ensure they are divided between the different areas you want to create. In this project for example, the meshes are edited to separate the interior and exterior into multiple meshes. Once the meshes are split, assign the correct Rendering Layers to them, and specify which ones APV should use in the <strong>Adaptive Probe Volume Tab</strong>.</p>
<p>You don&#39;t need to assign layers to every object in the tent, only to those susceptible to leaking, like the walls or objects near the walls.</p>
<p>When generating lighting, the system will automatically assign layers to the probes during the bake process based on the nearby objects, eliminating the need to manually assign layers per probe. To facilitate this automatic probe assignment, you need to assign layers to larger objects. In the oasis environment tent example, the interior layer is assigned to the walls and ceiling of the tent to ensure that most of the interior probes hit them during baking and are automatically assigned to the interior mask. Probes are assigned to the layer they encounter most frequently.</p>
<p>Once this is done, click <strong>Generate Lighting</strong> and observe that leaking is eliminated for the tent, thanks to the separate interior and exterior masks.</p>
<p><img src="_page_84_Picture_5.jpeg" alt=""></p>
<p>Light leaks without and with rendering layers</p>
<p>Get more information <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-fixissues.html">here</a> about fixing issues with APVs</p>
<h2 id="-span-id-page-85-0-span-streaming-apvs"><span id="page-85-0"></span>Streaming APVs</h2>
<p><a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-streaming.html">APV streaming</a> enables you to use APV-based lighting in large worlds. APV streaming bakes APV data that&#39;s larger than the available CPU or GPU memory, and loads it at runtime when it&#39;s needed. At runtime, as the camera moves, URP loads only APV data from cells within the camera&#39;s view frustum.</p>
<p>You can enable and disable streaming for different URP quality levels. Enable streaming with the following steps:</p>
<ul>
<li><ol>
<li>Select <strong>Edit &gt; Project Settings &gt; Quality</strong> from the main menu.</li>
</ol>
</li>
<li><ol>
<li>Select a Quality Level.</li>
</ol>
</li>
<li><ol>
<li>Double-click the Render Pipeline Asset to open it in the Inspector.</li>
</ol>
</li>
<li><ol>
<li>Expand the Lighting tab.</li>
</ol>
</li>
<li><ol>
<li>You can now enable two types of streaming:<ul>
<li>a. Enable Disk Streaming to stream from disk to CPU memory.</li>
<li>b. Enable GPU Streaming to stream from CPU memory to GPU memory. You must enable Enable Disk Streaming first.</li>
</ul>
</li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th>T Lighting</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Main Light</td>
<td>Per Pixel</td>
<td>D</td>
</tr>
<tr>
<td>Cast Shadows</td>
<td>V</td>
<td></td>
</tr>
<tr>
<td>Shadow Resolution</td>
<td>2048</td>
<td>D</td>
</tr>
<tr>
<td>Light Probe System</td>
<td>Adaptive Probe Volumes</td>
<td></td>
</tr>
<tr>
<td>Memory Budget</td>
<td>Memory Budget Medium</td>
<td>D</td>
</tr>
<tr>
<td>SH Bands</td>
<td>Spherical Harmonics L1</td>
<td></td>
</tr>
<tr>
<td>Enable GPU Streaming</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Enable Disk Streaming</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Enable Lighting Scenarios</td>
<td>V</td>
<td></td>
</tr>
<tr>
<td>Enable Lighting Scenario Blending</td>
<td>&gt;</td>
<td></td>
</tr>
<tr>
<td>Blending Memory Budget</td>
<td>Memory Budget Medium</td>
<td></td>
</tr>
<tr>
<td>Estimated GPU Memory cost: 111 MB.</td>
<td></td>
</tr>
</tbody>
</table>
<p>You can configure streaming settings in the same window. Refer to URP Asset for more information.</p>
<p><span id="page-86-0"></span><img src="_page_86_Picture_0.jpeg" alt=""></p>
<h4 id="-debug-streaming-"><strong>Debug streaming</strong></h4>
<p>The smallest section URP loads and uses is a cell, which is the same size as the largest brick in an APV. You can influence the size of cells in an APV by adjusting the density of light probes</p>
<p>Use the Rendering Debugger to view the cells in an APV or debug streaming.</p>
<p><img src="_page_86_Picture_5.jpeg" alt=""></p>
<p>APV Streaming</p>
<h3 id="sky-occlusion">Sky occlusion</h3>
<p>Sky occlusion is the process whereby if a GameObject samples a color from the sky, Unity will dim the color if the light can&#39;t reach the GameObject. Sky occlusion in Unity uses the sky color from the ambient probe, which updates at runtime. This means you can dynamically light GameObjects as the sky color changes. For example, you can change the sky color from light to dark to simulate the effect of a day-night cycle.</p>
<p><strong>Note</strong>: If you enable sky occlusion, APVs might take longer to bake, and Unity might use more memory at runtime.</p>
<p>When you <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes-skyocclusion.html">enable sky occlusion</a>, Unity bakes an additional static sky occlusion value into each probe in an APV. The sky occlusion value is the amount of indirect light the probe receives from the sky, including light that bounced off static GamesObjects.</p>
<p>The main benefit of using sky occlusion is you can modify the sky lighting at runtime.</p>
<p><img src="_page_87_Picture_1.jpeg" alt=""></p>
<p>Let&#39;s look at the series of images on the left to illustrate this:</p>
<ul>
<li>The top image shows the problem that occurs when you can&#39;t bake the sky lighting because you need it to change at runtime. In this image only an ambient probe is used with no baking resulting in a poor result.</li>
<li>In the second to fifth images the ambient probe is used together with sky occlusion. You could also light this image with a regular APV bake, with sky occlusion disabled but then the lighting would not change at runtime.</li>
</ul>
<p>An example of the results of using sky occlusion in a scene. The images are from the Unity Asset Store package <a href="https://assetstore.unity.com/packages/tools/particles-effects/azure-sky-dynamic-skybox-36050">Azure[Sky] Dynamic Skybox</a> by 7stars.</p>
<p><img src="_page_88_Picture_0.jpeg" alt=""></p>
<p>Follow these steps to enable sky occlusion:</p>
<ul>
<li><ol>
<li>Enable the GPU Light Baker (formerly called Progressive GPU Lightmapper). Unity doesn&#39;t support sky occlusion if you use Progressive CPU. Go to <strong>Window &gt; Rendering &gt; Lighting</strong>.</li>
</ol>
</li>
<li><ol>
<li>Go to the Scene panel.</li>
</ol>
</li>
<li><ol>
<li>Set Lightmapper to Progressive GPU.</li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th>▼ Lightmapping Settings</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Lightmapper</td>
<td>Progressive GPU</td>
<td></td>
</tr>
<tr>
<td>Importance Sampling</td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li><ol>
<li>Open the Adaptive Probe Volumes panel.</li>
</ol>
</li>
<li><ol>
<li>Enable Sky Occlusion.</li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th>Sky Occlusion Settings</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Sky Occlusion</td>
<td>&gt;</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Samples</td>
<td></td>
<td></td>
<td></td>
<td>2048</td>
</tr>
<tr>
<td>Bounces</td>
<td></td>
<td></td>
<td></td>
<td>2</td>
</tr>
<tr>
<td>Albedo Override</td>
<td></td>
<td></td>
<td></td>
<td>0.6</td>
</tr>
<tr>
<td>Sky Direction</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>To update the lighting data, you must also bake the APV after you enable or disable sky occlusion. Once the sky occlusion is baked, the scene lighting will respond to the ambient probe updates. In URP, the ambient probe is updated in real-time only when using the Color or Gradient Mode, not the Skybox mode. This means you&#39;ll probably have to manually animate a color to match the animated sky visuals.</p>
<p><strong>Note</strong>: URP now supports per-vertex quality sampling for probes. This is especially useful to boost performance on lower-end devices. To set the sampling mode use the <strong>URP Asset</strong> in the Lighting section. <strong>Advanced Properties</strong> must be active to view the option; press the ellipsis at the top right of the Lighting panel to activate it. With Advanced Properties active, the <strong>SH Evaluation Mode</strong> drop-down will appear.</p>
<table>
<thead>
<tr>
<th>AND OF OF<br>SH Evaluation Mode</th>
<th>V Auto</th>
</tr>
</thead>
<tbody>
<tr>
<td>Shadows</td>
<td>Per Vertex</td>
</tr>
<tr>
<td>Post-processing</td>
<td>Mixed</td>
</tr>
<tr>
<td>Volumes</td>
<td>Per Pixel</td>
</tr>
</tbody>
</table>
<h4 id="-more-information-"><strong>More Information</strong></h4>
<ul>
<li>Adaptive Probe Volume<a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@17.0/manual/probevolumes.html">s documentation</a></li>
<li>GDC 2023 session: <a href="https://www.youtube.com/watch?v=iU7X5xICkc8">Efficient and impactful lighting with Adaptive Probe Volumes</a></li>
</ul>
<h3 id="-span-id-page-89-0-span-light-probes-vs-apvs"><span id="page-89-0"></span>Light probes vs APVs</h3>
<p><img src="_page_89_Picture_2.jpeg" alt=""></p>
<p>Light Probe Groups in use in the top image, and APVs in the bottom image; images are from the Unity Asset Store package <a href="https://assetstore.unity.com/packages/3d/environments/industrial/archvizpro-photostudio-urp-225832">ArchVizPRO</a>  <a href="https://assetstore.unity.com/packages/3d/environments/industrial/archvizpro-photostudio-urp-225832">Photostudio URP</a> by ArchVizPro</p>
<p>The bottom image shows how smoothly a transition from dark to light works with APV. In the top image, the Light Probe Group results in a bright light on the car door because a single interpolated probe is used per object. This is because the door is a separate GameObject to the rest of the door and uses a different probe, resulting in a rendering error.</p>
<p><img src="_page_90_Picture_0.jpeg" alt=""></p>
<p>The table below compares the features of light probes and APVs.</p>
<table>
<thead>
<tr>
<th>Light Probe Groups</th>
<th>Adaptive Probe Volumes</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Time-consuming to place probes and move</td>
<td>Fast to place and easy to update as</td>
<td></td>
<td></td>
</tr>
<tr>
<td>them if geometry changes</td>
<td>geometry changes</td>
<td></td>
<td></td>
</tr>
<tr>
<td>A single interpolated probe is used for</td>
<td>Each pixel is individually lit:</td>
<td></td>
<td></td>
</tr>
<tr>
<td>lighting objects:</td>
<td>—</td>
<td></td>
<td></td>
</tr>
<tr>
<td>—</td>
<td>This ensures smooth transitions.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Objects cannot transition well from</td>
<td>—</td>
<td></td>
<td></td>
</tr>
<tr>
<td>darkness to light and stand out.</td>
<td>Volumetric effects work well using</td>
<td></td>
<td></td>
</tr>
<tr>
<td>—</td>
<td>APV because the APV grid is easy to</td>
<td></td>
<td></td>
</tr>
<tr>
<td>It can cause problems for big objects.</td>
<td>sample at any location.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Static objects are usually lit using light maps.<br>Only dynamic objects use probes.</td>
<td>No need for lightmaps or lightmap UVs:<br>—<br>Use a single lighting solution for all<br>objects in a scene.<br>—<br>Light large worlds with a constrained<br>memory budget.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Probes can be freely placed and moved at</td>
<td>Probes are placed in a grid structure and</td>
<td></td>
<td></td>
</tr>
<tr>
<td>runtime.</td>
<td>cannot be moved at run time.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Switch GI is not supported.</td>
<td>The Lighting Scenario asset allows for<br>switching between different lighting, e.g.,<br>from day to night, turning a light on or off,<br>and so on.</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="-span-id-page-91-0-span-screen-space-refraction"><span id="page-91-0"></span>Screen space refraction</h2>
<p><img src="_page_91_Picture_1.jpeg" alt=""></p>
<p>In the indie game <em><a href="https://www.playarctico.com/">Arctico</a></em>, by the developers Claudio and Antonio, you have to build your base camp and explore a glacier landscape. The abundant water in the game reflects the surface, an effect that can be achieved with screen space reflection. Screen space reflection is used to fake a reflective surface in real-time, while screen space refraction is used to simulate transparency and the bending of light as it passes through a medium.</p>
<p><img src="_page_92_Picture_1.jpeg" alt=""></p>
<p>An example of screen space refraction</p>
<p>Screen space refraction uses the current opaque texture created by the render pipeline as the source texture to map pixels to the model being rendered. It can&#39;t show models that are not part of the opaque texture. The method is about deforming the UV used to sample the image.</p>
<p>In this recipe, you&#39;ll learn how to use a normal map to create refraction effects as well as tint a refraction effect. The additional tinting seen in the image above is achieved by lerping the calculated pixel color with a <strong>Color</strong> property.</p>
<p>To see the effect in action, take a look at <strong>Scenes &gt; Refraction &gt; Refraction</strong>.</p>
<p>The technique requires the opaque texture to be available to the shader. Find the URP Settings Asset currently assigned in <strong>Edit &gt; Project Settings… &gt; Graphics &gt; Scriptable Render Pipeline Settings</strong>. In the Inspector, make sure <strong>Opaque Texture</strong> is enabled. If you also enable Opaque Downsampling, you&#39;ll get a small performance boost. It also introduces a small blur to what you see through the refractive object, which can improve the visual appearance.</p>
<p><img src="_page_93_Picture_0.jpeg" alt=""></p>
<table>
<thead>
<tr>
<th>Depth Texture</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Opaque Texture</td>
<td>V</td>
<td></td>
</tr>
<tr>
<td>Opaque Downsampling   2x Bilinear</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Terrain Holes</td>
<td>&gt;</td>
</tr>
</tbody>
</table>
<p>Setting Opaque Texture and Opaque Downsampling</p>
<p>The first step to creating the shader is to create a new Shader Graph Asset. Right-click in the Project window, and select <strong>Create &gt; Shader Graph &gt; URP &gt; Lit Shader Graph.</strong></p>
<table>
<thead>
<tr>
<th>Shader Graph</th>
<th>URP</th>
<th>Lit Shader Graph</th>
</tr>
</thead>
<tbody>
<tr>
<td>Shader</td>
<td>Builtin</td>
<td>Unlit Shader Graph</td>
</tr>
<tr>
<td>Shader Variant Collection<br>Testing</td>
<td>Blank Shader Graph<br>Sub Graph</td>
<td>Sprite Custom Lit Shader Graph<br>Sprite Unlit Shader Graph</td>
</tr>
<tr>
<td>Playables<br>Assembly Definition</td>
<td>Custom Render Texture</td>
<td>Sprite Lit Shader Graph</td>
</tr>
</tbody>
</table>
<p>Creating a new Lit Shader Graph</p>
<p>Create a material using this shader by selecting the Shader Graph Asset and choosing <strong>Create &gt; Material</strong>. Apply this material to the object you want to be refractive.</p>
<p>Now double-click on the Shader Graph Asset to open it. Create a <strong>Scene Color</strong> node, and connect this to <strong>Fragment &gt; Base Color.</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Vertex</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Object Space · · O Position(3)</td>
</tr>
<tr>
<td></td>
<td>Object Space · O Normal(3)</td>
</tr>
<tr>
<td></td>
<td>Object Space · · O Tangent(3)</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>Scene Color</td>
<td>Fragment</td>
</tr>
<tr>
<td>Default &gt; 0- 0 UV(4) Out(3) 0-</td>
<td>@ Base Color(3)</td>
</tr>
<tr>
<td></td>
<td>Tangent Space · · O Normal (Tangent Space)(3)</td>
</tr>
</tbody>
</table>
<p>Using a Scene Color node</p>
<p>Scene Color only works with transparent materials since it relies on opaque objects having been rendered. In the render pipeline, transparent objects are rendered after opaque objects. Set <strong>Graph Inspector &gt; Graph Settings &gt; Surface Type</strong> to <strong>Transparent</strong>.</p>
<p><img src="_page_94_Picture_0.jpeg" alt=""></p>
<p>The Scene Color node by default uses normalized screen coordinates for the UV and so maps the opaque texture to each pixel with lighting affected by the smoothness, resulting in the image below.</p>
<p><img src="_page_94_Picture_3.jpeg" alt=""></p>
<p>The result of using Scene Color</p>
<p>Since the goal is to manipulate the UV used by the Scene Color node, you need to override the default UV behavior. Create a <strong>Screen Position</strong> node and an <strong>Add</strong> node. Drag the output of the Screen Position node to input A of the Add node and set the B input as [0.1, 0.1, 0, 0].</p>
<p><img src="_page_94_Figure_6.jpeg" alt=""></p>
<p>Adding nodes to control the UV</p>
<p><img src="_page_95_Picture_0.jpeg" alt=""></p>
<h4 id="now-you-ll-see-the-opaque-texture-offset-">Now you&#39;ll see the Opaque Texture offset.</h4>
<p><img src="_page_95_Picture_3.jpeg" alt=""></p>
<p>Opaque Texture offset</p>
<p>For each rendered pixel, you want the offset to be controlled by the camera&#39;s view direction, the normal for the object at the current screen position, and a scaling value. Shader Graph has a node that will calculate refraction given these three inputs; it actually has two scaling values, but you&#39;ll only use one.</p>
<p>You can add a new float property called <strong>IOR</strong>, short for <a href="https://en.wikipedia.org/wiki/Refractive_index">Index of Refraction</a> for scaling. Set it as a slider, with min 1 and max 6. For view direction, add a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/View-Direction-Node.html?q=view direction">View Direction</a> node and link it to a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Normalize-Node.html?q=Normalize">Normalize</a> node to guarantee it&#39;s of unit length.</p>
<p>Add a Normal node set to World Space, and again link it to a Normalize node. Create a Refract node, and link the normalized View Direction to the Incident input. Link the normalized Normal to the Refract node Normal input and link the IOR property to the IORMedium input.</p>
<p><img src="_page_96_Picture_0.jpeg" alt=""></p>
<p>At this point, the Refracted output is in World space, but to offset the Screen space UV we need it in Tangent space. Add a Transform node setting the input as World and the output as Tangent. For the type, choose Direction. Use this as the A input to the Add node with Screen Position as the B input. You get the graph you see below.</p>
<p><img src="_page_96_Figure_3.jpeg" alt=""></p>
<p>Basic refraction graph</p>
<p><img src="_page_96_Picture_5.jpeg" alt=""></p>
<p>An IOR of 5.44 will result in the visual effect seen in the following image.</p>
<p>Basic Screen Space Refraction</p>
<p><img src="_page_97_Picture_0.jpeg" alt=""></p>
<p>You can tint the result by adding a <strong>Color</strong> property. Add a Lerp node, and use an <strong>Opacity</strong> property set to slider mode, range 0-1, as the T input. The output from Scene Color is set as input A and Color as input B.</p>
<p><img src="_page_97_Figure_3.jpeg" alt=""></p>
<p>Adding a tinting stage to the graph</p>
<p><img src="_page_98_Picture_0.jpeg" alt=""></p>
<h4 id="you-should-now-be-able-to-tint-the-output-">You should now be able to tint the output.</h4>
<p><img src="_page_98_Picture_3.jpeg" alt=""></p>
<p>A tinted version</p>
<p>The normal affects the refraction, so a single plane will just get an offset version of the Opaque Texture.</p>
<p>Now it&#39;s time to add a normal map. You start by adding a <strong>Texture 2D</strong> property to the shader that you name <strong>Normal Map</strong>, and a float property as a slider, called <strong>Normal Strength</strong>, with a range of 0-1. Create a Sample Texture 2D node, and set it as <strong>Type Normal, Space Tangent</strong>. Set the <strong>Texture</strong> input to the <strong>Normal Map</strong> property. Create a <strong>Normal Strength</strong> node and set input as the RGBA(4) output from the Sample Texture 2D node. Set the <strong>Strength</strong> input as the <strong>Normal Strength</strong> property. Create an Add node with input A as the output from the Normal Strength node and input B from the Transform World to the Tangent node. Follow these steps, and you should end up with this graph.</p>
<p><img src="_page_99_Figure_2.jpeg" alt=""></p>
<p>Adding a normal map</p>
<p><img src="_page_100_Picture_0.jpeg" alt=""></p>
<p>Using a suitable normal map should result in the effect seen in the image below, in this case using a single quad instead of the diamond. Refraction for a planar mesh simply shows an offset of the Opaque Texture. Using a normal map with a planar mesh can be a useful way to hide this artifact.</p>
<p><img src="_page_100_Picture_3.jpeg" alt=""></p>
<p>Using a Normal Map</p>
<p>An alternative to using the Refract node is to add a Custom Function Node with a Vector3 viewDir input, a Vector3 normal input, and an IOR input. If you use this option, set your IOR property as a slider with the range -0.15 to 2, not 1-6. Set a Vector3 as the output. The code is very simple so just use a String not a file:</p>
<p>Out = refract(viewDir, normal, IOR);</p>
<p>It gives different results and is worth experimenting with.</p>
<h4 id="-more-resources-"><strong>More resources</strong></h4>
<ul>
<li><a href="https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-refraction.html">Screen space refraction</a> by David Lettier</li>
<li><a href="https://github.com/Steven-Cannavan/URP_ScreenSpacePlanarReflections">ScreenSpace planar reflection</a> GitHub repo by Steven Cannavan</li>
<li><a href="https://bootcamp.uxdesign.cc/reflection-probe-vs-screen-space-reflection-4b134c46ca16">Reflection probes vs Screen space reflection</a> by Kyle W. Powers</li>
<li><a href="https://www.youtube.com/watch?v=C5YfSmSLZHI">Shader Graph refraction</a> tutorial by AE Tuts</li>
<li><a href="https://www.youtube.com/watch?v=Qri_fN01hMc">Crystal Shader Graph in Unity</a> by Binary Lunar</li>
</ul>
<h2 id="-span-id-page-101-0-span-volumetrics"><span id="page-101-0"></span>Volumetrics</h2>
<p><img src="_page_101_Picture_1.jpeg" alt=""></p>
<p>The game <em><a href="https://www.ea.com/en-gb/games/lost-in-random">Lost in Random</a></em> by Zoink! immerses players into a fantasy kingdom with a very unique art direction where great lighting plays a huge role in creating its atmosphere. They recreated volumetric fog in URP, as seen in this <a href="https://agentlien.github.io/fog/">article.</a></p>
<p><span id="page-102-0"></span><img src="_page_102_Picture_1.jpeg" alt=""></p>
<p>Volumetric cloud</p>
<p>This is a recipe for using ray marching to render a 3D texture. Unity supports 3D textures, which are an array of images placed in a grid on a single texture, rather like a Texture Atlas. The difference is that each image is the same size. Using a 3D UV value, you can source a texel from the grid of images with UV.Z defining the row and column of the individual image to use. The image below shows a typical 3D texture, its import settings, and a preview in the Inspector.</p>
<p><img src="_page_102_Picture_4.jpeg" alt=""></p>
<p>Left to right: A 3D texture, its import settings, and a preview of it in the Inspector</p>
<p><img src="_page_103_Picture_0.jpeg" alt=""></p>
<p><img src="_page_103_Picture_1.jpeg" alt=""></p>
<p>Here is a great <a href="https://www.youtube.com/shorts/IGx_I4nzfxQ">YouTube short</a> explaining how to use <a href="https://www.blender.org/">Blender</a> to create a 3D cloud texture.</p>
<p>YouTube short explaining how to create a 3D cloud texture</p>
<p>As with the previous recipes, this shader will be built with Shader Graph. To view the finished product, go to <strong>Scenes &gt; Volumetric Clouds</strong>, and open the <strong>VolumetricClouds</strong> scene. Note that the scene includes a Camera, Directional Light, and a cube. The cube uses the Material RaymarchMat.</p>
<p>To start the recipe, you&#39;ll need to give the RaymarchMat material the shader named <strong>Shader Graphs/Raymarchv1SG,</strong> created by Nik Lever. You should now see a sphere. If you adjust the <strong>densityScale</strong>, you can begin to see transparency at the edges.</p>
<p><img src="_page_103_Picture_6.jpeg" alt=""></p>
<p>Using the Shader Graph Raymarchv1SG</p>
<p>You&#39;re supposed to be rendering a cube, but instead you see a sphere: What&#39;s going on? The answer is <a href="https://en.wikipedia.org/wiki/Ray_marching">ray marching.</a> Ray marching, according to its <a href="https://en.wikipedia.org/wiki/Ray_marching#:~:text=Ray marching is a class,some function at each step.">Wikipedia page,</a> &quot;is a class of rendering methods for 3D computer graphics where rays are traversed iteratively, effectively dividing each ray into smaller ray segments, sampling some function at each step. This function can encode volumetric data for volume ray casting, distance fields for accelerated intersection finding of surfaces, among other information.&quot;</p>
<p><img src="_page_104_Figure_2.jpeg" alt=""></p>
<p>Ray marching</p>
<p>With this first version, a sphere is defined using a Vector4. XYZ defines the position of the sphere, relative to the object and W its radius. For each pixel, a direction is calculated for a ray that comes directly from the camera (represented by the dotted gray line in the diagram above). Set a density value to 0, then move along this line calculating at each blue dot inside the sphere to add a small value to density. When the ray has traveled through the sphere, you&#39;ll have a value for how much of the sphere is in a line directly from the camera to the pixel you&#39;re rendering. This density value is used as the Base Color in the Shader Graph. Ignore the Sun and the red dots for now; these will be considered later when it&#39;s time to add lighting, in the fourth version of this shader.</p>
<p>This graph uses a Custom Function Node based on the file via <strong>Scripts &gt; HLSL &gt; Raymarch. hlsl</strong>. For this first version, you&#39;ll use the function raymarchv1. The variable density is initialized to 0. Then you enter a for loop for numSteps count. The rayOrigin is moved by stepSize in the direction defined by rayDirection.</p>
<p>How far are you from the sphere origin? You can use the HLSL function distance to calculate the length of a vector from the sphere origin to the current value for rayOrigin. If this is less than the sphere radius (Sphere.w), then add 0.1 to the density value. The output value result is the accumulated density value times densityScale.</p>
<p><img src="_page_105_Picture_0.jpeg" alt=""></p>
<pre><code><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">raymarchv1_float</span>(<span class="hljs-params"> float3 rayOrigin, float3 rayDirection, <span class="hljs-keyword">float</span> numSteps, 
 <span class="hljs-keyword">float</span> stepSize, <span class="hljs-keyword">float</span> densityScale, float4 Sphere, 
 <span class="hljs-keyword">out</span> <span class="hljs-keyword">float</span> result </span>)
</span>{
     <span class="hljs-keyword">float</span> density = <span class="hljs-number">0</span>;
     <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i =<span class="hljs-number">0</span>; i&lt; numSteps; i++){
 rayOrigin += (rayDirection*stepSize);

 <span class="hljs-comment">//Calculate density</span>
 <span class="hljs-keyword">float</span> sphereDist = distance(rayOrigin, Sphere.xyz);
 <span class="hljs-keyword">if</span>(sphereDist &lt; Sphere.w){
 density += <span class="hljs-number">0.1</span>;
 }

     }
     result = density * densityScale;
}
</code></pre><p>For your calculations, you&#39;ll work in Object Space. You get the rayOrigin using a Position node, and to get the rayDirection you need a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Camera-Node.html?q=camera">Camera</a> node that links the position output to a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Transform-Node.html?q=transform">Transform</a> node, with the input set as World and the output Object.</p>
<p>You now have the pixel position and Camera Position in Object Space, which enables you to get the ray direction using a <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/Subtract-Node.html?q=Subtract">Subtract</a> node, with Position as input A and Camera Position as input B. This rayDirection is normalized using a Normalize Node. The other inputs to the Custom Function Node are the float properties, numSteps, number of blue dots per ray, stepSize, the distance between blue dots, densityScale, and the Vector4 sphere discussed earlier. The density output goes directly to Base Color and Alpha. Note that this shader is set to be transparent and unlit, requiring you to calculate the lighting.</p>
<p><img src="_page_106_Picture_0.jpeg" alt=""></p>
<p><img src="_page_106_Figure_2.jpeg" alt=""></p>
<p>Version 1 of the Ray march shader</p>
<p>Ray marching comes to life when a 3D texture is added to determine the shape. You&#39;ll introduce a 3D texture in version 2. Start by setting RaymarchMat to use <strong>Shader Graphs &gt; Raymarch2SG.</strong> The Custom Function used is raymarchv2.</p>
<pre><code><span class="hljs-type">void</span> raymarchv2_float( float3 rayOrigin, float3 rayDirection, <span class="hljs-type">float</span> numSteps, 
 <span class="hljs-type">float</span> stepSize, <span class="hljs-type">float</span> densityScale, UnityTexture3D volumeTex,
 UnitySamplerState volumeSampler, float3 <span class="hljs-keyword">offset</span>, 
 <span class="hljs-keyword">out</span> <span class="hljs-type">float</span> result )
{
     <span class="hljs-type">float</span> density = <span class="hljs-number">0</span>;
     <span class="hljs-type">float</span> transmission = <span class="hljs-number">0</span>;
     <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">0</span>; i&lt; numSteps; i++){
 rayOrigin += (rayDirection*stepSize);

 <span class="hljs-comment">//Calculate density</span>
 <span class="hljs-type">float</span> sampledDensity = SAMPLE_TEXTURE3D(volumeTex, volumeSampler, rayOrigin + 
<span class="hljs-keyword">offset</span>).r;
 density += sampledDensity;

     }
     result = density * densityScale;
}
</code></pre><p><img src="_page_107_Picture_0.jpeg" alt=""></p>
<p>You&#39;ll notice there are three new inputs:</p>
<ul>
<li>A UnityTexture3D volumeTex that comes directly from a Material property.</li>
<li>A MACRO, SAMPLE_TEXTURE3D, necessary when working with 3D textures, that needs a SamplerState instance.<ul>
<li>There is a node for SamplerState that allows you to select the wrapping option. You&#39;ll set that to clamp so that UV values outside of the range 0 - 1 are clamped at 0 for values less than 0 and at 1 for values above 1.</li>
</ul>
</li>
<li>Offset, which is a value you can use to move around our 3D texture inside the Cube.</li>
</ul>
<p>Now, instead of checking whether you are inside a sphere, you&#39;ll get a sampledDensity value using the float3 sample position of rayOrigin plus offset. You only need one channel here, the red channel, R.</p>
<p>The image below shows a render of version 2. It&#39;s beginning to look like a cloud.</p>
<p><img src="_page_107_Picture_9.jpeg" alt=""></p>
<p>Version 2 of the shader</p>
<p><img src="_page_108_Picture_0.jpeg" alt=""></p>
<p>The final version of the shader introduces lighting. Use the shader named Shader Graphs/ Raymarchv3SG for the Material RaymarchMat. This time, you&#39;ll use the function raymarch. The function uses six new parameters: <strong>numLightSteps</strong>, <strong>lightStepSize</strong>, <strong>lightDir, lightAbsorb</strong>, and <strong>transmittance</strong>, and returns a float3 vector.</p>
<p>To build up the final values, initialize three new variables: <strong>transmission</strong>, <strong>lightAccumulation</strong> and <strong>finalLight</strong>. The code is the same as version 2 up to the light loop comment. Look again at the &quot;ray marching&quot; illustration shown earlier: For each step along the view direction ray, represented by the blue dots, you get a ray towards the main light, which is yellow in the diagram. The red dots represent the step-by-step sampling of the 3D texture. The more cloud you find, the less light that will hit that part of the view direction ray. This process determines how bright each pixel is.</p>
<pre><code>void raymarch_float( <span class="hljs-built_in">float</span>3 rayOrigin, <span class="hljs-built_in">float</span>3 rayDirection, <span class="hljs-built_in">float</span> numSteps, 
 <span class="hljs-built_in">float</span> stepSize, <span class="hljs-built_in">float</span> densityScale, UnityTexture3D volumeTex,
 UnitySamplerState volumeSampler, <span class="hljs-built_in">float</span>3 offset, 
 <span class="hljs-built_in">float</span> numLightSteps, <span class="hljs-built_in">float</span> lightStepSize, <span class="hljs-built_in">float</span>3 lightDir,
 <span class="hljs-built_in">float</span> lightAbsorb, <span class="hljs-built_in">float</span> darknessThreshold, <span class="hljs-built_in">float</span> transmittance,
 out <span class="hljs-built_in">float</span>3 result )
{
     <span class="hljs-built_in">float</span> density = 0;
     <span class="hljs-built_in">float</span> transmission = 0;
     <span class="hljs-built_in">float</span> lightAccumulation = 0;
     <span class="hljs-built_in">float</span> finalLight = 0;

     <span class="hljs-keyword">for</span>(int i =0; i&lt; numSteps; i++){
 rayOrigin += (rayDirection*stepSize);

 <span class="hljs-built_in">float</span>3 samplePos = rayOrigin+offset;
 <span class="hljs-built_in">float</span> sampledDensity = 
 SAMPLE_TEXTURE3D(volumeTex, volumeSampler, samplePos).r;
 density += sampledDensity*densityScale;
 //light loop
 <span class="hljs-built_in">float</span>3 lightRayOrigin = samplePos;

 <span class="hljs-keyword">for</span>(int j = 0; j &lt; numLightSteps; j++){
 lightRayOrigin += -lightDir*lightStepSize;
 <span class="hljs-built_in">float</span> lightDensity = 
 SAMPLE_TEXTURE3D(volumeTex, volumeSampler, lightRayOrigin).r;
 lightAccumulation += lightDensity;
 }
</code></pre><p><img src="_page_109_Picture_0.jpeg" alt=""></p>
<p>}</p>
<pre><code> float lightTransmission = exp(-lightAccumulation)<span class="hljs-comment">;</span>
 float <span class="hljs-keyword">shadow </span>= darknessThreshold + 
 lightTransmission * (<span class="hljs-number">1</span>.<span class="hljs-number">0</span> - darknessThreshold)<span class="hljs-comment">;</span>
 finalLight += density*transmittance*<span class="hljs-keyword">shadow;
</span> transmittance *= exp(-density*lightAbsorb)<span class="hljs-comment">;</span>
     }
 transmission = exp(-density)<span class="hljs-comment">;</span>
     result = float3(finalLight, transmission, transmittance)<span class="hljs-comment">;</span>
</code></pre><p>The light loop is easy to understand, repeating the number of times that have been specified for the <strong>numLightSteps</strong> variable. Bear in mind that this is a nested loop, so keep the numLightSteps count as low as possible. You move from samplePos towards the main light by using minus lightDir. Then, lightDensity is added to lightAccumulation. Some math is required outside the light loop:</p>
<p>float lightTransmission = exp(-lightAccumulation);</p>
<p>First, lightTransmission is set as e-lightAccumulation . The <a href="https://en.wikipedia.org/wiki/E_(mathematical_constant">constant e</a>), Euler&#39;s number, is about 2.718. The graph below shows the result of this function. The horizontal axis is the value of lightAccumulation and the vertical axis exp(-lightAccumulation). When the accumulated light density, lightAccumulation, is 0, exp(-lightAccumulation) equals 1. As lightAccumulation increases exp(-lightAccumulation) quickly drops away, nearing 0 if lightAccumulation is 5 or more.</p>
<p><img src="_page_109_Figure_6.jpeg" alt=""></p>
<p>Graph of e-x for the range 0 to 4</p>
<p><img src="_page_110_Picture_0.jpeg" alt=""></p>
<p>float shadow = darknessThreshold + lightTransmission * (1.0 - darknessThreshold);</p>
<p>A shadow value is calculated next. Use the property called <strong>darknessThreshold</strong>. The graph below shows the shadow value in the vertical axis, for a darknessThreshold of 0.15. If lightAccumulation is 0 then shadow equals 1, whereas if lightAccumulation approaches 5, then shadow tends to the darknessThreshold constant value.</p>
<p><img src="_page_110_Figure_3.jpeg" alt=""></p>
<p>finalLight += density*transmittance*shadow;</p>
<p>Density * transmittance * shadow is added to the finalLight accumulated value. If the accumulated light density, lightAccumulation, is high, then shadow will tend to 0 and therefore, the accumulated value for finalLight will be less.</p>
<p>transmittance *= exp(-density*lightAbsorb);</p>
<p>The initial value of transmittance is a passed-in property, but for each view direction step, its value is multiplied by e-density*lightAbsorb. The property <strong>lightAbsorb</strong> controls how much light gets lost in the cloud by scattering.</p>
<p>For version 3, the result is a float3 containing the finalLight, transmission, and transmittance.</p>
<p>The graph for version 3 is shown below. Now that the output from the Custom Function is a float3, a Split node is added. Output R goes to a Lerp node T input. Version 3 has several new properties, including <strong>color</strong> and <strong>shadowColor</strong>, with the former the B input and the latter the A input.</p>
<p>If finalLight raymarch node Out.x is 0, then the shadowColor will be passed to the Lerp node output. If finalLight is 1, then color is passed to the output. In the range 0-1 a linear interpolation of shadowColor and color is the output. The Lerp node output goes directly to Fragment &gt; Base Color.</p>
<p>Alpha uses the raymarch node with transmission value Out.y. This value is 0 when Alpha should be 1 and 1 when it should be 0. A <a href="https://docs.unity3d.com/Packages/com.unity.shadergraph@17.0/manual/One-Minus-Node.html">One Minus</a> node is used to correct the Split node B value and link this to Fragment &gt; Alpha.</p>
<p><img src="_page_111_Figure_3.jpeg" alt=""></p>
<p>Final version</p>
<p><img src="_page_112_Picture_0.jpeg" alt=""></p>
<h4 id="this-gives-the-result-you-see-below-">This gives the result you see below.</h4>
<p><img src="_page_112_Picture_3.jpeg" alt=""></p>
<p>A cloud with ray marching</p>
<p>Houdini is a useful tool when creating the 3D texture. Alternatives to a 3D texture include using multilayered <a href="https://en.wikipedia.org/wiki/Perlin_noise">Perlin noise</a>, or pre-baking a <a href="https://www.ronja-tutorials.com/post/029-tiling-noise/">tileable noise</a> texture using Unity. Hopefully, this recipe will be a starting point for your journey into ray marching.</p>
<h4 id="-more-resources-"><strong>More resources</strong></h4>
<ul>
<li><a href="https://www.youtube.com/watch?v=0G8CVQZhMXw">Volumetric ray marching cloud shader</a> by dmeville</li>
<li><a href="https://www.youtube.com/watch?v=4QOcCGI6xOU&amp;t=2s">Coding adventure: Clouds</a> by Sebastian Lague</li>
<li><a href="https://80.lv/articles/building-volumetric-clouds-with-houdini/">Creating Volumetric Clouds with Houdini</a> by Camelia Slimani</li>
<li><a href="https://assetstore.unity.com/packages/vfx/shaders/fullscreen-camera-effects/os-altos-volumetric-clouds-procedural-skybox-and-day-night-cycle-221227?utm_source=demand-gen&amp;utm_medium=pdf&amp;utm_campaign=render-with-quality-and-flexibility&amp;utm_content=urp-cookbook">Altos sky system</a>, by OccaSoftware</li>
<li><a href="https://github.com/adrianpolimeni/RealTimeVolumetricClouds">Real Time Volumetric Clouds</a> by Adrian Polimeni</li>
</ul>
<h2 id="-span-id-page-113-0-span-procedural-noise"><span id="page-113-0"></span>Procedural noise</h2>
<p><img src="_page_113_Figure_1.jpeg" alt=""></p>
<p>The game <em>Please, Touch the Artwork</em> <a href="https://unity.com/case-study/please-touch-the-artwork">by Thomas Waterzool,</a> uses procedural generation to help speed up the level creation process, a good case for randomizing game design elements.</p>
<p><span id="page-114-0"></span>Procedural noise is a powerful technique used for generating textures, terrains, and other environmental elements in a way that appears random but can be finely controlled. By using algorithms to generate these effects, developers can create vast, varied worlds without the need to handcraft every detail.</p>
<p>Unity supports several types of procedural noise, including Gradient/<a href="https://en.wikipedia.org/wiki/Perlin_noise">Perlin</a> noise, <a href="https://en.wikipedia.org/wiki/Simplex_noise">Simplex</a> noise, Voronoi/<a href="https://en.wikipedia.org/wiki/Worley_noise">Worley,</a> each of which has unique applications and benefits. Understanding and implementing procedural noise can enhance both the aesthetic and performance aspects of a game or interactive experience.</p>
<p><img src="_page_114_Picture_3.jpeg" alt=""></p>
<p>Procedural Noise examples left to right: Gradient, Simplex, and Voronoi</p>
<h2 id="types-of-procedural-noise">Types of procedural noise</h2>
<p>The most common types of procedural noise in Unity are Perlin noise and Simplex noise, both developed by Ken Perlin.</p>
<p>Perlin noise is the go-to algorithm for creating smooth, natural-looking variations in textures, terrains, and animations. It generates smooth, continuous patterns that avoid harsh edges, making it ideal for things like mountainous terrain, cloud textures, or water waves. Simplex noise is a more efficient alternative, especially for three-dimensional applications. It provides a smoother result and is computationally less intensive than Perlin noise, which is helpful for mobile games or VR environments where performance is critical.</p>
<h2 id="implementing-procedural-noise-in-unity">Implementing procedural noise in Unity</h2>
<p>In Unity, procedural noise can be implemented through shaders or directly in C# scripts. For 2D textures, the Mathf.PerlinNoise() function is particularly accessible and useful. It generates noise values based on X and Y coordinates, which makes it easy to apply to terrains, textures, or particle effects. By adjusting parameters like frequency, amplitude, and scale, developers can control the noise output to achieve different visual effects.</p>
<p><span id="page-115-0"></span><img src="_page_115_Picture_0.jpeg" alt=""></p>
<p>For example, a developer might want to use Perlin noise to create a heightmap for a terrain. This can be done by generating a grid of Perlin noise values and mapping those values to height levels. As each noise value corresponds to a point on the terrain, this can result in hills, valleys, and other natural formations without the need to manually sculpt them. Unity&#39;s terrain tools can then apply these heightmaps to create a 3D terrain, which can be further refined by adding textures or using other procedural generation techniques.</p>
<h4 id="-procedural-heightmap-example-"><strong>Procedural heightmap example</strong></h4>
<p>Here&#39;s a basic example of how to generate a heightmap in Unity using Perlin noise in a C# script. This script will create a terrain with hills and valleys by applying Perlin noise to a Terrain GameObject.</p>
<ul>
<li><ol>
<li>Create a Terrain GameObject in Unity.</li>
</ol>
</li>
<li><ol>
<li>Attach the script below to Terrain.</li>
</ol>
</li>
<li><ol>
<li>Adjust the parameters as desired to control the look of the terrain.</li>
</ol>
</li>
</ul>
<p><img src="_page_115_Picture_8.jpeg" alt=""></p>
<pre><code>Procedural <span class="hljs-built_in">height</span> <span class="hljs-built_in">example</span>
</code></pre><p>using Unity.VisualScripting; using UnityEngine; [ExecuteInEditMode] public class ProceduralHeight : MonoBehaviour { public int depth = 20; // The max height of the terrain public float scale = 20f; // Controls how &quot;stretched&quot; the noise appears</p>
<pre><code> <span class="hljs-keyword">private</span> <span class="hljs-built_in">int</span> <span class="hljs-built_in">width</span> = <span class="hljs-number">256</span>; <span class="hljs-comment">// Width of the terrain</span>
 <span class="hljs-keyword">private</span> <span class="hljs-built_in">int</span> <span class="hljs-built_in">height</span> = <span class="hljs-number">256</span>; <span class="hljs-comment">// Height of the terrain</span>
 <span class="hljs-keyword">private</span> <span class="hljs-built_in">float</span> offsetX = <span class="hljs-number">100</span>f; <span class="hljs-comment">// Offset for X coordinate (randomized at start)</span>
 <span class="hljs-keyword">private</span> <span class="hljs-built_in">float</span> offsetY = <span class="hljs-number">100</span>f; <span class="hljs-comment">// Offset for Y coordinate (randomized at start)</span>
 <span class="hljs-keyword">private</span> <span class="hljs-built_in">int</span> _depth;
 <span class="hljs-keyword">private</span> <span class="hljs-built_in">float</span> _scale;
 <span class="hljs-keyword">private</span> Terrain terrain;
 <span class="hljs-keyword">void</span> Start()
 {
 <span class="hljs-comment">// Randomize offsets for a unique terrain each time</span>
 offsetX = Random.Range(<span class="hljs-number">0</span>f, <span class="hljs-number">9999</span>f);
 offsetY = Random.Range(<span class="hljs-number">0</span>f, <span class="hljs-number">9999</span>f);
 _depth = depth;
 _scale = <span class="hljs-built_in">scale</span>;
 terrain = GetComponent&lt;Terrain&gt;();
 terrain.terrainData = GenerateTerrain(terrain.terrainData);
 }
 TerrainData GenerateTerrain(TerrainData terrainData)
 {
 terrainData.heightmapResolution = <span class="hljs-built_in">width</span> + <span class="hljs-number">1</span>;
 terrainData.<span class="hljs-built_in">size</span> = <span class="hljs-keyword">new</span> Vector3(<span class="hljs-built_in">width</span>, depth, <span class="hljs-built_in">height</span>);
 terrainData.SetHeights(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, GenerateHeights());
 <span class="hljs-keyword">return</span> terrainData;
 }
 <span class="hljs-built_in">float</span>[,] GenerateHeights()
 {
 <span class="hljs-built_in">float</span>[,] heights = <span class="hljs-keyword">new</span> <span class="hljs-built_in">float</span>[<span class="hljs-built_in">width</span>, <span class="hljs-built_in">height</span>];
 <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> x = <span class="hljs-number">0</span>; x &lt; <span class="hljs-built_in">width</span>; x++)
 {
 <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> y = <span class="hljs-number">0</span>; y &lt; <span class="hljs-built_in">height</span>; y++)
 {
 <span class="hljs-built_in">float</span> xCoord = (<span class="hljs-built_in">float</span>)x / <span class="hljs-built_in">width</span> * <span class="hljs-built_in">scale</span> + offsetX;
 <span class="hljs-built_in">float</span> yCoord = (<span class="hljs-built_in">float</span>)y / <span class="hljs-built_in">height</span> * <span class="hljs-built_in">scale</span> + offsetY;
</code></pre><pre><code> heights[x, y] = Mathf.PerlinNoise(xCoord, yCoord)<span class="hljs-comment">;</span>
 }
 }
 return heights<span class="hljs-comment">;</span>
 }
 void Update(){
 if ( depth != _depth <span class="hljs-title">||</span> <span class="hljs-keyword">scale </span>!= _scale ){
 terrain.terrainData = GenerateTerrain(terrain.terrainData)<span class="hljs-comment">;</span>
 _depth = depth<span class="hljs-comment">;</span>
 _scale = <span class="hljs-keyword">scale;
</span> }
 }
}
</code></pre><p>Let&#39;s look at some key values/properties in this script:</p>
<ul>
<li><strong>depth</strong>: This is the vertical scale of the terrain; a higher value will create taller hills.</li>
<li><strong>width and height</strong>: These are the dimensions of the terrain.</li>
<li><strong>scale</strong>: This controls how &quot;zoomed in&quot; the noise is; a higher value creates smoother, broader hills.</li>
<li><strong>offsetX and offsetY</strong>: Random offsets are added to vary the terrain layout each time the script runs.</li>
</ul>
<p>The GenerateHeights() function creates a 2D array representing the height values of each point on the terrain. Each value is generated using Mathf.PerlinNoise, which gives a smooth, natural variation based on xCoord and yCoord (scaled by width, height, and scale).</p>
<p>In the resources you will find this example at <strong>Scenes &gt; Procedural Noise &gt; Terrain</strong>. This simple heightmap generation technique can be modified to add more complexity, like using multiple layers of Perlin noise for fractal-like terrain or applying additional noise functions.</p>
<p><span id="page-118-0"></span><img src="_page_118_Picture_1.jpeg" alt=""></p>
<p>Examples of using procedural noise for a wood texture</p>
<p>Another use for procedural noise is to create shaders that mirror natural textures, like the wood textures pictured above. Take a look at <strong>Scenes &gt; Procedural Noise &gt; Wood</strong> to find this example. Here you&#39;ll see two different wood shaders, one highly stylised and the other more realistic. Let&#39;s start by creating the stylised version:</p>
<ul>
<li><ol>
<li>Create a Shader Graph via <strong>Shader Graph &gt; URP &gt; Lit Shader Graph</strong></li>
</ol>
</li>
<li><ol>
<li>Add these properties:<ul>
<li>a. ColorA (Color default: 7D490B)</li>
<li>b. ColorB (Color default: BB905D)</li>
<li>c. Frequency (Float default: 2)</li>
<li>d. NoiseScale (Float default: 6)</li>
<li>e. PatternIntensity (Float default: 0.6)</li>
<li>f. Contrast (Float default: 4)</li>
</ul>
</li>
</ol>
</li>
<li><ol>
<li>Create a <strong>Geometry &gt; UV</strong> node, Channel UV0.</li>
</ol>
</li>
<li><ol>
<li>Create a <strong>Procedural &gt; Noise &gt; GradientNoise</strong> node. Link the output from step 3 to the <strong>UV</strong> input and set the <strong>Scale</strong> input to <strong>5</strong>. Leave the <strong>Hash Type</strong> as <strong>Deterministic</strong>.</li>
</ol>
</li>
<li><ol>
<li>Create a <strong>Multiply</strong> node and set input <strong>A</strong> as the output from step 4 and input <strong>B</strong> as the property NoiseScale.</li>
</ol>
</li>
<li><ol>
<li>Create an <strong>Add</strong> node and link the output from step 5 to input <strong>B</strong>.</li>
</ol>
</li>
<li><ol>
<li>Return to the <strong>UV</strong> node of step 3. Above the <strong>Gradient Noise</strong> node create a <strong>Split</strong> node and link the output from the <strong>UV</strong> node to its input.</li>
</ol>
</li>
</ul>
<p><img src="_page_119_Picture_0.jpeg" alt=""></p>
<ul>
<li><ol>
<li>Create a <strong>Multiply</strong> node and set the Split node <strong>G</strong> output as input <strong>A</strong> and the property Frequency as input <strong>B.</strong></li>
</ol>
</li>
<li><ol>
<li>Link the output from step 8 to input <strong>A</strong> of step 6.</li>
</ol>
</li>
<li><ol>
<li>Create a <strong>Fraction</strong> node and link the output from the <strong>Add</strong> node to its input.</li>
</ol>
</li>
<li><ol>
<li>Create a <strong>One Minus</strong> node and link its input to the output from step 10.</li>
</ol>
</li>
<li><ol>
<li>Create a <strong>Multiply</strong> node and link the output from step 11 to input <strong>A</strong> and the property Contrast to input <strong>B</strong>.</li>
</ol>
</li>
<li><ol>
<li>Create another <strong>Multiply</strong> node and link input <strong>A</strong> to the output from step 10 and input <strong>B</strong> to the output from step 12.</li>
</ol>
</li>
<li><ol>
<li>Create a <strong>Power</strong> node and link input <strong>A</strong> to the output from step 13 and input <strong>B</strong> to the property PatternIntensity.</li>
</ol>
</li>
<li><ol>
<li>Create an <strong>Add</strong> node and link input <strong>A</strong> to the <strong>R</strong> output of the <strong>Split</strong> node from step 7 and the <strong>B</strong> input link to the output from step 14.</li>
</ol>
</li>
<li><ol>
<li>Create a <strong>Lerp</strong> node and set input <strong>A</strong> to the property ColorA, input <strong>B</strong> to the property ColorB and input <strong>T</strong> to the output from step 15.</li>
</ol>
</li>
</ul>
<p><img src="_page_119_Figure_11.jpeg" alt=""></p>
<ol>
<li>Link the output from step 16 to the <strong>Fragment Base Color</strong>.</li>
</ol>
<p>The stylised wood Shader Graph</p>
<p>The UV <strong>u value</strong> is used to set the grain position that is passed directly to the final Add node. The <strong>v value</strong> goes through a complex chain of manipulation. Starting with Gradient Noise this is manipulated via Multiply nodes to adjust the frequency of the grain lines (pattern) and scale. The key to the shader is using the fractional part of the calculated value via the Fraction node. You can also use properties to adjust the contrast and intensity of the grain or pattern.</p>
<p><img src="_page_120_Figure_1.jpeg" alt=""></p>
<p>Stages of the shader: Gradient Noise (left); the effect of using the fractional part giving multiple grain lines (second to left); boosting the contrast (second to right), and adding color (right)</p>
<p>Lets look at the more realistic version that&#39;s achieved using a Custom Function node. This is an adapted version of <a href="https://www.shadertoy.com/view/mdy3R1">this example from ShaderToy</a>. ShaderToy is a great source of shader code, however all code uses the GLSL syntax, not HLSL, and will need to be adapted for use in a Shader Graph Custom Function. If you&#39;re knowledgeable in HLSL coding then the conversion should be somewhat straight-forward: vec -&gt; float, fract -&gt; frac, mix -&gt; lerp.</p>
<table>
<thead>
<tr>
<th>Shadertoy</th>
<th>Search</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Browse</th>
<th>New Sign In</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Shader Inputs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>2<br>ന<br>ব<br>ഗ<br>ଚ<br>7<br>00<br>ರಿ<br>10<br>11<br>12<br>13<br>14</td>
<td>1 // &#39;Procedural Wood texture&#39; dean_the_coder (Twitter: @deanthecoder)<br>// <a href="https://www.shadertoy.com/view/mdy3R1">https://www.shadertoy.com/view/mdy3R1</a><br>/ / Processed by &#39;GLSL Shader Shrinker<br>// (<a href="https://github.com/deanthecoder/GLSLShaderShrinker">https://github.com/deanthecoder/GLSLShaderShrinker</a>)<br>11<br>/ / I spent some time working on some noise functions and then<br>// used them to make a wood texture. I&#39;d never used musgrave<br>/ / noise before - Looks very useful!<br>11<br>/ / Thanks to Evvvvil, Flopine, Nusan, BigWings, Iq, Shane,<br>totetmatt, Blackle, Dave Hoskins, byt3_m3chanic, tater,<br>// and a bunch of others for sharing their time and knowledge!<br>11</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>15<br>16</td>
<td></td>
<td></td>
<td>// License: Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Views: 1716, Tags:<br>procedural, texture, wood, material, nature<br>never used musgrave noise before - Looks very cool.<br>Comments (11)<br>Sign in to post a comment<br>TimelordQ, 2023-08-07<br>Amazing work. Thank you!</td>
<td>40.21 60.1 fps 840 x 473<br>Procedural Wood texture<br>I spent some time working on some noise functions and them to make a wood texture. I&#39;d<br>I&#39;ve tried to keep the code size down - Let me know if you find this useful!</td>
<td>==============================================================================================================================================================================<br>REC<br>ba<br>Created by dean_the_coder in 2023-<br>03-07</td>
<td>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43</td>
<td>#define R   iResolution<br>▼ float h31(vec3 p3) {<br>▼ float n31(vec3 p) {<br>vec3 ip = floor(p) ;<br>p = fract (p) ;</td>
<td>#define sat(x) clamp(x, 0., 1.)<br>#define S(a, b, c) smoothstep(a, b, c)<br>#define S01(a) S(0., 1., a)<br>float sum2(vec2 v) { return dot(v, vec2(1)); }<br>p3 = fract (p3 <em> . 1031) ;<br>p3 += dot(p3, p3.yzx + 333.3456);<br>return fract (sum2 (p3.xy) </em> p3.z) ;<br>float h21(vec2 p) { return h31(p.xyx); }<br>const vec3 s = vec3 (7, 157, 113) ;<br>p = p <em> p </em> (3. - 2. * p);<br>vec4 h = vec4(0, s.yz, sum2(s.yz)) + dot(ip, s);<br>h.xy = mix(h.xz, h.yw, p.y);</td>
<td>// Thanks Shane - <a href="https://www.shadertoy.com/view/lstGRB">https://www.shadertoy.com/view/lstGRB</a><br>h = mix(fract(sin(h) <em> 43758.545), fract(sin(h + s.x) </em> 43758.545), p.x);</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Thanks everyone!</td>
<td>dean_the_coder, 2023-03-09<br>@Shane My local version of this returns a vec4 where the w component is a &#39;depth&#39;<br>value. I left it out here for simplicity (&#39;an exercise for the user&#39;), but can easily be<br>added by combining n1, n2, and grain.</td>
<td></td>
<td>44<br>45</td>
<td>Compiled in 0.1 secs</td>
<td>return mix(h.x, h.y, p.z);</td>
<td>1879 chars</td>
<td></td>
<td>S V</td>
<td></td>
</tr>
<tr>
<td>Shane, 2023-03-09</td>
<td>Nicely done. I have a few simple go-to timber texture routines that I use (using<br>noise and smooth fract), but they&#39;re not as detailed as this. One thing that I&#39;ve<br>noticed is that a lot of textures already come prelit (probably using directional<br>derivative lighting) which can make them pop a bit more. I&#39;ve been too lazy to<br>generate my own textures lately, but when I do, I usually find that I have to provide<br>a less detailed version for bump mapping and distance functions.</td>
<td></td>
<td>iChannel0</td>
<td></td>
<td>iChannel1</td>
<td>iChannel2</td>
<td>iChannel3</td>
<td></td>
</tr>
</tbody>
</table>
<p>An <a href="https://www.shadertoy.com/view/mdy3R1">example</a> of a shader from the ShaderToy website</p>
<p>The Custom Function, seen in the code below, requires just setting the <strong>Geometry &gt; Position</strong> node to <strong>Object</strong> if you are happy to take the default colors. Or add two <strong>Color</strong> properties to define the colors; ColorA provides the grain lines color, ColorB the base color.</p>
<table>
<thead>
<tr>
<th>WoodBSG</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Vertex</th>
<th></th>
<th>Graph Inspector</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Shader Graphs</td>
<td>+</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Object Space · · O Position(3)</td>
<td></td>
<td></td>
<td>Node Settings Graph Settings</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>· ColorA</td>
<td>Color</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Object Space · O Normal(3)</td>
<td></td>
<td></td>
<td>wood (Custom Function) Node</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>· ColorB</td>
<td>Color</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Object Space · · O Tangent(3)</td>
<td></td>
<td>Precision</td>
<td></td>
<td>Inherit</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Preview<br>Inputs</td>
<td></td>
<td>Inherit</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>position</td>
<td></td>
<td>Vector 3</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Position</td>
<td></td>
<td></td>
<td>wood (Custom Function)</td>
<td></td>
<td></td>
<td>Fragment</td>
<td></td>
<td>COIA<br>colB</td>
<td></td>
<td>Vector 4<br>Vector 4</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Out(3) O</td>
<td></td>
<td>9 position(3)</td>
<td>result(4) @</td>
<td></td>
<td>--- @ Base Color(3)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>+<br>-</td>
<td></td>
</tr>
<tr>
<td>Space</td>
<td>Object</td>
<td>· ColorA(4) ·</td>
<td>O colA(4)<br>O colB(4)</td>
<td></td>
<td></td>
<td>Tangent Space · O Normal (Tangent Space)(3)</td>
<td></td>
<td>Outputs</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>&gt;</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>result</td>
<td></td>
<td>Vector 4</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>· ColorB(4) ●</td>
<td></td>
<td></td>
<td>× 0</td>
<td>· · · · Metallic(1)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>+</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>X 0.5 · · O Smoothness(1)</td>
<td></td>
<td>Type</td>
<td>File</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Name<br>Source</td>
<td>wood<br>wood</td>
<td></td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>HDR · O Emission(3)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>X 1 1 · · O Ambient Occlusion(1)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Custom Function wood Shader Graph, using user defined colors</p>
<table>
<thead>
<tr>
<th>#define sat(x)<br>clamp(x, 0.0, 1.0)<br>#define S(a, b, c)<br>smoothstep(a, b, c)<br>#define S01(a)<br>S(0.0, 1.0, a)</th>
</tr>
</thead>
<tbody>
<tr>
<td>float sum2(float2 v) { return dot(v, float2(1.0, 1.0)); }</td>
</tr>
<tr>
<td>///////////////////////////////////////////////////////////////////////////////</td>
</tr>
<tr>
<td>float h31(float3 p3) {<br>p3 = frac(p3 <em> 0.1031);<br>p3 += dot(p3, p3.yzx + 333.3456);<br>return frac(sum2(p3.xy) </em> p3.z);</td>
</tr>
<tr>
<td>}</td>
</tr>
<tr>
<td>float h21(float2 p) { return h31(p.xyx); }</td>
</tr>
<tr>
<td>float n31(float3 p) {<br>const float3 s = float3(7, 157, 113);</td>
</tr>
</tbody>
</table>
<pre><code><span class="hljs-comment">// Thanks Shane - https://www.shadertoy.com/view/lstGRB</span>
      float3 ip = floor(p);
      p = frac(p);
      p = p * p * (<span class="hljs-number">3.</span> - <span class="hljs-number">2.</span> * p);
      float4 h = float4(<span class="hljs-number">0</span>, s.yz, sum2(s.yz)) + dot(ip, s);
      h = lerp(frac(sin(h) * <span class="hljs-number">43758.545</span>), frac(sin(h + s.x) * <span class="hljs-number">43758.545</span>), p.x);
      h.xy = lerp(h.xz, h.yw, p.y);
      return lerp(h.x, h.y, p.z);
}
<span class="hljs-comment">// roughness: (0.0, 1.0], default: 0.5</span>
<span class="hljs-comment">// Returns unsigned noise [0.0, 1.0]</span>
<span class="hljs-type">float</span> fbm(float3 p, int octaves, <span class="hljs-type">float</span> roughness) {
      <span class="hljs-type">float</span> sum = <span class="hljs-number">0.</span>,
       amp = <span class="hljs-number">1.</span>,
       tot = <span class="hljs-number">0.</span>;
      roughness = sat(roughness);
      for (int i = <span class="hljs-number">0</span>; i &lt; octaves; i++) {
 sum += amp * n31(p);
 tot += amp;
 amp *= roughness;
 p *= <span class="hljs-number">2.</span>;
      }
      return sum / tot;
}
float3 randomPos(<span class="hljs-type">float</span> seed) {
      float4 s = float4(seed, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>);
      return float3(h21(s.xy), h21(s.xz), h21(s.xw)) * <span class="hljs-number">1e2</span> + <span class="hljs-number">1e2</span>;
}
<span class="hljs-comment">// Returns unsigned noise [0.0, 1.0]</span>
<span class="hljs-type">float</span> fbmDistorted(float3 p) {
      p += (float3(n31(p + randomPos(<span class="hljs-number">0.</span>)), n31(p + randomPos(<span class="hljs-number">1.</span>)), n31(p + randomPos(<span class="hljs-number">2.</span>))) * 
<span class="hljs-number">2.</span> - <span class="hljs-number">1.</span>) * <span class="hljs-number">1.12</span>;
      return fbm(p, <span class="hljs-number">8</span>, <span class="hljs-number">.5</span>);
}
<span class="hljs-comment">// float3: detail(/octaves), dimension(/inverse contrast), lacunarity</span>
<span class="hljs-comment">// Returns signed noise.</span>
<span class="hljs-type">float</span> musgraveFbm(float3 p, <span class="hljs-type">float</span> octaves, <span class="hljs-type">float</span> dimension, <span class="hljs-type">float</span> lacunarity) 
{
</code></pre><p><img src="_page_123_Picture_0.jpeg" alt=""></p>
<pre><code><span class="hljs-type">float</span> sum = <span class="hljs-number">0.</span>,
       amp = <span class="hljs-number">1.</span>,
       m = pow(lacunarity, -dimension);
      for (<span class="hljs-type">float</span> i = <span class="hljs-number">0.</span>; i &lt; octaves; i++) {
 <span class="hljs-type">float</span> n = n31(p) * <span class="hljs-number">2.</span> - <span class="hljs-number">1.</span>;
 sum += n * amp;
 amp *= m;
 p *= lacunarity;
      }
      return sum;
}
<span class="hljs-comment">// Wave noise along X axis.</span>
float3 waveFbmX(float3 p) {
      <span class="hljs-type">float</span> n = p.x * <span class="hljs-number">20.</span>;
      n += <span class="hljs-number">.4</span> * fbm(p * <span class="hljs-number">3.</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3.</span>);
      return float3(sin(n) * <span class="hljs-number">.5</span> + <span class="hljs-number">.5</span>, p.yz);
}
<span class="hljs-comment">///////////////////////////////////////////////////////////////////////////////</span>
<span class="hljs-comment">// Math</span>
<span class="hljs-type">float</span> remap01(<span class="hljs-type">float</span> f, <span class="hljs-type">float</span> in1, <span class="hljs-type">float</span> in2) { return sat((f - in1) / (in2 - in1)); }
<span class="hljs-comment">///////////////////////////////////////////////////////////////////////////////</span>
<span class="hljs-comment">// Wood material.</span>
float3 matWood(float3 p, float3 colA, float3 colB ) {
      <span class="hljs-type">float</span> n1 = fbmDistorted(p * float3(<span class="hljs-number">7.8</span>, <span class="hljs-number">1.17</span>, <span class="hljs-number">1.17</span>));
      n1 = lerp(n1, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.2</span>);
      <span class="hljs-type">float</span> n2 = lerp(musgraveFbm(float3(n1, n1, n1) * <span class="hljs-number">4.6</span>, <span class="hljs-number">8.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">2.5</span>), n1, <span class="hljs-number">0.85</span>);
      <span class="hljs-type">float</span> dirt = <span class="hljs-number">1.</span> - musgraveFbm(waveFbmX(p * float3(<span class="hljs-number">.01</span>, <span class="hljs-number">.15</span>, <span class="hljs-number">.15</span>)), <span class="hljs-number">15.</span>, <span class="hljs-number">.26</span>, <span class="hljs-number">2.4</span>) * <span class="hljs-number">.4</span>;
      <span class="hljs-type">float</span> grain = <span class="hljs-number">1.</span> - S(<span class="hljs-number">.2</span>, <span class="hljs-number">1.</span>, musgraveFbm(p * float3(<span class="hljs-number">500</span>, <span class="hljs-number">6</span>, <span class="hljs-number">1</span>), <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.5</span>)) * <span class="hljs-number">.2</span>;
      n2 *= dirt * grain;

 <span class="hljs-comment">// The three float3 values are the RGB wood colors - Tweak to suit.</span>
      return lerp(lerp(colA, colB, remap01(n2, <span class="hljs-number">.19</span>, <span class="hljs-number">.56</span>)), float3(<span class="hljs-number">.52</span>, <span class="hljs-number">.32</span>, <span class="hljs-number">.19</span>), remap01(n2, 
<span class="hljs-number">.56</span>, <span class="hljs-number">1.</span>));
}
</code></pre><p><img src="_page_124_Picture_0.jpeg" alt=""></p>
<pre><code><span class="hljs-comment">//Hard coded colors</span>
void wood_float( float3 pos, out float4 result){
 float3 colA = float3(<span class="hljs-number">.03</span>, <span class="hljs-number">.012</span>, <span class="hljs-number">.003</span>);
 float3 colB = float3(<span class="hljs-number">.25</span>, <span class="hljs-number">.11</span>, <span class="hljs-number">.04</span>);
 result = float4(pow(matWood(pos, colA, colB), float3(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>) * <span class="hljs-number">0.4545</span>), <span class="hljs-number">0</span>);
}
<span class="hljs-comment">//User selectable colors, colA provides the grain lines</span>
</code></pre><pre><code>void wood_float( <span class="hljs-built_in">float</span>3 pos, <span class="hljs-built_in">float</span>3 colA, <span class="hljs-built_in">float</span>3 colB, out <span class="hljs-built_in">float</span>4 result){
 result = <span class="hljs-built_in">float</span>4(pow(matWood(pos, colA, colB ), <span class="hljs-built_in">float</span>3(1,1,1) * 0.4545), 0);
</code></pre><p>}</p>
<p>Starting with the wood_float function, the code calls the function matWood. This makes use of the functions fbmDistorted, musgraveFbm and waveFbm. There are a lot of carefully chosen &quot;magic&quot; numbers that the developer probably took a lot of time tweaking to get the best results. <strong><a href="https://en.wikipedia.org/wiki/Fractional_Brownian_motion">fBm</a></strong> (Fractional Brownian motion) or multifractal noise, is a type of procedural noise developed by <a href="https://en.wikipedia.org/wiki/Ken_Musgrave">Ken Musgrave</a>, and consequently often called Musgrave noise. It&#39;s used for generating realistic textures and terrains. Unlike traditional noise functions such as Perlin noise, Musgrave noise simulates complex, natural phenomena like clouds, mountains, and flowing water by combining multiple layers (or octaves) of noise with varying frequencies and amplitudes.</p>
<p>Let&#39;s look in more detail at the musgraveFbm function, starting with its parameters:</p>
<ul>
<li><strong>P (float3)</strong>: This is a position value, usually in object space.</li>
<li><strong>octaves (float)</strong>: Musgrave noise works by layering several octaves of noise functions. The result is a composite effect that builds detail as more layers are added, making the texture look progressively more complex.</li>
<li><strong>dimension (float)</strong>: This inversely controls the contrast.</li>
<li><strong>lacunarity</strong>: This controls the frequency ratio between octaves. Higher lacunarity values make each octave smaller and more detailed, resulting in more intricate patterns.</li>
</ul>
<p>The function initializes three float values:</p>
<ul>
<li><ol>
<li>The returned sum value</li>
</ol>
</li>
<li><ol>
<li>An amplitude value, amp</li>
</ol>
</li>
<li><ol>
<li>A multiplier, m</li>
</ol>
</li>
</ul>
<p>The multiplier is the lacunarity raised to the power negative dimension. Then it enters a loop for each octave. The function n31 returns a Perlin noise value from a float3 input. Since this is in the range 0 to 1 you remap it to the range -1 to 1, then add this to the cumulative</p>
<p><span id="page-125-0"></span>sum value after multiplying it by the current amplitude value, amp. Then, adjust the amp value by multiplying by the multiplier constant value m. Finally, change the sample position p by multiplying by the lacunarity parameter.</p>
<p>Musgrave noise is highly valued in computer graphics for its ability to produce realistic, natural textures that require minimal manual input. By layering and varying different noise functions, it creates intricate, fractal-like patterns that resemble the complexity of real-world landscapes, clouds, and organic materials such as the wood example described.</p>
<p><img src="_page_125_Picture_3.jpeg" alt=""></p>
<p>Raymarch clouds using Musgrave noise</p>
<h3 id="benefits-of-procedural-noise">Benefits of procedural noise</h3>
<p>In shaders, procedural noise can also be used to create animated textures or dynamic materials. For instance, adding noise to a water shader can create realistic ripple effects. By animating the noise over time, the water surface appears to move, giving a sense of fluidity and realism. You can combine procedural noise with color gradients and transparency effects to create varied and dynamic materials, such as clouds or swaying grass.</p>
<p>Using procedural noise offers several benefits, mainly for scalability and variety. Manually creating large, diverse environments can be time-consuming and require substantial memory. Procedural noise allows developers to create complex environments that are unique yet consistent, without using much storage space. Additionally, because procedural noise algorithms are deterministic, the same &quot;random&quot; environment can be recreated if needed by using the same seed values.</p>
<p>This technique also allows for infinite environments, which is useful in games that need continuous terrain, such as open-world or survival games. By generating the environment procedurally, Unity can load and unload sections of terrain as needed, reducing memory usage and improving performance.</p>
<p><span id="page-126-0"></span><img src="_page_126_Picture_0.jpeg" alt=""></p>
<h2 id="challenges-and-optimization">Challenges and optimization</h2>
<p>Procedural noise can be computationally intensive, especially when used in 3D applications or for large environments. To optimize performance, you can limit the resolution of noise generation or employ caching strategies to store previously calculated noise values. Additionally, blending different noise values at different scales can add complexity and also improve realism by adding multiple layers of detail.</p>
<p>Overall, procedural noise in Unity is a versatile tool that can help you create diverse, scalable, and engaging worlds that adapt dynamically to player interactions.</p>
<h2 id="-span-id-page-127-0-span-compute-shaders"><span id="page-127-0"></span>Compute shaders</h2>
<p><img src="_page_127_Picture_1.jpeg" alt=""></p>
<p>If you want your game to have complex simulations <a href="https://www.paradoxinteractive.com/games/cities-skylines-ii/about">like Cities</a>: Skylines II by Colossal Order and Paradox Interactive, you can consider offloading part of this work to the GPU thanks to compute shaders.</p>
<p><span id="page-128-0"></span>This recipe uses compute shaders. A compute shader can be used for any computationally intensive task that involves the same calculations being applied to multiple entities. We&#39;ll look at particle effects and flocking as examples. If you are totally new to compute shaders then check out the resources at the end of this recipe.</p>
<p>Even though Unity provides two systems for creating particle systems, the Built-In Particle System and the VFX Graph, in this recipe you&#39;ll create your own. This will help you understand the techniques necessary to create shaders that work with instanced meshes, allowing you to create visual effects featuring tens of thousands of meshes. Depending on your GPU, a million low-polygon meshes are feasible. These techniques can be used to create grass, hair, water, armies, and crowds.</p>
<h3 id="particlefun">ParticleFun</h3>
<p><img src="_page_128_Picture_4.jpeg" alt=""></p>
<p>The ParticleFun recipe in action</p>
<p>Open <strong>Scenes &gt; Compute Shaders &gt; ParticleFun &gt; ParticleFun</strong> and then in Visual Studio Code, open ParticleFun.cs, ParticleFun.compute and ParticleFun.shader from the same folder. If you run the program you&#39;ll see that particles move towards the mouse position and change color over time. But first let&#39;s review the code, starting with this script:</p>
<pre><code><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ParticleFun</span> : <span class="hljs-title">MonoBehaviour</span>
</span>{
 <span class="hljs-keyword">private</span> <span class="hljs-type">Vector2</span> cursorPos;
 <span class="hljs-comment">// struct</span>
 <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">Particle</span>
 </span>{
 <span class="hljs-keyword">public</span> <span class="hljs-type">Vector3</span> position;
 <span class="hljs-keyword">public</span> <span class="hljs-type">Vector3</span> velocity;
</code></pre><pre><code> <span class="hljs-keyword">public</span> <span class="hljs-keyword">float</span> life;
 }
 <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> SIZE_PARTICLE = <span class="hljs-number">7</span> * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">float</span>);
 <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> particleCount = <span class="hljs-number">1000000</span>;
 <span class="hljs-keyword">public</span> Material material;
 <span class="hljs-keyword">public</span> ComputeShader shader;
 <span class="hljs-keyword">int</span> kernelID;
 ComputeBuffer particleBuffer;
 <span class="hljs-keyword">int</span> groupSizeX;
 RenderParams rp;
</code></pre><p>The particle has a position, velocity, and life values. The data for an individual particle uses 7 floats, so the size of a particle is 7 times the size of a float. Then, declare a number of public variables that the user can adjust in the Inspector. The material will be the Particle material that uses the ParticleFun.shader and the shader will be ParticleFun.compute.</p>
<p>In the Start method, call Init. The Init method initializes each particle. The position is set to <strong>Homogeneous Clip Space</strong>. That is a value between -w and w for each axis; w is the fourth component of the coordinate. Assuming w=1, then -1, -1, -1 is bottom-left at near frustum and 1, 1, 1 is top-right, far frustum.</p>
<pre><code>Vector v = new Vector3()<span class="hljs-comment">;</span>
v.x = <span class="hljs-built_in">Random</span>.value * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>.<span class="hljs-number">0</span>f<span class="hljs-comment">;</span>
v.y = <span class="hljs-built_in">Random</span>.value * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>.<span class="hljs-number">0</span>f<span class="hljs-comment">;</span>
v.z = <span class="hljs-built_in">Random</span>.value * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>.<span class="hljs-number">0</span>f<span class="hljs-comment">;</span>
v.<span class="hljs-keyword">Normalize();
</span>v *= <span class="hljs-built_in">Random</span>.value * <span class="hljs-number">5</span><span class="hljs-comment">;</span>
particleArray[i].position.x = v.x<span class="hljs-comment">;</span>
particleArray[i].position.y = v.y<span class="hljs-comment">;</span>
particleArray[i].position.z = v.z<span class="hljs-comment">;</span>
particleArray[i].velocity.x = <span class="hljs-number">0</span>
particleArray[i].velocity.y = <span class="hljs-number">0</span><span class="hljs-comment">;</span>
particleArray[i].velocity.z = <span class="hljs-number">0</span><span class="hljs-comment">;</span>
</code></pre><p>particleArray[i].life = Random.value * 5.0f + 1.0f;</p>
<p><img src="_page_130_Picture_0.jpeg" alt=""></p>
<p>Start by creating a Vector3 with x, y, and z set to a random value between -1 and 1. Then, normalize this vector; remember to set the vector to have the length 1. Expand the length of 5. Using this vector, set the position of an individual particle in the particle array. Velocity is set to 0 and life to a random value between 1 and 6.</p>
<pre><code> <span class="hljs-comment">// create compute buffer</span>
 particleBuffer = <span class="hljs-keyword">new</span> ComputeBuffer(particleCount, SIZE_PARTICLE);
 particleBuffer.SetData(particleArray);
 <span class="hljs-comment">// find the id of the kernel</span>
 kernelID = shader.FindKernel(<span class="hljs-string">"CSParticle"</span>);
 <span class="hljs-keyword">uint</span> threadsX;
 shader.GetKernelThreadGroupSizes(kernelID, <span class="hljs-keyword">out</span> threadsX, <span class="hljs-keyword">out</span> _, <span class="hljs-keyword">out</span> _);
 groupSizeX = Mathf.CeilToInt((<span class="hljs-keyword">float</span>)particleCount / (<span class="hljs-keyword">float</span>)threadsX);
 <span class="hljs-comment">// bind the compute buffer to the shader and the compute shader</span>
 shader.SetBuffer(kernelID, <span class="hljs-string">"particleBuffer"</span>, particleBuffer);
 material.SetBuffer(<span class="hljs-string">'"particleBuffer", particleBuffer);</span>
</code></pre><pre><code> rp = <span class="hljs-keyword">new</span> <span class="hljs-type">RenderParams</span>(material);
 rp.worldBounds = <span class="hljs-keyword">new</span> <span class="hljs-type">Bounds</span>(Vector3.zero, <span class="hljs-number">10000</span>*Vector3.one);
</code></pre><p>The next step is to create a <strong><a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/ComputeBuffer-ctor.html">ComputeBuffer</a></strong>. Notice this has two parameters, the number of elements, count and the size of each element, stride. Then you need to populate the buffer using the SetData method. This transfers data from RAM to the GPU memory. To be accessed by a <strong><a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/ComputeShader.html">ComputeShader</a></strong> all data must be in GPU memory. You call code in a ComputeShader using a special type of function in the ComputeShader code called a kernel. Each kernel has a unique ID. You can find its ID by calling the <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/ComputeShader.FindKernel.html">FindKernel</a> method using the function name as a parameter.</p>
<p>Each kernel has three thread parameters x, y, and z. The magic of compute shaders is the way they work in parallel. For the particle example the thread group sizes are set as 256, 1, 1. To get the best performance from the GPU you&#39;ll need to know about the actual device architecture.</p>
<p>From a C# script you can access the thread group sizes using the compute shader method <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/ComputeShader.GetKernelThreadGroupSizes.html">GetKernelThreadGroupSizes</a>. To make sure you have a thread covering every particle you&#39;ll need to dispatch the kernel for the number of times as shown in this code:</p>
<h4 id="mathf-ceiltoint-float-particlecount-float-threadsx-">Mathf.CeilToInt((float)particleCount / (float)threadsX)</h4>
<p><img src="_page_131_Picture_0.jpeg" alt=""></p>
<p>For this example, all the work is in the x thread. Notice that the particleBuffer is passed to the material as well as the compute shader.</p>
<pre><code><span class="hljs-keyword">shader.SetBuffer(kernelID, </span><span class="hljs-string">"particleBuffer"</span>, particleBuffer)<span class="hljs-comment">;</span>
material.SetBuffer(<span class="hljs-string">"particleBuffer"</span>, particleBuffer)<span class="hljs-comment">;</span>
</code></pre><p>This is the principle trick of this example. You can use a shared ComputeBuffer, resident on the GPU, across the compute shader and a vertex-fragment shader. So, you can manipulate the content of the buffer in the compute shader and when it&#39;s time to render the object using a vertex-fragment shader, it makes use of the same buffer in the render.</p>
<p>You&#39;ll need to initialize a <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/RenderParams.html">RenderParams</a> instance; simply set a large Bounds instance. This will be needed when you use <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/Graphics.RenderPrimitives.html">Graphics.RenderPrimitives</a> to actually render the particles.</p>
<p>This brings you to the Update method. Here you set the deltaTime and mousePosition for the compute shader. Then, <a href="https://docs.unity3d.com/6000.0/Documentation/ScriptReference/ComputeShader.Dispatch.html">Dispatch</a> the kernelID you found earlier. When you Dispatch you set the number of work groups for the x, y, and z dimensions. Since you want to run the kernel for particleCount times and the x thread group size is 256, you&#39;ll precalculate groupSizeX to be the integer ceiling of particleCount / 256. Ceiling simply means if it was 7/2 the floating point value is 3.5; by taking the integer ceiling you raise this to the next whole number, 4. Using groupSizeX for the x dimension ensures the kernel will run with the x value having each index value from 0 to particleCount-1 and higher, if particleCount is not an exact multiple of 256. Once the Dispatch has completed, the particleBuffer will contain the new position values for each particle.</p>
<p>From here, you&#39;ll use a method of the Graphics interface, however, it first requires some explanation. This method, RenderPrimitives takes four parameters:</p>
<ul>
<li>A RenderParams instance that at a minimum defines a Bounds area</li>
<li>The type of mesh topology (here you&#39;re rendering points, but you could be rendering lines of triangles)</li>
<li>The vertex count in a single instance; for points this will always be just one</li>
<li>The instance count; for this example that is the number of particles</li>
</ul>
<pre><code><span class="hljs-keyword">void</span> Update()
 {
 <span class="hljs-built_in">float</span>[] mousePosition2D = { cursorPos.x, cursorPos.y };
 <span class="hljs-comment">// Send data to the compute shader</span>
 <span class="hljs-built_in">shader</span>.SetFloat(<span class="hljs-string">"deltaTime"</span>, Time.deltaTime);
 <span class="hljs-built_in">shader</span>.SetFloats(<span class="hljs-string">"mousePosition"</span>, mousePosition2D);
</code></pre><p><img src="_page_132_Picture_0.jpeg" alt=""></p>
<pre><code> <span class="hljs-comment">// Update the Particles</span>
 shader.Dispatch(kernelID, groupSizeX, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>);
</code></pre><pre><code> <span class="hljs-selector-tag">Graphics</span><span class="hljs-selector-class">.RenderPrimitives</span>(<span class="hljs-selector-tag">rp</span>, <span class="hljs-selector-tag">MeshTopology</span><span class="hljs-selector-class">.Points</span>, 1, <span class="hljs-selector-tag">particleCount</span> );
</code></pre><p>}</p>
<p>The actual rendering will be handled by the shader attached to the material. Let&#39;s look at that now.</p>
<p>In the ParticleFun.shader file we need to add a reference to the buffer. This will need a definition of the particle struct.</p>
<pre><code>struct Particle{
float3 position<span class="hljs-comment">;</span>
 float3 velocity<span class="hljs-comment">;</span>
 float life<span class="hljs-comment">;</span>
}<span class="hljs-comment">;</span>
</code></pre><p>StructuredBuffer<Particle> particleBuffer;</p>
<p>Set the buffer as a <a href="https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/sm5-object-structuredbuffer">StructuredBuffer</a>, not a <a href="https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/sm5-object-rwstructuredbuffer">RWStructuredBuffer</a>, because the shader will not be writing to this buffer. The compute shader will do that.</p>
<p>We have a _PointSize property. Notice that the Attributes struct, an instance of which is passed to the vert function, has an instanceID property. For a point shader the instance id will be set to 0 through to particles count – 1. We use this value as an index into the buffer. Since the compute shader is going to update this position value we have a way of using the compute shader for positioning and the vertex-fragment shader to do the rendering. When using MeshTopology points, the shader must set the input parameter with the semantic PSIZE to the pixel point size of a point. Here you set it to the variable PointSize which was passed by the script.</p>
<pre><code><span class="hljs-keyword">Shader </span><span class="hljs-string">"Custom/ParticleFun"</span>
{
 Properties
 {
 _PointSize(<span class="hljs-string">"Point size"</span>, Float) = <span class="hljs-number">5</span>.<span class="hljs-number">0</span>
 }
 <span class="hljs-keyword">SubShader
</span> {
 Tags { <span class="hljs-string">"RenderType"</span> = <span class="hljs-string">"Opaque"</span> <span class="hljs-string">"RenderPipeline"</span> = <span class="hljs-string">"UniversalPipeline"</span> }
</code></pre><p><img src="_page_133_Picture_0.jpeg" alt=""></p>
<p>{</p>
<pre><code> Pass
 HLSLPROGRAM
 <span class="hljs-comment">#pragma vertex vert</span>
 <span class="hljs-comment">#pragma fragment frag</span>
 struct Particle{
 float3 position<span class="hljs-comment">;</span>
 float3 velocity<span class="hljs-comment">;</span>
 float life<span class="hljs-comment">;</span>
 }<span class="hljs-comment">;</span>
 StructuredBuffer&lt;Particle&gt; particleBuffer<span class="hljs-comment">;</span>
 <span class="hljs-comment">#include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"</span>
 CBUFFER_START(UnityPerMaterial)
 float _PointSize<span class="hljs-comment">;</span>
 CBUFFER_END
 struct Attributes
 {
 float4 positionOS : POSITION<span class="hljs-comment">;</span>
 uint <span class="hljs-keyword">instanceID </span>: SV_InstanceID<span class="hljs-comment">;</span>
 UNITY_VERTEX_INPUT_INSTANCE_ID
 }<span class="hljs-comment">;</span>
 struct Varyings
 {
 float4 positionHCS : SV_POSITION<span class="hljs-comment">;</span>
 float4 color : COLOR<span class="hljs-comment">;</span>
 float size: PSIZE<span class="hljs-comment">;</span>
 }<span class="hljs-comment">;</span>
 Varyings vert(Attributes IN)
 {
</code></pre><pre><code> Varyings OUT<span class="hljs-comment">;</span>
</code></pre><pre><code> Particle particle = particleBuffer[<span class="hljs-keyword">IN</span>.instanceID];
 // Color
 <span class="hljs-built_in">float</span> lerpVal = particle.life * <span class="hljs-number">0.</span>5f;
 <span class="hljs-keyword">OUT</span>.color = half4(<span class="hljs-number">1.</span>f - lerpVal+<span class="hljs-number">0.1</span>, lerpVal+<span class="hljs-number">0.1</span>, <span class="hljs-number">1.</span>f, lerpVal);
 // <span class="hljs-keyword">Position</span>
 <span class="hljs-keyword">OUT</span>.positionHCS = TransformObjectToHClip(particle.<span class="hljs-keyword">position</span>);
 <span class="hljs-keyword">OUT</span>.<span class="hljs-built_in">size</span> = _PointSize;
 <span class="hljs-keyword">return</span> <span class="hljs-keyword">OUT</span>;
 }
 half4 frag(Varyings <span class="hljs-keyword">IN</span>) : SV_Target
 {
 <span class="hljs-keyword">return</span> <span class="hljs-keyword">IN</span>.color;
 }
 ENDHLSL
 }
 }
</code></pre><p>Let&#39;s switch to the compute shader. Create a compute shader via <strong>Create &gt; Shaders &gt; Compute Shader</strong> by right-clicking in the Project view<strong>.</strong></p>
<table>
<thead>
<tr>
<th>Shader</th>
<th></th>
<th>Standard Surface Shader</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Shader Graph</td>
<td>&gt;</td>
<td>Unlit Shader</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Testing</td>
<td>&gt;</td>
<td>Image Effect Shader</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Terrain</td>
<td>&gt;</td>
<td>Compute Shader</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Text Core</td>
<td>&gt;</td>
<td>Ray Tracing Shader</td>
<td></td>
<td></td>
</tr>
<tr>
<td>TextMeshPro</td>
<td>&gt;</td>
<td>Custom Render Texture</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Timeline</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Tutorials</td>
<td>A</td>
<td>Shader Variant Collection</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Create a compute shader.</p>
<p>}</p>
<p><img src="_page_135_Picture_0.jpeg" alt=""></p>
<p>You need to define a buffer for your particles. It needs a struct that matches the one in the script and you need to declare a RWStructuredBuffer because this shader is going to write to the buffer.</p>
<pre><code><span class="hljs-comment">// Particle's data</span>
<span class="hljs-keyword">struct</span> Particle
{
 float3 <span class="hljs-built_in">position</span>;
 float3 velocity;
 <span class="hljs-keyword">float</span> life;
};
</code></pre><p>// Particle&#39;s data, shared with the shader RWStructuredBuffer<Particle> particleBuffer;</p>
<p>In the CSParticle kernel you have:</p>
<pre><code>Particle <span class="hljs-keyword">particle</span> = particleBuffer[id.x];

<span class="hljs-keyword">particle</span>.life -= deltaTime;
float3 delta = float3(mousePosition.xy, <span class="hljs-number">0</span>) - <span class="hljs-keyword">particle</span>.position;
float3 dir = <span class="hljs-keyword">normalize</span>(delta);
<span class="hljs-keyword">particle</span>.velocity += dir;
<span class="hljs-keyword">particle</span>.position += <span class="hljs-keyword">particle</span>.velocity * deltaTime;
particleBuffer[id.x] = <span class="hljs-keyword">particle</span>;

<span class="hljs-keyword">if</span> (<span class="hljs-keyword">particle</span>.life &lt; <span class="hljs-number">0</span>) respawn(id.x);
</code></pre><p>Grab the particle from the buffer whose index is id.x. Because of the way you Dispatch this kernel this will have a value of 0 through to particleCount-1, as discussed. Then, decrement its life. Use the deltaTime property passed with each screen update to the compute shader by the script. It&#39;s the time in seconds that has elapsed since the last update. It will be a tiny value since Unity aims to display a new rendered frame at around 60 times a second. So deltaTime will be around 16ms or 0.016 seconds.</p>
<p>Create a vector from the particle to the mouse position, setting the <strong>z</strong> value to 0 – this is the default value for z for the particles. The mouse position value is set to the world position of the mouse. In the ParticleFun.cs file, use an OnGUI event.</p>
<p><img src="_page_136_Picture_0.jpeg" alt=""></p>
<pre><code>void OnGUI()
 {
 Vector3 p = <span class="hljs-keyword">new</span> <span class="hljs-type">Vector3</span>();
 Camera c = Camera.main;
 Event e = Event.current;
 Vector2 mousePos = <span class="hljs-keyword">new</span> <span class="hljs-type">Vector2</span>();
 <span class="hljs-comment">// Get the mouse position from Event.</span>
 <span class="hljs-comment">// Note that the y position from Event is inverted.</span>
 mousePos.x = e.mousePosition.x;
 mousePos.y = c.pixelHeight - e.mousePosition.y;
 p = c.ScreenToWorldPoint(<span class="hljs-keyword">new</span> <span class="hljs-type">Vector3</span>(mousePos.x, mousePos.y, 
 c.nearClipPlane ));
 cursorPos.x = p.x;
 cursorPos.y = p.y; 
 }
</code></pre><p>This detects a mouse press and we use a bit of code to convert this into a screen position. Screen coordinates use bottom as 0, whereas Event.mousePosition uses top as 0. To invert the y value, subtract mousePosition.y from the Camera pixelHeight. Then use the Camera method ScreenToWorldPoint (this method requires a Z value) and finally, nearClipPlane ensures a successful result.</p>
<p>If you want to know more about converting mouse positions to world coordinates, see <a href="https://gamedevbeginner.com/how-to-convert-the-mouse-position-to-world-space-in-unity-2d-3d/#screen_to_world_3d">this</a>  <a href="https://gamedevbeginner.com/how-to-convert-the-mouse-position-to-world-space-in-unity-2d-3d/#screen_to_world_3d">post</a> from Game Dev Beginner.</p>
<p>Now you have a vector you can use to accelerate the particle, away from the mouse position. Then use the particle&#39;s velocity modulated by deltaTime.</p>
<p>If a particle&#39;s life is less than 0 then respawn the particle and use a fast-random function to generate the random values necessary. XorShift random number generators were a great discovery for real-time graphics by <a href="https://en.wikipedia.org/wiki/George_Marsaglia">George Marsaglia</a>.</p>
<p>The particle is positioned within a sphere of radius 0.8, centered around the mouse position and z=0. Reset the life of a new particle to four seconds and reset the velocity to zero.</p>
<p><img src="_page_137_Picture_0.jpeg" alt=""></p>
<pre><code>void respawn(uint id)
{
      rng_state = id;
      <span class="hljs-type">float</span> tmp = (<span class="hljs-number">1.0</span> / <span class="hljs-number">4294967296.0</span>);
      <span class="hljs-type">float</span> f0 = <span class="hljs-type">float</span>(rand_xorshift()) * tmp - <span class="hljs-number">0.5</span>;
      <span class="hljs-type">float</span> f1 = <span class="hljs-type">float</span>(rand_xorshift()) * tmp - <span class="hljs-number">0.5</span>;
      float3 normalF3 = normalize(float3(f0, f1, <span class="hljs-number">0.0</span>)) * <span class="hljs-number">0.8</span>f;
      normalF3 *= <span class="hljs-type">float</span>(rand_xorshift()) * tmp;
      particleBuffer[id].position = float3(normalF3.x + mousePosition.x, normalF3.y + 
mousePosition.y, <span class="hljs-number">0.0</span>);
      <span class="hljs-comment">// reset the life of this particle</span>
      particleBuffer[id].life = <span class="hljs-number">4</span>;
      particleBuffer[id].velocity = float3(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>);
}
</code></pre><p>Let&#39;s use the life property to control the color of the particle. Back in the vertex-fragment shader in the vert function add this code to the color assignment:</p>
<pre><code><span class="hljs-type">float</span> life = particleBuffer[instance_id].life;
<span class="hljs-type">float</span> lerpVal = life * <span class="hljs-number">0.25</span>f;
o.color = fixed4(<span class="hljs-number">1.0</span>f - lerpVal+<span class="hljs-number">0.1</span>, lerpVal+<span class="hljs-number">0.1</span>, <span class="hljs-number">1.0</span>f, lerpVal);
</code></pre><p>Now the particles change color. The lerp value will be a value between 1 and 0, because life is set to 4 by the respawn function and you multiply this value by 0.25. The red channel will start at 0.1 and increase to 1.1 as life decreases, while the green channel starts at 1.1 and decreases over time to 0.1. Blue is always 1 and alpha decreases over time, causing the pixel to fade away.</p>
<p>This example shows how useful it can be to combine a compute shader and vertex-fragment shader when rendering multiple instances of the same asset. The simplest asset of all is used, a single pixel color value. The next example shows you how to extend this concept to render multiple mesh objects.</p>
<h2 id="-span-id-page-138-0-span-adding-a-mesh-object"><span id="page-138-0"></span>Adding a mesh object</h2>
<p><img src="_page_138_Picture_2.jpeg" alt=""></p>
<p>The Instanced Flocking scene</p>
<p>This mesh object example implements flocking. The idea of flocking came from observations of birds in flight, and the conclusion that the motion of a flock could be controlled with simple rules, the first one being that a bird could only know about the behavior of a small group of birds nearby and within eyesight. It was first suggested at <a href="https://www.cs.toronto.edu/~dt/siggraph97-course/cwr87/">Siggraph 87</a> in a paper presented by <a href="https://www.red3d.com/cwr/">Craig Reynolds</a>. He called members of the flock Boids, a term that has stuck as a common term for an individual member of a computer simulated flock. Boid is short for &quot;Bird-oid&quot; object.</p>
<p>Once you&#39;ve scanned the members of the flock that are within a certain user-defined radius and in the line of sight, use these three rules to adjust the position, orientation, and velocity of each boid.</p>
<ol>
<li><strong>Separation</strong>: Steer to avoid crowding other local boids. To achieve this, find the vector that points away from vectors from the current boid to other local boids.</li>
</ol>
<p><img src="_page_138_Figure_7.jpeg" alt=""></p>
<p>Image by Craig Reynolds, <em><a href="https://www.red3d.com/cwr/boids/">Boids: Background and Update</a></em></p>
<p><img src="_page_139_Picture_0.jpeg" alt=""></p>
<ol>
<li><strong>Alignment</strong>: Steer towards the average heading of local boids. Lerp the current heading with the average heading of local boids.</li>
</ol>
<p><img src="_page_139_Picture_3.jpeg" alt=""></p>
<p>Image by Craig Reynolds, <em><a href="https://www.red3d.com/cwr/boids/">Boids: Background and Update</a></em></p>
<ol>
<li><strong>Cohesion</strong>: Steer to move toward the average position of local boids. Find the average position of local boids and steer towards it.</li>
</ol>
<p><img src="_page_139_Figure_6.jpeg" alt=""></p>
<p>Image by Craig Reynolds, <em><a href="https://www.red3d.com/cwr/boids/">Boids: Background and Update</a></em></p>
<p>If separation and cohesion seem to be opposites of each other, remember that for separation you are working with vectors from the current boid. Whereas for cohesion you are considering the average position of a group of boids in the calculation.</p>
<p>Let&#39;s look at a concrete example.</p>
<p>Open the scene <strong>InstancedFlocking</strong> from <strong>Scenes &gt; ComputeShaders &gt; Instanced</strong> and in VS Code, open InstancedFlocking.cs and InstancedFlocking.compute from the same folder. You&#39;re going to be working with another ComputeBuffer, with this example focusing on the compute shader.</p>
<p><img src="_page_140_Picture_0.jpeg" alt=""></p>
<p>A boid has a position, direction, and a noise offset value. A constructor method is added to this struct, but the data in the struct is only two Vector3s and a float.</p>
<pre><code>public struct <span class="hljs-keyword">Boid
</span> {
 public Vector3 position<span class="hljs-comment">;</span>
 public Vector3 <span class="hljs-keyword">direction;
</span> public float noise_offset<span class="hljs-comment">;</span>
 public <span class="hljs-keyword">Boid(Vector3 </span>pos, Vector3 <span class="hljs-keyword">dir, </span>float offset)
 {
 position.x = pos.x<span class="hljs-comment">;</span>
 position.y = pos.y<span class="hljs-comment">;</span>
 position.z = pos.z<span class="hljs-comment">;</span>
 <span class="hljs-keyword">direction.x </span>= <span class="hljs-keyword">dir.x;
</span> <span class="hljs-keyword">direction.y </span>= <span class="hljs-keyword">dir.y;
</span> <span class="hljs-keyword">direction.z </span>= <span class="hljs-keyword">dir.z;
</span> noise_offset = offset<span class="hljs-comment">;</span>
 }
 }
</code></pre><p>There are a number of public properties that will allow the user to adjust the behavior of the flock; each will be examined as they come up when coding the shader.</p>
<p>InitBoids creates and populates a boids array. Position is a random value inside a sphere.</p>
<p>InitShader creates and sets the ComputeBuffer from the boids array and sets a number of properties in the compute shader.</p>
<p>The Update method sets time and delta time and then dispatches the kernel. Once the compute shader has calculated the new position and direction for each boid, use RenderMeshIndirect to actually render the boids. <a href="https://docs.unity3d.com/ScriptReference/Graphics.RenderMeshIndirect.html">RenderMeshIndirect</a> needs a <a href="https://docs.unity3d.com/ScriptReference/RenderParams.html">RenderParams</a> instance, a mesh, and a <a href="https://docs.unity3d.com/ScriptReference/GraphicsBuffer.html">GraphicsBuffer</a> instance. The RenderParams instance is initialized in the Start method.</p>
<pre><code>renderParams = <span class="hljs-keyword">new</span> <span class="hljs-type">RenderParams</span>(boidMaterial);
renderParams.worldBounds = <span class="hljs-keyword">new</span> <span class="hljs-type">Bounds</span>(Vector3.zero, Vector3.one * <span class="hljs-number">1000</span>);
</code></pre><p><img src="_page_141_Picture_0.jpeg" alt=""></p>
<p>Notice the constructor is passed a material. This material must support instancing. The method you&#39;ll use is to edit the URP Lit shader. To do this, copy the following files from the folder <strong>Library/PackageCache/com.unity.render-pipelines.universal/Shaders</strong>:</p>
<ul>
<li>Lit.shader</li>
<li>LitForwardPass.hlsl</li>
<li>ShadowCasterPass.hlsl, if shadows are supported</li>
</ul>
<p>Put these files together into a subfolder.</p>
<p>Take a look at <strong>Scenes &gt; ComputeShaders &gt; Instanced &gt; Shader</strong> in VS Code. Notice the three files copied from the PackageCache folder. The changes to Lit.shader are minimal. The shader name is changed.</p>
<pre><code><span class="hljs-keyword">Shader </span><span class="hljs-string">"Custom/Flocking/Instanced"</span>
{
</code></pre><p>…</p>
<p>And the path to LitForwardPass.hlsl is changed to a single dot, meaning it will use the file in the same folder as the Lit.shader file. In the LitForwardPass.hlsl file, define the boid struct and the boidsBuffer.</p>
<pre><code>struct <span class="hljs-keyword">Boid
</span>{
 float3 position<span class="hljs-comment">; </span>
 float3 <span class="hljs-keyword">direction; </span>
 float noise_offset<span class="hljs-comment">;</span>
}<span class="hljs-comment">;</span>
</code></pre><pre><code>StructuredBuffer&lt;Boid&gt; boidsBuffer<span class="hljs-comment">;</span>
</code></pre><p>We add a create_matrix function. This creates a rotation and position matrix from a position, direction, and up vectors.</p>
<pre><code>float4x4 create_matrix(float3 pos, float3 dir, float3 up) {
 float3 <span class="hljs-built_in">zaxis</span> = normalize(dir);
 float3 <span class="hljs-built_in">xaxis</span> = normalize(cross(up, <span class="hljs-built_in">zaxis</span>));
 float3 <span class="hljs-built_in">yaxis</span> = cross(<span class="hljs-built_in">zaxis</span>, <span class="hljs-built_in">xaxis</span>);
 <span class="hljs-built_in">return</span> float4x4(
 <span class="hljs-built_in">xaxis</span>.x, <span class="hljs-built_in">yaxis</span>.x, <span class="hljs-built_in">zaxis</span>.x, pos.x,
 <span class="hljs-built_in">xaxis</span>.y, <span class="hljs-built_in">yaxis</span>.y, <span class="hljs-built_in">zaxis</span>.y, pos.y,
 <span class="hljs-built_in">xaxis</span>.z, <span class="hljs-built_in">yaxis</span>.z, <span class="hljs-built_in">zaxis</span>.z, pos.z,
 <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>
 );
}
</code></pre><p><img src="_page_142_Picture_0.jpeg" alt=""></p>
<p>It&#39;s important to add an SV_instanceID element to the Attributes struct. This will allow access to the instanceID in the vertex shader.</p>
<pre><code>struct Attributes
{
 float4 positionOS : POSITION<span class="hljs-comment">;</span>
 float3 <span class="hljs-keyword">normalOS </span>: <span class="hljs-keyword">NORMAL;
</span> float4 tangentOS : TANGENT<span class="hljs-comment">;</span>
 float2 texcoord : TEXCOORD0<span class="hljs-comment">;</span>
 float2 staticLightmapUV : TEXCOORD1<span class="hljs-comment">;</span>
 float2 dynamicLightmapUV : TEXCOORD2<span class="hljs-comment">;</span>
 uint <span class="hljs-keyword">instanceID </span>: SV_InstanceID<span class="hljs-comment">;</span>
 UNITY_VERTEX_INPUT_INSTANCE_ID
</code></pre><h4 id="-">};</h4>
<p>The Lit.shader defines the function LitPassVertex as the vertex shader. This function is in the LitForwardPass.hlsl for a Forward render pipeline.</p>
<p>Now that you have an instanceID in the input Attributes parameter, you can source an individual boid from the boidsBuffer. Use the create_matrix function and the boid position and direction to create a matrix. Edit the parameter passed to GetVertexPosition in the default version of this file by multiplying input.positionOS by the matrix just created. GetVertexPosition is expecting a float3 so outside the mul function, add .xyz to change the float4 output to a float3. Do the same for the NormalInput:</p>
<pre><code><span class="hljs-keyword">Boid </span><span class="hljs-keyword">boid </span>= <span class="hljs-keyword">boidsBuffer[input.instanceID];</span>
</code></pre><pre><code>float4x4 mat = create_matrix(<span class="hljs-name">boid</span>.position, boid.direction, float3(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>))<span class="hljs-comment">;</span>
</code></pre><pre><code>VertexPositionInputs vertexInput = GetVertexPositionInputs(<span class="hljs-name">mul</span>(<span class="hljs-name">mat</span>, input.positionOS).xyz)<span class="hljs-comment">;</span>
</code></pre><p>VertexNormalInputs normalInput = GetVertexNormalInputs(mul(mat, input.normalOS), mul(mat, input.tangentOS));</p>
<p>Now you have a material to use. Let&#39;s look at how you use this to display multiple mesh objects.</p>
<p>An additional buffer, argsBuffer, which is a graphics buffer, has been added to the script. The vertex-fragment shader will use this buffer when rendering. You&#39;ll need to add the code to initialize the buffer.</p>
<p>When creating the buffer, set the type as IndirectArguments, the elements in the array as 1, and the size of an element as the size of an IndirectDrawIndexedArgs. Create an array of a single IndirectDrawIndexedArgs. Set the indexCountPerInstance to the vertex count of the boidMesh using the GetIndexCount method and set the instanceCount to the numOfBoids property.</p>
<p><img src="_page_143_Picture_0.jpeg" alt=""></p>
<p>Copy the data over to the argsBuffer using the SetData method. Now the data is resident on the GPU:</p>
<pre><code>argsBuffer = <span class="hljs-keyword">new</span> GraphicsBuffer(GraphicsBuffer.Target.IndirectArguments, <span class="hljs-number">1</span>, GraphicsBuffer.
IndirectDrawIndexedArgs.<span class="hljs-built_in">size</span>);
</code></pre><pre><code>GraphicsBuffer.IndirectDrawIndexedArgs<span class="hljs-string">[]</span> data = new GraphicsBuffer.IndirectDrawIndexedArgs<span class="hljs-string">[1]</span>;
</code></pre><pre><code><span class="hljs-class"><span class="hljs-keyword">data</span>[0].indexCountPerInstance = boidMesh.<span class="hljs-type">GetIndexCount</span>(0);</span>
</code></pre><pre><code><span class="hljs-class"><span class="hljs-keyword">data</span>[0].instanceCount = (<span class="hljs-title">uint</span>)numOfBoids;</span>
</code></pre><pre><code><span class="hljs-title">argsBuffer</span>.<span class="hljs-type">SetData</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>);</span>
</code></pre><p>This leads to the Update method. Set the Time and deltaTime in the compute shader. Dispatch it and once this has calculated the boid positions and directions, use RenderMeshIndirect to render the boids.</p>
<pre><code> <span class="hljs-keyword">void</span> Update()
</code></pre><pre><code> {
</code></pre><pre><code> shader.SetFloat(<span class="hljs-string">"time"</span>, <span class="hljs-keyword">Time</span>.<span class="hljs-built_in">time</span>);
 shader.SetFloat(<span class="hljs-string">"deltaTime"</span>, <span class="hljs-keyword">Time</span>.deltaTime);
</code></pre><pre><code> <span class="hljs-selector-tag">shader</span><span class="hljs-selector-class">.Dispatch</span>(<span class="hljs-selector-tag">this</span><span class="hljs-selector-class">.kernelHandle</span>, <span class="hljs-selector-tag">groupSizeX</span>, 1, 1);
</code></pre><pre><code> Graphics.RenderMeshIndirect( renderParams, <span class="hljs-keyword">boidMesh, </span>argsBuffer )<span class="hljs-comment">;</span>
</code></pre><h4 id="-">}</h4>
<p>That is everything to know about the script. Let&#39;s switch focus to the flocking code in the compute shader.</p>
<p>This involves applying the three simple rules outlined earlier: Separation, alignment, and cohesion. First, get the boid from the buffer based on id.x. Then, set up the initial separation, alignment, and cohesion values. Only consider nearby boids when calculating these values. Inevitably, the current boid is inside this radius so nearbyCount starts at 1, not 0. Then for every boid you can ignore it if the loop is pointing at the current boid.</p>
<p>To apply the updates, i must not equal id.x. Then you get the boid pointed to by the i variable. There&#39;s one more check to see if the distance from the current boid to the temp boid is less than the property neighbor distance:</p>
<pre><code><span class="hljs-keyword">Boid </span><span class="hljs-keyword">boid </span>= <span class="hljs-keyword">boidsBuffer[id.x];
</span>float3 separation = <span class="hljs-number">0</span><span class="hljs-comment">;</span>
float3 alignment = <span class="hljs-number">0</span><span class="hljs-comment">;</span>
float3 cohesion = flockPosition<span class="hljs-comment">;</span>
</code></pre><p><img src="_page_144_Picture_0.jpeg" alt=""></p>
<pre><code>uint nearbyCount = <span class="hljs-number">1</span>; <span class="hljs-comment">// Add self that is ignored in loop</span>
<span class="hljs-built_in">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; boidsCount; i++)
{
 <span class="hljs-built_in">if</span> (i != <span class="hljs-keyword">int</span>(id.x))
 {
 Boid tempBoid = boidsBuffer[i];
 <span class="hljs-built_in">if</span> (distance(boid.<span class="hljs-built_in">position</span>, tempBoid.<span class="hljs-built_in">position</span>) &lt; neighbourDistance){
 <span class="hljs-comment">//position calculation goes here</span>
 }
 }
}
</code></pre><p>The most complex calculation is separation, which is explained in more detail later in this section, but first, let&#39;s look at alignment and cohesion. For alignment, you&#39;ll get the sum of each boid&#39;s direction and for cohesion, the sum of each boid&#39;s position, with the cohesion variable having the value flock position:</p>
<pre><code>alignment += tempBoid.<span class="hljs-keyword">direction;
</span>cohesion += tempBoid.position<span class="hljs-comment">;</span>
nearbyCount++<span class="hljs-comment">;</span>
</code></pre><p>Then outside the loop you use the nearbyCount value to get the average of value for alignment and cohesion by dividing the accumulated values by the nearbyCount value. For cohesion you need to subtract the current boids position and normalize the result. This leaves the target direction as the sum of the three properties. But when applying the newly calculated direction, only use it at a very low blend with the existing boid direction. By using lerp, only 6% of the calculated direction is used to 94% of the existing.</p>
<p>Now that you have a boid direction (this is the forward vector not a rotation value), you can use it to update the position by multiplying it by the speed and deltaTime. The last step is to apply our updates back to the buffer.</p>
<pre><code><span class="hljs-symbol">float</span> avg = <span class="hljs-number">1</span>.<span class="hljs-number">0</span> / nearbyCount<span class="hljs-comment">;</span>
<span class="hljs-symbol">alignment</span> *= avg<span class="hljs-comment">;</span>
<span class="hljs-symbol">cohesion</span> *= avg<span class="hljs-comment">;</span>
<span class="hljs-symbol">cohesion</span> = normalize(cohesion - <span class="hljs-keyword">boid.position);
</span><span class="hljs-symbol">float3</span> direction = alignment + separation + cohesion<span class="hljs-comment">;</span>
<span class="hljs-keyword">boid.direction </span>= lerp(direction, normalize(<span class="hljs-keyword">boid.direction), </span><span class="hljs-number">0</span>.<span class="hljs-number">94</span>)<span class="hljs-comment">;</span>
<span class="hljs-keyword">boid.position </span>+= <span class="hljs-keyword">boid.direction </span>* <span class="hljs-keyword">boidSpeed </span>* deltaTime<span class="hljs-comment">;</span>
<span class="hljs-keyword">boidsBuffer[id.x] </span>= <span class="hljs-keyword">boid;</span>
</code></pre><p><img src="_page_145_Picture_0.jpeg" alt=""></p>
<p>If you run the program now the boids will all merge into one, as you haven&#39;t yet applied a separation value. Let&#39;s fix that.</p>
<p>Back in the for loop you first get a vector, and its length, from the temporary boid to the boid you&#39;re currently setting. If the temporary boid is farther away than the value set as neighbor distance then ignore it. If it&#39;s less than this distance you need to allow for it in your separation value. Use the offset vector and scale this by a value that increases the nearer you get to the boid you&#39;re setting. Because you&#39;re using 1 over dist you&#39;ll need to make sure dist is never zero, so avoid division by zero.</p>
<p>So, what does 1/dist minus 1/neighbourDistance mean? Think about what happens when dist is small, 1/dist is very large and you can ignore the small value of 1/neighbor distance. Whereas if this boid is near the limit of neighbor distance then you get more or less 1/neighbor distance minus 1/neighbor distance or zero. So when a boid is close to the target boid, you massively increase the separation vector and when one is near the neighbor distance value you ignore it.</p>
<pre><code>float3 offset = boid.position - tempBoid.position;
<span class="hljs-built_in">float</span> <span class="hljs-built_in">dist</span> = length(offset);
<span class="hljs-keyword">if</span> (<span class="hljs-built_in">dist</span>&lt;neighbourDistance){
 <span class="hljs-built_in">dist</span> = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">dist</span>, <span class="hljs-number">0.000001</span>);
 separation += offset * (<span class="hljs-number">1.0</span>/<span class="hljs-built_in">dist</span> - <span class="hljs-number">1.0</span>/neighbourDistance);
}
</code></pre><p>Now you have a flocking example. After a few seconds all the boids come together and orbit around the flock position.</p>
<p><img src="_page_145_Picture_7.jpeg" alt=""></p>
<p>SkinnedFlocking scene</p>
<p><img src="_page_146_Picture_0.jpeg" alt=""></p>
<p>This final example uses a <a href="https://docs.unity3d.com/ScriptReference/SkinnedMeshRenderer.html">Skinned Mesh Renderer</a>. Open the scene called SkinnedFlocking via the <strong>Scenes &gt; ComputeShaders &gt; Skinned</strong> folder and run it. You&#39;ll see loads of birds flying around. In VS Code, open SkinnedFlocking.cs from the Scripts folder. From the Shader folder, open SkinnedFlocking.compute and LightForwardPass.hlsl.</p>
<p> Let&#39;s start with the script. This is essentially the same as the previous example, with a few changes. First, the boid struct now has a frame property. Use this to select the animation frame to display. There is now a numOfFrames property that is passed to the compute shader and the custom Lit shader. The script also uses the EnableKeyword and DisableKeyword methods of the shader to add or remove a define from the shader. This example also has a third buffer, the vertexAnimationBuffer, and you might have noticed a new method, GenerateVertexAnimationBuffer. Let&#39;s review this method.</p>
<p> Before the program starts you&#39;ll use an <a href="https://docs.unity3d.com/ScriptReference/Animator.html">Animator</a> component to set the mesh into a series of poses. Then put the vertex positions in the pose into a buffer. By choosing the correct index from this buffer you can display a series of different poses of the mesh. The first thing you need is the Animator component. Animations in Unity can use layers, but keep it simple here and use just layer one.</p>
<pre><code><span class="hljs-attribute">animator</span> = boidObject.GetComponentInChildren&lt;Animator&gt;();
<span class="hljs-attribute">int iLayer</span> = 0;
</code></pre><p>You can see that the Animator is set to automatically play the animation clip FlapWings. To set a pose you&#39;ll need the state info, a new Mesh to store the pose and a couple of variables.</p>
<pre><code><span class="hljs-attribute">AnimatorStateInfo aniStateInfo</span> = animator.GetCurrentAnimatorStateInfo(iLayer);
</code></pre><pre><code><span class="hljs-attribute"> Mesh bakedMesh</span> = new Mesh();
<span class="hljs-attribute"> float sampleTime</span> = 0;
<span class="hljs-attribute"> float perFrameTime</span> = 0;
</code></pre><p>This script has a public property called animationClip that&#39;s set to the FlapWings animation. Use the frame rate and length properties included in the <a href="https://docs.unity3d.com/ScriptReference/AnimationClip.html">AnimationClip</a> to determine the number of frames to bake. Because this value is going to be used so often to set the index in the vertexAnimationBuffer to use for a vertex, it will be more efficient if it&#39;s a power of two. The Mathf object has a useful method to set this value. Now you can get the duration of an individual frame.</p>
<pre><code><span class="hljs-attribute">numOfFrames</span> = Mathf.ClosestPowerOfTwo((int)(animationClip.frameRate * animationClip.length));
<span class="hljs-attribute"> perFrameTime</span> = animationClip.length / numOfFrames;
</code></pre><p><img src="_page_147_Picture_0.jpeg" alt=""></p>
<p>You&#39;ll need the vertex count of the mesh. Now you can set up an array of Vector4s to store the vertex data. It&#39;s going to be vertex count times numOfFrames long. You&#39;re all set to grab the vertex data, which is done in a for loop. Recall that you got an aniStateInfo object that you can use to get the animation to play, setting the layer and start time. Then, call update with a deltaTime of zero. This will update the mesh to the position defined by this animation. Now you can bake the mesh to your bakedMesh object and then iterate through the vertices to store their values in the vertex array.</p>
<pre><code><span class="hljs-attribute">var vertexCount</span> = boidSMR.sharedMesh.vertexCount;
</code></pre><pre><code> Vector4[] vertexAnimationData = <span class="hljs-keyword">new</span> Vector4[vertexCount * numOfFrames];
 <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; numOfFrames; i++)
 {
 animator.Play(aniStateInfo.shortNameHash, iLayer, sampleTime);
 animator.Update(<span class="hljs-number">0</span>f);
 boidSMR.BakeMesh(bakedMesh);
 <span class="hljs-comment">//Grab the vertices into the vertex array here</span>
 <span class="hljs-keyword">for</span>(<span class="hljs-built_in">int</span> j = <span class="hljs-number">0</span>; j &lt; vertexCount; j++)
 {
 Vector4 <span class="hljs-built_in">vertex</span> = bakedMesh.vertices[j];
 <span class="hljs-built_in">vertex</span>.w = <span class="hljs-number">1</span>;
 vertexAnimationData[(j * numOfFrames) + i] = <span class="hljs-built_in">vertex</span>;
 }
 sampleTime += perFrameTime;
 }
</code></pre><p>At this stage you&#39;ll have an array of vertices that you can use to set up your ComputeBuffer. Pass this buffer to the material. The compute shader doesn&#39;t need this buffer, only the shader. You&#39;ll only read from the buffer.</p>
<pre><code>vertexAnimationBuffer = new ComputeBuffer(vertexCount * numOfFrames, <span class="hljs-number">16</span>)<span class="hljs-comment">;</span>
vertexAnimationBuffer.SetData(vertexAnimationData)<span class="hljs-comment">;</span>
<span class="hljs-keyword">boidMaterial.SetBuffer("vertexAnimation", </span>vertexAnimationBuffer)<span class="hljs-comment">;</span>
</code></pre><p>Now let&#39;s look at how the compute shader has been affected.</p>
<p>Use the current speed, deltaTime and the property boidFrameSpeed to adjust the boid frame property. A faster moving bird will flap quicker. Check that the frame property hasn&#39;t passed the numOfFrames property. If it has then subtract this value, so boid.frame will be in the range 0 to numOfFrames.</p>
<p><img src="_page_148_Picture_0.jpeg" alt=""></p>
<pre><code><span class="hljs-keyword">boid.frame </span>= <span class="hljs-keyword">boid.frame </span>+ velocity * deltaTime * <span class="hljs-keyword">boidFrameSpeed;
</span>if (<span class="hljs-keyword">boid.frame </span>&gt;= numOfFrames) <span class="hljs-keyword">boid.frame </span>-= numOfFrames<span class="hljs-comment">;</span>
</code></pre><p>Now let&#39;s review the shader. It&#39;s essential that the Attributes instance passed to the vertex shader function includes vertexID as well as instanceID.</p>
<pre><code>struct Attributes
{
 float4 positionOS : POSITION<span class="hljs-comment">;</span>
 float3 <span class="hljs-keyword">normalOS </span>: <span class="hljs-keyword">NORMAL;
</span> float4 tangentOS : TANGENT<span class="hljs-comment">;</span>
 float2 texcoord : TEXCOORD0<span class="hljs-comment">;</span>
 float2 staticLightmapUV : TEXCOORD1<span class="hljs-comment">;</span>
 float2 dynamicLightmapUV : TEXCOORD2<span class="hljs-comment">;</span>
 uint <span class="hljs-keyword">instanceID </span>: SV_InstanceID<span class="hljs-comment">;</span>
 uint vertexID : SV_VertexID<span class="hljs-comment">;</span>
 UNITY_VERTEX_INPUT_INSTANCE_ID
</code></pre><p>};</p>
<p>Now in the LitVertexPass function you&#39;ll get the boid from the boidsBuffer using the instanceID. If FRAME_INTERPOLATION is not defined then set positionOS.xyz to the vertexAnimation buffer at index vertexID * numFrames + boid.frame.</p>
<p>If FRAME_INTERPOLATION is defined then set positionOS.xyz to a blend of the current boid.frame value and the next frame using lerp. The blend value is determined by the fractional part of boid.frame. If this is 0 then the blend will be just the frame value. If the fractional part is 0.5 then it will result in a linear interpolation of 50% frame and 50% next.</p>
<pre><code><span class="hljs-keyword">Boid </span><span class="hljs-keyword">boid </span>= <span class="hljs-keyword">boidsBuffer[input.instanceID];</span>
</code></pre><pre><code> <span class="hljs-meta">#<span class="hljs-meta-keyword">ifdef</span> FRAME_INTERPOLATION</span>
 uint <span class="hljs-keyword">next</span> = boid.frame + <span class="hljs-number">1</span><span class="hljs-comment">;</span>
 <span class="hljs-keyword">if</span> (<span class="hljs-keyword">next</span> &gt;= numOfFrames) <span class="hljs-keyword">next</span> = <span class="hljs-number">0</span><span class="hljs-comment">;</span>
 float frameInterpolation = frac(boidsBuffer[input.instanceID].frame)<span class="hljs-comment">;</span>
 input.positionOS.xyz = lerp(vertexAnimation[input.vertexID * numOfFrames + boid.
frame], vertexAnimation[input.vertexID * numOfFrames + <span class="hljs-keyword">next</span>], frameInterpolation)<span class="hljs-comment">;</span>
 <span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span>
 input.positionOS.xyz = vertexAnimation[input.vertexID * numOfFrames + boid.frame]<span class="hljs-comment">;</span>
 <span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span>
</code></pre><p><img src="_page_149_Picture_0.jpeg" alt=""></p>
<p>Play the app now and see how the speed of the birds flapping their wings is affected by their velocity. Do this by following these steps:</p>
<ul>
<li><ol>
<li>Set up a vertex buffer that stores a number of poses of the bird, performing the flap wings animation.</li>
</ol>
</li>
<li><ol>
<li>In the compute shader set the Boid property frame to a value between 0 and numOfFrames.</li>
</ol>
</li>
<li><ol>
<li>The speed at which this frame property updates is dictated by the velocity of the bird.</li>
</ol>
</li>
<li><ol>
<li>In the surface shader you&#39;ll get the current and next frames using the Boid property.</li>
</ol>
</li>
<li><ol>
<li>In the vertex shader, use a combination of vertexID and the current and next frame values to get the appropriate value from the vertex buffer.</li>
</ol>
</li>
</ul>
<p>This recipe moves from using a compute shader to position single points being rendered, to rendering multiple animated meshes. Compute shaders offer a great performance boost for some computationally expensive processes and are well worth learning to use effectively.</p>
<h4 id="-more-resources-"><strong>More resources</strong></h4>
<p><a href="https://www.udemy.com/course/compute-shaders">Learn to Write Unity Compute Shaders</a></p>
<h2 id="-span-id-page-150-0-span-conclusion"><span id="page-150-0"></span>Conclusion</h2>
<p>As mentioned in the introduction to this guide, the e-book <em><a href="https://resources.unity.com/games/introduction-universal-render-pipeline-for-advanced-unity-creators?UNGATED=TRUE?utm_source=demand-gen&amp;utm_medium=pdf&amp;utm_campaign=render-with-quality-and-flexibility&amp;utm_content=urp-cookbook">Introduction to the Universal</a>  <a href="https://resources.unity.com/games/introduction-universal-render-pipeline-for-advanced-unity-creators?UNGATED=TRUE?utm_source=demand-gen&amp;utm_medium=pdf&amp;utm_campaign=render-with-quality-and-flexibility&amp;utm_content=urp-cookbook">Render Pipeline for advanced Unity creators</a></em> is a valuable guide for helping experienced Unity developers and technical artists get the best out of the latest capabilities available with <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/urp-introduction.html">URP</a>.</p>
<p>All of Unity&#39;s advanced technical e-books are available from the <a href="https://unity.com/how-to?utm_source=demand-gen&amp;utm_medium=pdf&amp;utm_campaign=render-with-quality-and-flexibility&amp;utm_content=urp-cookbook">Unity best practices hub</a>. The e-books can also be found on the <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/best-practice-guides.html?utm_source=demand-gen&amp;utm_medium=pdf&amp;utm_campaign=render-with-quality-and-flexibility&amp;utm_content=urp-cookbook">advanced best practices</a> Documentation page.</p>
<p><img src="_page_151_Picture_0.jpeg" alt=""></p>
<p><a href="https://unity.com/es">unity.com</a></p>
</body></html>